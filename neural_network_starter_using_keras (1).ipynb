{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "name": "neural_network_starter_using_keras.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WqYmPPAukMYr",
        "colab_type": "text"
      },
      "source": [
        "# Regression practice\n",
        "\n",
        "*Modelos Avanzados para el Analisis de Datos II*\n",
        "\n",
        "### 6th Class\n",
        "\n",
        "Developed by:\n",
        "\n",
        "* Andres Martinez\n",
        "* Nicolas Gil\n",
        "* Jorge Medina\n",
        "\n",
        "Source: https://www.kaggle.com/c/house-prices-advanced-regression-techniques\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "f077f279ec7cbcf71df65a7cd6bfe3415b75a377",
        "_cell_guid": "510fd1e0-253c-42d2-995a-eb5c1b0cafd1",
        "trusted": true,
        "id": "whPnLCZCHbeY",
        "colab_type": "code",
        "outputId": "a5469dce-2fcb-4027-b484-99f888e31cfd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from keras.layers import Dense\n",
        "from keras.models import Sequential\n",
        "from keras.regularizers import l1\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras import backend as K\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from scipy.stats import skew\n",
        "from scipy.stats.stats import pearsonr\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "from mpl_toolkits import mplot3d\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "\n",
        "from matplotlib import cm\n",
        "from matplotlib.ticker import LinearLocator, FormatStrFormatter\n",
        "\n",
        "from subprocess import check_output\n",
        "#print(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "0910d432d0813423b8dc0a05cbd95e0c1cc6130f",
        "id": "028WjFIcHbeg",
        "colab_type": "text"
      },
      "source": [
        "## Obtain Data\n",
        "\n",
        "Actually, the train and test data are saved on the GitHub https://github.com/afmartinezt/Advanced_Models_II By the way, the original source comes from: https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "f397b7a851c5659b0e109e7364ba15c18f285f63",
        "trusted": true,
        "id": "OuNiojJLHbeh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "url = \"https://github.com/afmartinezt/Advanced_Models_II/raw/master/\"\n",
        "\n",
        "train = pd.read_csv(url + \"train.csv\", index_col=\"Id\")\n",
        "test = pd.read_csv(url + \"test.csv\", index_col=\"Id\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_lDvEL1cJRPl",
        "colab_type": "code",
        "outputId": "41bcaae8-184b-4582-ad62-2f48262d0011",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 247
        }
      },
      "source": [
        "train.head()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>MSSubClass</th>\n",
              "      <th>MSZoning</th>\n",
              "      <th>LotFrontage</th>\n",
              "      <th>LotArea</th>\n",
              "      <th>Street</th>\n",
              "      <th>Alley</th>\n",
              "      <th>LotShape</th>\n",
              "      <th>LandContour</th>\n",
              "      <th>Utilities</th>\n",
              "      <th>LotConfig</th>\n",
              "      <th>LandSlope</th>\n",
              "      <th>Neighborhood</th>\n",
              "      <th>Condition1</th>\n",
              "      <th>Condition2</th>\n",
              "      <th>BldgType</th>\n",
              "      <th>HouseStyle</th>\n",
              "      <th>OverallQual</th>\n",
              "      <th>OverallCond</th>\n",
              "      <th>YearBuilt</th>\n",
              "      <th>YearRemodAdd</th>\n",
              "      <th>RoofStyle</th>\n",
              "      <th>RoofMatl</th>\n",
              "      <th>Exterior1st</th>\n",
              "      <th>Exterior2nd</th>\n",
              "      <th>MasVnrType</th>\n",
              "      <th>MasVnrArea</th>\n",
              "      <th>ExterQual</th>\n",
              "      <th>ExterCond</th>\n",
              "      <th>Foundation</th>\n",
              "      <th>BsmtQual</th>\n",
              "      <th>BsmtCond</th>\n",
              "      <th>BsmtExposure</th>\n",
              "      <th>BsmtFinType1</th>\n",
              "      <th>BsmtFinSF1</th>\n",
              "      <th>BsmtFinType2</th>\n",
              "      <th>BsmtFinSF2</th>\n",
              "      <th>BsmtUnfSF</th>\n",
              "      <th>TotalBsmtSF</th>\n",
              "      <th>Heating</th>\n",
              "      <th>HeatingQC</th>\n",
              "      <th>CentralAir</th>\n",
              "      <th>Electrical</th>\n",
              "      <th>1stFlrSF</th>\n",
              "      <th>2ndFlrSF</th>\n",
              "      <th>LowQualFinSF</th>\n",
              "      <th>GrLivArea</th>\n",
              "      <th>BsmtFullBath</th>\n",
              "      <th>BsmtHalfBath</th>\n",
              "      <th>FullBath</th>\n",
              "      <th>HalfBath</th>\n",
              "      <th>BedroomAbvGr</th>\n",
              "      <th>KitchenAbvGr</th>\n",
              "      <th>KitchenQual</th>\n",
              "      <th>TotRmsAbvGrd</th>\n",
              "      <th>Functional</th>\n",
              "      <th>Fireplaces</th>\n",
              "      <th>FireplaceQu</th>\n",
              "      <th>GarageType</th>\n",
              "      <th>GarageYrBlt</th>\n",
              "      <th>GarageFinish</th>\n",
              "      <th>GarageCars</th>\n",
              "      <th>GarageArea</th>\n",
              "      <th>GarageQual</th>\n",
              "      <th>GarageCond</th>\n",
              "      <th>PavedDrive</th>\n",
              "      <th>WoodDeckSF</th>\n",
              "      <th>OpenPorchSF</th>\n",
              "      <th>EnclosedPorch</th>\n",
              "      <th>3SsnPorch</th>\n",
              "      <th>ScreenPorch</th>\n",
              "      <th>PoolArea</th>\n",
              "      <th>PoolQC</th>\n",
              "      <th>Fence</th>\n",
              "      <th>MiscFeature</th>\n",
              "      <th>MiscVal</th>\n",
              "      <th>MoSold</th>\n",
              "      <th>YrSold</th>\n",
              "      <th>SaleType</th>\n",
              "      <th>SaleCondition</th>\n",
              "      <th>SalePrice</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Id</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>60</td>\n",
              "      <td>RL</td>\n",
              "      <td>65.0</td>\n",
              "      <td>8450</td>\n",
              "      <td>Pave</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Reg</td>\n",
              "      <td>Lvl</td>\n",
              "      <td>AllPub</td>\n",
              "      <td>Inside</td>\n",
              "      <td>Gtl</td>\n",
              "      <td>CollgCr</td>\n",
              "      <td>Norm</td>\n",
              "      <td>Norm</td>\n",
              "      <td>1Fam</td>\n",
              "      <td>2Story</td>\n",
              "      <td>7</td>\n",
              "      <td>5</td>\n",
              "      <td>2003</td>\n",
              "      <td>2003</td>\n",
              "      <td>Gable</td>\n",
              "      <td>CompShg</td>\n",
              "      <td>VinylSd</td>\n",
              "      <td>VinylSd</td>\n",
              "      <td>BrkFace</td>\n",
              "      <td>196.0</td>\n",
              "      <td>Gd</td>\n",
              "      <td>TA</td>\n",
              "      <td>PConc</td>\n",
              "      <td>Gd</td>\n",
              "      <td>TA</td>\n",
              "      <td>No</td>\n",
              "      <td>GLQ</td>\n",
              "      <td>706</td>\n",
              "      <td>Unf</td>\n",
              "      <td>0</td>\n",
              "      <td>150</td>\n",
              "      <td>856</td>\n",
              "      <td>GasA</td>\n",
              "      <td>Ex</td>\n",
              "      <td>Y</td>\n",
              "      <td>SBrkr</td>\n",
              "      <td>856</td>\n",
              "      <td>854</td>\n",
              "      <td>0</td>\n",
              "      <td>1710</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>Gd</td>\n",
              "      <td>8</td>\n",
              "      <td>Typ</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Attchd</td>\n",
              "      <td>2003.0</td>\n",
              "      <td>RFn</td>\n",
              "      <td>2</td>\n",
              "      <td>548</td>\n",
              "      <td>TA</td>\n",
              "      <td>TA</td>\n",
              "      <td>Y</td>\n",
              "      <td>0</td>\n",
              "      <td>61</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>2008</td>\n",
              "      <td>WD</td>\n",
              "      <td>Normal</td>\n",
              "      <td>208500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>20</td>\n",
              "      <td>RL</td>\n",
              "      <td>80.0</td>\n",
              "      <td>9600</td>\n",
              "      <td>Pave</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Reg</td>\n",
              "      <td>Lvl</td>\n",
              "      <td>AllPub</td>\n",
              "      <td>FR2</td>\n",
              "      <td>Gtl</td>\n",
              "      <td>Veenker</td>\n",
              "      <td>Feedr</td>\n",
              "      <td>Norm</td>\n",
              "      <td>1Fam</td>\n",
              "      <td>1Story</td>\n",
              "      <td>6</td>\n",
              "      <td>8</td>\n",
              "      <td>1976</td>\n",
              "      <td>1976</td>\n",
              "      <td>Gable</td>\n",
              "      <td>CompShg</td>\n",
              "      <td>MetalSd</td>\n",
              "      <td>MetalSd</td>\n",
              "      <td>None</td>\n",
              "      <td>0.0</td>\n",
              "      <td>TA</td>\n",
              "      <td>TA</td>\n",
              "      <td>CBlock</td>\n",
              "      <td>Gd</td>\n",
              "      <td>TA</td>\n",
              "      <td>Gd</td>\n",
              "      <td>ALQ</td>\n",
              "      <td>978</td>\n",
              "      <td>Unf</td>\n",
              "      <td>0</td>\n",
              "      <td>284</td>\n",
              "      <td>1262</td>\n",
              "      <td>GasA</td>\n",
              "      <td>Ex</td>\n",
              "      <td>Y</td>\n",
              "      <td>SBrkr</td>\n",
              "      <td>1262</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1262</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>TA</td>\n",
              "      <td>6</td>\n",
              "      <td>Typ</td>\n",
              "      <td>1</td>\n",
              "      <td>TA</td>\n",
              "      <td>Attchd</td>\n",
              "      <td>1976.0</td>\n",
              "      <td>RFn</td>\n",
              "      <td>2</td>\n",
              "      <td>460</td>\n",
              "      <td>TA</td>\n",
              "      <td>TA</td>\n",
              "      <td>Y</td>\n",
              "      <td>298</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>2007</td>\n",
              "      <td>WD</td>\n",
              "      <td>Normal</td>\n",
              "      <td>181500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>60</td>\n",
              "      <td>RL</td>\n",
              "      <td>68.0</td>\n",
              "      <td>11250</td>\n",
              "      <td>Pave</td>\n",
              "      <td>NaN</td>\n",
              "      <td>IR1</td>\n",
              "      <td>Lvl</td>\n",
              "      <td>AllPub</td>\n",
              "      <td>Inside</td>\n",
              "      <td>Gtl</td>\n",
              "      <td>CollgCr</td>\n",
              "      <td>Norm</td>\n",
              "      <td>Norm</td>\n",
              "      <td>1Fam</td>\n",
              "      <td>2Story</td>\n",
              "      <td>7</td>\n",
              "      <td>5</td>\n",
              "      <td>2001</td>\n",
              "      <td>2002</td>\n",
              "      <td>Gable</td>\n",
              "      <td>CompShg</td>\n",
              "      <td>VinylSd</td>\n",
              "      <td>VinylSd</td>\n",
              "      <td>BrkFace</td>\n",
              "      <td>162.0</td>\n",
              "      <td>Gd</td>\n",
              "      <td>TA</td>\n",
              "      <td>PConc</td>\n",
              "      <td>Gd</td>\n",
              "      <td>TA</td>\n",
              "      <td>Mn</td>\n",
              "      <td>GLQ</td>\n",
              "      <td>486</td>\n",
              "      <td>Unf</td>\n",
              "      <td>0</td>\n",
              "      <td>434</td>\n",
              "      <td>920</td>\n",
              "      <td>GasA</td>\n",
              "      <td>Ex</td>\n",
              "      <td>Y</td>\n",
              "      <td>SBrkr</td>\n",
              "      <td>920</td>\n",
              "      <td>866</td>\n",
              "      <td>0</td>\n",
              "      <td>1786</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>Gd</td>\n",
              "      <td>6</td>\n",
              "      <td>Typ</td>\n",
              "      <td>1</td>\n",
              "      <td>TA</td>\n",
              "      <td>Attchd</td>\n",
              "      <td>2001.0</td>\n",
              "      <td>RFn</td>\n",
              "      <td>2</td>\n",
              "      <td>608</td>\n",
              "      <td>TA</td>\n",
              "      <td>TA</td>\n",
              "      <td>Y</td>\n",
              "      <td>0</td>\n",
              "      <td>42</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>9</td>\n",
              "      <td>2008</td>\n",
              "      <td>WD</td>\n",
              "      <td>Normal</td>\n",
              "      <td>223500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>70</td>\n",
              "      <td>RL</td>\n",
              "      <td>60.0</td>\n",
              "      <td>9550</td>\n",
              "      <td>Pave</td>\n",
              "      <td>NaN</td>\n",
              "      <td>IR1</td>\n",
              "      <td>Lvl</td>\n",
              "      <td>AllPub</td>\n",
              "      <td>Corner</td>\n",
              "      <td>Gtl</td>\n",
              "      <td>Crawfor</td>\n",
              "      <td>Norm</td>\n",
              "      <td>Norm</td>\n",
              "      <td>1Fam</td>\n",
              "      <td>2Story</td>\n",
              "      <td>7</td>\n",
              "      <td>5</td>\n",
              "      <td>1915</td>\n",
              "      <td>1970</td>\n",
              "      <td>Gable</td>\n",
              "      <td>CompShg</td>\n",
              "      <td>Wd Sdng</td>\n",
              "      <td>Wd Shng</td>\n",
              "      <td>None</td>\n",
              "      <td>0.0</td>\n",
              "      <td>TA</td>\n",
              "      <td>TA</td>\n",
              "      <td>BrkTil</td>\n",
              "      <td>TA</td>\n",
              "      <td>Gd</td>\n",
              "      <td>No</td>\n",
              "      <td>ALQ</td>\n",
              "      <td>216</td>\n",
              "      <td>Unf</td>\n",
              "      <td>0</td>\n",
              "      <td>540</td>\n",
              "      <td>756</td>\n",
              "      <td>GasA</td>\n",
              "      <td>Gd</td>\n",
              "      <td>Y</td>\n",
              "      <td>SBrkr</td>\n",
              "      <td>961</td>\n",
              "      <td>756</td>\n",
              "      <td>0</td>\n",
              "      <td>1717</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>Gd</td>\n",
              "      <td>7</td>\n",
              "      <td>Typ</td>\n",
              "      <td>1</td>\n",
              "      <td>Gd</td>\n",
              "      <td>Detchd</td>\n",
              "      <td>1998.0</td>\n",
              "      <td>Unf</td>\n",
              "      <td>3</td>\n",
              "      <td>642</td>\n",
              "      <td>TA</td>\n",
              "      <td>TA</td>\n",
              "      <td>Y</td>\n",
              "      <td>0</td>\n",
              "      <td>35</td>\n",
              "      <td>272</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>2006</td>\n",
              "      <td>WD</td>\n",
              "      <td>Abnorml</td>\n",
              "      <td>140000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>60</td>\n",
              "      <td>RL</td>\n",
              "      <td>84.0</td>\n",
              "      <td>14260</td>\n",
              "      <td>Pave</td>\n",
              "      <td>NaN</td>\n",
              "      <td>IR1</td>\n",
              "      <td>Lvl</td>\n",
              "      <td>AllPub</td>\n",
              "      <td>FR2</td>\n",
              "      <td>Gtl</td>\n",
              "      <td>NoRidge</td>\n",
              "      <td>Norm</td>\n",
              "      <td>Norm</td>\n",
              "      <td>1Fam</td>\n",
              "      <td>2Story</td>\n",
              "      <td>8</td>\n",
              "      <td>5</td>\n",
              "      <td>2000</td>\n",
              "      <td>2000</td>\n",
              "      <td>Gable</td>\n",
              "      <td>CompShg</td>\n",
              "      <td>VinylSd</td>\n",
              "      <td>VinylSd</td>\n",
              "      <td>BrkFace</td>\n",
              "      <td>350.0</td>\n",
              "      <td>Gd</td>\n",
              "      <td>TA</td>\n",
              "      <td>PConc</td>\n",
              "      <td>Gd</td>\n",
              "      <td>TA</td>\n",
              "      <td>Av</td>\n",
              "      <td>GLQ</td>\n",
              "      <td>655</td>\n",
              "      <td>Unf</td>\n",
              "      <td>0</td>\n",
              "      <td>490</td>\n",
              "      <td>1145</td>\n",
              "      <td>GasA</td>\n",
              "      <td>Ex</td>\n",
              "      <td>Y</td>\n",
              "      <td>SBrkr</td>\n",
              "      <td>1145</td>\n",
              "      <td>1053</td>\n",
              "      <td>0</td>\n",
              "      <td>2198</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>Gd</td>\n",
              "      <td>9</td>\n",
              "      <td>Typ</td>\n",
              "      <td>1</td>\n",
              "      <td>TA</td>\n",
              "      <td>Attchd</td>\n",
              "      <td>2000.0</td>\n",
              "      <td>RFn</td>\n",
              "      <td>3</td>\n",
              "      <td>836</td>\n",
              "      <td>TA</td>\n",
              "      <td>TA</td>\n",
              "      <td>Y</td>\n",
              "      <td>192</td>\n",
              "      <td>84</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>12</td>\n",
              "      <td>2008</td>\n",
              "      <td>WD</td>\n",
              "      <td>Normal</td>\n",
              "      <td>250000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    MSSubClass MSZoning  LotFrontage  ...  SaleType SaleCondition SalePrice\n",
              "Id                                    ...                                  \n",
              "1           60       RL         65.0  ...        WD        Normal    208500\n",
              "2           20       RL         80.0  ...        WD        Normal    181500\n",
              "3           60       RL         68.0  ...        WD        Normal    223500\n",
              "4           70       RL         60.0  ...        WD       Abnorml    140000\n",
              "5           60       RL         84.0  ...        WD        Normal    250000\n",
              "\n",
              "[5 rows x 80 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qDMfmyYyJ8zE",
        "colab_type": "code",
        "outputId": "86014bfb-741b-4d5b-c552-549efb3b931c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 247
        }
      },
      "source": [
        "test.head()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>MSSubClass</th>\n",
              "      <th>MSZoning</th>\n",
              "      <th>LotFrontage</th>\n",
              "      <th>LotArea</th>\n",
              "      <th>Street</th>\n",
              "      <th>Alley</th>\n",
              "      <th>LotShape</th>\n",
              "      <th>LandContour</th>\n",
              "      <th>Utilities</th>\n",
              "      <th>LotConfig</th>\n",
              "      <th>LandSlope</th>\n",
              "      <th>Neighborhood</th>\n",
              "      <th>Condition1</th>\n",
              "      <th>Condition2</th>\n",
              "      <th>BldgType</th>\n",
              "      <th>HouseStyle</th>\n",
              "      <th>OverallQual</th>\n",
              "      <th>OverallCond</th>\n",
              "      <th>YearBuilt</th>\n",
              "      <th>YearRemodAdd</th>\n",
              "      <th>RoofStyle</th>\n",
              "      <th>RoofMatl</th>\n",
              "      <th>Exterior1st</th>\n",
              "      <th>Exterior2nd</th>\n",
              "      <th>MasVnrType</th>\n",
              "      <th>MasVnrArea</th>\n",
              "      <th>ExterQual</th>\n",
              "      <th>ExterCond</th>\n",
              "      <th>Foundation</th>\n",
              "      <th>BsmtQual</th>\n",
              "      <th>BsmtCond</th>\n",
              "      <th>BsmtExposure</th>\n",
              "      <th>BsmtFinType1</th>\n",
              "      <th>BsmtFinSF1</th>\n",
              "      <th>BsmtFinType2</th>\n",
              "      <th>BsmtFinSF2</th>\n",
              "      <th>BsmtUnfSF</th>\n",
              "      <th>TotalBsmtSF</th>\n",
              "      <th>Heating</th>\n",
              "      <th>HeatingQC</th>\n",
              "      <th>CentralAir</th>\n",
              "      <th>Electrical</th>\n",
              "      <th>1stFlrSF</th>\n",
              "      <th>2ndFlrSF</th>\n",
              "      <th>LowQualFinSF</th>\n",
              "      <th>GrLivArea</th>\n",
              "      <th>BsmtFullBath</th>\n",
              "      <th>BsmtHalfBath</th>\n",
              "      <th>FullBath</th>\n",
              "      <th>HalfBath</th>\n",
              "      <th>BedroomAbvGr</th>\n",
              "      <th>KitchenAbvGr</th>\n",
              "      <th>KitchenQual</th>\n",
              "      <th>TotRmsAbvGrd</th>\n",
              "      <th>Functional</th>\n",
              "      <th>Fireplaces</th>\n",
              "      <th>FireplaceQu</th>\n",
              "      <th>GarageType</th>\n",
              "      <th>GarageYrBlt</th>\n",
              "      <th>GarageFinish</th>\n",
              "      <th>GarageCars</th>\n",
              "      <th>GarageArea</th>\n",
              "      <th>GarageQual</th>\n",
              "      <th>GarageCond</th>\n",
              "      <th>PavedDrive</th>\n",
              "      <th>WoodDeckSF</th>\n",
              "      <th>OpenPorchSF</th>\n",
              "      <th>EnclosedPorch</th>\n",
              "      <th>3SsnPorch</th>\n",
              "      <th>ScreenPorch</th>\n",
              "      <th>PoolArea</th>\n",
              "      <th>PoolQC</th>\n",
              "      <th>Fence</th>\n",
              "      <th>MiscFeature</th>\n",
              "      <th>MiscVal</th>\n",
              "      <th>MoSold</th>\n",
              "      <th>YrSold</th>\n",
              "      <th>SaleType</th>\n",
              "      <th>SaleCondition</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Id</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1461</th>\n",
              "      <td>20</td>\n",
              "      <td>RH</td>\n",
              "      <td>80.0</td>\n",
              "      <td>11622</td>\n",
              "      <td>Pave</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Reg</td>\n",
              "      <td>Lvl</td>\n",
              "      <td>AllPub</td>\n",
              "      <td>Inside</td>\n",
              "      <td>Gtl</td>\n",
              "      <td>NAmes</td>\n",
              "      <td>Feedr</td>\n",
              "      <td>Norm</td>\n",
              "      <td>1Fam</td>\n",
              "      <td>1Story</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>1961</td>\n",
              "      <td>1961</td>\n",
              "      <td>Gable</td>\n",
              "      <td>CompShg</td>\n",
              "      <td>VinylSd</td>\n",
              "      <td>VinylSd</td>\n",
              "      <td>None</td>\n",
              "      <td>0.0</td>\n",
              "      <td>TA</td>\n",
              "      <td>TA</td>\n",
              "      <td>CBlock</td>\n",
              "      <td>TA</td>\n",
              "      <td>TA</td>\n",
              "      <td>No</td>\n",
              "      <td>Rec</td>\n",
              "      <td>468.0</td>\n",
              "      <td>LwQ</td>\n",
              "      <td>144.0</td>\n",
              "      <td>270.0</td>\n",
              "      <td>882.0</td>\n",
              "      <td>GasA</td>\n",
              "      <td>TA</td>\n",
              "      <td>Y</td>\n",
              "      <td>SBrkr</td>\n",
              "      <td>896</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>896</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>TA</td>\n",
              "      <td>5</td>\n",
              "      <td>Typ</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Attchd</td>\n",
              "      <td>1961.0</td>\n",
              "      <td>Unf</td>\n",
              "      <td>1.0</td>\n",
              "      <td>730.0</td>\n",
              "      <td>TA</td>\n",
              "      <td>TA</td>\n",
              "      <td>Y</td>\n",
              "      <td>140</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>120</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>MnPrv</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>2010</td>\n",
              "      <td>WD</td>\n",
              "      <td>Normal</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1462</th>\n",
              "      <td>20</td>\n",
              "      <td>RL</td>\n",
              "      <td>81.0</td>\n",
              "      <td>14267</td>\n",
              "      <td>Pave</td>\n",
              "      <td>NaN</td>\n",
              "      <td>IR1</td>\n",
              "      <td>Lvl</td>\n",
              "      <td>AllPub</td>\n",
              "      <td>Corner</td>\n",
              "      <td>Gtl</td>\n",
              "      <td>NAmes</td>\n",
              "      <td>Norm</td>\n",
              "      <td>Norm</td>\n",
              "      <td>1Fam</td>\n",
              "      <td>1Story</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>1958</td>\n",
              "      <td>1958</td>\n",
              "      <td>Hip</td>\n",
              "      <td>CompShg</td>\n",
              "      <td>Wd Sdng</td>\n",
              "      <td>Wd Sdng</td>\n",
              "      <td>BrkFace</td>\n",
              "      <td>108.0</td>\n",
              "      <td>TA</td>\n",
              "      <td>TA</td>\n",
              "      <td>CBlock</td>\n",
              "      <td>TA</td>\n",
              "      <td>TA</td>\n",
              "      <td>No</td>\n",
              "      <td>ALQ</td>\n",
              "      <td>923.0</td>\n",
              "      <td>Unf</td>\n",
              "      <td>0.0</td>\n",
              "      <td>406.0</td>\n",
              "      <td>1329.0</td>\n",
              "      <td>GasA</td>\n",
              "      <td>TA</td>\n",
              "      <td>Y</td>\n",
              "      <td>SBrkr</td>\n",
              "      <td>1329</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1329</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>Gd</td>\n",
              "      <td>6</td>\n",
              "      <td>Typ</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Attchd</td>\n",
              "      <td>1958.0</td>\n",
              "      <td>Unf</td>\n",
              "      <td>1.0</td>\n",
              "      <td>312.0</td>\n",
              "      <td>TA</td>\n",
              "      <td>TA</td>\n",
              "      <td>Y</td>\n",
              "      <td>393</td>\n",
              "      <td>36</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Gar2</td>\n",
              "      <td>12500</td>\n",
              "      <td>6</td>\n",
              "      <td>2010</td>\n",
              "      <td>WD</td>\n",
              "      <td>Normal</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1463</th>\n",
              "      <td>60</td>\n",
              "      <td>RL</td>\n",
              "      <td>74.0</td>\n",
              "      <td>13830</td>\n",
              "      <td>Pave</td>\n",
              "      <td>NaN</td>\n",
              "      <td>IR1</td>\n",
              "      <td>Lvl</td>\n",
              "      <td>AllPub</td>\n",
              "      <td>Inside</td>\n",
              "      <td>Gtl</td>\n",
              "      <td>Gilbert</td>\n",
              "      <td>Norm</td>\n",
              "      <td>Norm</td>\n",
              "      <td>1Fam</td>\n",
              "      <td>2Story</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>1997</td>\n",
              "      <td>1998</td>\n",
              "      <td>Gable</td>\n",
              "      <td>CompShg</td>\n",
              "      <td>VinylSd</td>\n",
              "      <td>VinylSd</td>\n",
              "      <td>None</td>\n",
              "      <td>0.0</td>\n",
              "      <td>TA</td>\n",
              "      <td>TA</td>\n",
              "      <td>PConc</td>\n",
              "      <td>Gd</td>\n",
              "      <td>TA</td>\n",
              "      <td>No</td>\n",
              "      <td>GLQ</td>\n",
              "      <td>791.0</td>\n",
              "      <td>Unf</td>\n",
              "      <td>0.0</td>\n",
              "      <td>137.0</td>\n",
              "      <td>928.0</td>\n",
              "      <td>GasA</td>\n",
              "      <td>Gd</td>\n",
              "      <td>Y</td>\n",
              "      <td>SBrkr</td>\n",
              "      <td>928</td>\n",
              "      <td>701</td>\n",
              "      <td>0</td>\n",
              "      <td>1629</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>TA</td>\n",
              "      <td>6</td>\n",
              "      <td>Typ</td>\n",
              "      <td>1</td>\n",
              "      <td>TA</td>\n",
              "      <td>Attchd</td>\n",
              "      <td>1997.0</td>\n",
              "      <td>Fin</td>\n",
              "      <td>2.0</td>\n",
              "      <td>482.0</td>\n",
              "      <td>TA</td>\n",
              "      <td>TA</td>\n",
              "      <td>Y</td>\n",
              "      <td>212</td>\n",
              "      <td>34</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>MnPrv</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>2010</td>\n",
              "      <td>WD</td>\n",
              "      <td>Normal</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1464</th>\n",
              "      <td>60</td>\n",
              "      <td>RL</td>\n",
              "      <td>78.0</td>\n",
              "      <td>9978</td>\n",
              "      <td>Pave</td>\n",
              "      <td>NaN</td>\n",
              "      <td>IR1</td>\n",
              "      <td>Lvl</td>\n",
              "      <td>AllPub</td>\n",
              "      <td>Inside</td>\n",
              "      <td>Gtl</td>\n",
              "      <td>Gilbert</td>\n",
              "      <td>Norm</td>\n",
              "      <td>Norm</td>\n",
              "      <td>1Fam</td>\n",
              "      <td>2Story</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>1998</td>\n",
              "      <td>1998</td>\n",
              "      <td>Gable</td>\n",
              "      <td>CompShg</td>\n",
              "      <td>VinylSd</td>\n",
              "      <td>VinylSd</td>\n",
              "      <td>BrkFace</td>\n",
              "      <td>20.0</td>\n",
              "      <td>TA</td>\n",
              "      <td>TA</td>\n",
              "      <td>PConc</td>\n",
              "      <td>TA</td>\n",
              "      <td>TA</td>\n",
              "      <td>No</td>\n",
              "      <td>GLQ</td>\n",
              "      <td>602.0</td>\n",
              "      <td>Unf</td>\n",
              "      <td>0.0</td>\n",
              "      <td>324.0</td>\n",
              "      <td>926.0</td>\n",
              "      <td>GasA</td>\n",
              "      <td>Ex</td>\n",
              "      <td>Y</td>\n",
              "      <td>SBrkr</td>\n",
              "      <td>926</td>\n",
              "      <td>678</td>\n",
              "      <td>0</td>\n",
              "      <td>1604</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>Gd</td>\n",
              "      <td>7</td>\n",
              "      <td>Typ</td>\n",
              "      <td>1</td>\n",
              "      <td>Gd</td>\n",
              "      <td>Attchd</td>\n",
              "      <td>1998.0</td>\n",
              "      <td>Fin</td>\n",
              "      <td>2.0</td>\n",
              "      <td>470.0</td>\n",
              "      <td>TA</td>\n",
              "      <td>TA</td>\n",
              "      <td>Y</td>\n",
              "      <td>360</td>\n",
              "      <td>36</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>2010</td>\n",
              "      <td>WD</td>\n",
              "      <td>Normal</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1465</th>\n",
              "      <td>120</td>\n",
              "      <td>RL</td>\n",
              "      <td>43.0</td>\n",
              "      <td>5005</td>\n",
              "      <td>Pave</td>\n",
              "      <td>NaN</td>\n",
              "      <td>IR1</td>\n",
              "      <td>HLS</td>\n",
              "      <td>AllPub</td>\n",
              "      <td>Inside</td>\n",
              "      <td>Gtl</td>\n",
              "      <td>StoneBr</td>\n",
              "      <td>Norm</td>\n",
              "      <td>Norm</td>\n",
              "      <td>TwnhsE</td>\n",
              "      <td>1Story</td>\n",
              "      <td>8</td>\n",
              "      <td>5</td>\n",
              "      <td>1992</td>\n",
              "      <td>1992</td>\n",
              "      <td>Gable</td>\n",
              "      <td>CompShg</td>\n",
              "      <td>HdBoard</td>\n",
              "      <td>HdBoard</td>\n",
              "      <td>None</td>\n",
              "      <td>0.0</td>\n",
              "      <td>Gd</td>\n",
              "      <td>TA</td>\n",
              "      <td>PConc</td>\n",
              "      <td>Gd</td>\n",
              "      <td>TA</td>\n",
              "      <td>No</td>\n",
              "      <td>ALQ</td>\n",
              "      <td>263.0</td>\n",
              "      <td>Unf</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1017.0</td>\n",
              "      <td>1280.0</td>\n",
              "      <td>GasA</td>\n",
              "      <td>Ex</td>\n",
              "      <td>Y</td>\n",
              "      <td>SBrkr</td>\n",
              "      <td>1280</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1280</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>Gd</td>\n",
              "      <td>5</td>\n",
              "      <td>Typ</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Attchd</td>\n",
              "      <td>1992.0</td>\n",
              "      <td>RFn</td>\n",
              "      <td>2.0</td>\n",
              "      <td>506.0</td>\n",
              "      <td>TA</td>\n",
              "      <td>TA</td>\n",
              "      <td>Y</td>\n",
              "      <td>0</td>\n",
              "      <td>82</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>144</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2010</td>\n",
              "      <td>WD</td>\n",
              "      <td>Normal</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      MSSubClass MSZoning  LotFrontage  ...  YrSold SaleType SaleCondition\n",
              "Id                                      ...                               \n",
              "1461          20       RH         80.0  ...    2010       WD        Normal\n",
              "1462          20       RL         81.0  ...    2010       WD        Normal\n",
              "1463          60       RL         74.0  ...    2010       WD        Normal\n",
              "1464          60       RL         78.0  ...    2010       WD        Normal\n",
              "1465         120       RL         43.0  ...    2010       WD        Normal\n",
              "\n",
              "[5 rows x 79 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "ce1b71d4cb2819d0410489a58f073c58e47dea4e",
        "trusted": true,
        "id": "E15S8_66Hbel",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "all_data = pd.concat((train.loc[:,'MSSubClass':'SaleCondition'],\n",
        "                      test.loc[:,'MSSubClass':'SaleCondition']))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n6rEhVSaNDaF",
        "colab_type": "text"
      },
      "source": [
        "## Data Exploratory Analysis "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "593dd525d4e948d46c24e999a7836e42cb522c1a",
        "id": "ONrpnQCgHbep",
        "colab_type": "code",
        "outputId": "d3780d94-f079-493e-a8d1-38ea1b141cb9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "all_data.info()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 2919 entries, 1 to 2919\n",
            "Data columns (total 79 columns):\n",
            "MSSubClass       2919 non-null int64\n",
            "MSZoning         2915 non-null object\n",
            "LotFrontage      2433 non-null float64\n",
            "LotArea          2919 non-null int64\n",
            "Street           2919 non-null object\n",
            "Alley            198 non-null object\n",
            "LotShape         2919 non-null object\n",
            "LandContour      2919 non-null object\n",
            "Utilities        2917 non-null object\n",
            "LotConfig        2919 non-null object\n",
            "LandSlope        2919 non-null object\n",
            "Neighborhood     2919 non-null object\n",
            "Condition1       2919 non-null object\n",
            "Condition2       2919 non-null object\n",
            "BldgType         2919 non-null object\n",
            "HouseStyle       2919 non-null object\n",
            "OverallQual      2919 non-null int64\n",
            "OverallCond      2919 non-null int64\n",
            "YearBuilt        2919 non-null int64\n",
            "YearRemodAdd     2919 non-null int64\n",
            "RoofStyle        2919 non-null object\n",
            "RoofMatl         2919 non-null object\n",
            "Exterior1st      2918 non-null object\n",
            "Exterior2nd      2918 non-null object\n",
            "MasVnrType       2895 non-null object\n",
            "MasVnrArea       2896 non-null float64\n",
            "ExterQual        2919 non-null object\n",
            "ExterCond        2919 non-null object\n",
            "Foundation       2919 non-null object\n",
            "BsmtQual         2838 non-null object\n",
            "BsmtCond         2837 non-null object\n",
            "BsmtExposure     2837 non-null object\n",
            "BsmtFinType1     2840 non-null object\n",
            "BsmtFinSF1       2918 non-null float64\n",
            "BsmtFinType2     2839 non-null object\n",
            "BsmtFinSF2       2918 non-null float64\n",
            "BsmtUnfSF        2918 non-null float64\n",
            "TotalBsmtSF      2918 non-null float64\n",
            "Heating          2919 non-null object\n",
            "HeatingQC        2919 non-null object\n",
            "CentralAir       2919 non-null object\n",
            "Electrical       2918 non-null object\n",
            "1stFlrSF         2919 non-null int64\n",
            "2ndFlrSF         2919 non-null int64\n",
            "LowQualFinSF     2919 non-null int64\n",
            "GrLivArea        2919 non-null int64\n",
            "BsmtFullBath     2917 non-null float64\n",
            "BsmtHalfBath     2917 non-null float64\n",
            "FullBath         2919 non-null int64\n",
            "HalfBath         2919 non-null int64\n",
            "BedroomAbvGr     2919 non-null int64\n",
            "KitchenAbvGr     2919 non-null int64\n",
            "KitchenQual      2918 non-null object\n",
            "TotRmsAbvGrd     2919 non-null int64\n",
            "Functional       2917 non-null object\n",
            "Fireplaces       2919 non-null int64\n",
            "FireplaceQu      1499 non-null object\n",
            "GarageType       2762 non-null object\n",
            "GarageYrBlt      2760 non-null float64\n",
            "GarageFinish     2760 non-null object\n",
            "GarageCars       2918 non-null float64\n",
            "GarageArea       2918 non-null float64\n",
            "GarageQual       2760 non-null object\n",
            "GarageCond       2760 non-null object\n",
            "PavedDrive       2919 non-null object\n",
            "WoodDeckSF       2919 non-null int64\n",
            "OpenPorchSF      2919 non-null int64\n",
            "EnclosedPorch    2919 non-null int64\n",
            "3SsnPorch        2919 non-null int64\n",
            "ScreenPorch      2919 non-null int64\n",
            "PoolArea         2919 non-null int64\n",
            "PoolQC           10 non-null object\n",
            "Fence            571 non-null object\n",
            "MiscFeature      105 non-null object\n",
            "MiscVal          2919 non-null int64\n",
            "MoSold           2919 non-null int64\n",
            "YrSold           2919 non-null int64\n",
            "SaleType         2918 non-null object\n",
            "SaleCondition    2919 non-null object\n",
            "dtypes: float64(11), int64(25), object(43)\n",
            "memory usage: 1.8+ MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "fb7b6c2989b5b3ed0fb2bbbacc7d246a4b6a9797",
        "id": "mcPMQAWIHbeu",
        "colab_type": "code",
        "outputId": "f2553609-6a8b-429d-ab10-f72bf3baa9db",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 247
        }
      },
      "source": [
        "all_data.head()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>MSSubClass</th>\n",
              "      <th>MSZoning</th>\n",
              "      <th>LotFrontage</th>\n",
              "      <th>LotArea</th>\n",
              "      <th>Street</th>\n",
              "      <th>Alley</th>\n",
              "      <th>LotShape</th>\n",
              "      <th>LandContour</th>\n",
              "      <th>Utilities</th>\n",
              "      <th>LotConfig</th>\n",
              "      <th>LandSlope</th>\n",
              "      <th>Neighborhood</th>\n",
              "      <th>Condition1</th>\n",
              "      <th>Condition2</th>\n",
              "      <th>BldgType</th>\n",
              "      <th>HouseStyle</th>\n",
              "      <th>OverallQual</th>\n",
              "      <th>OverallCond</th>\n",
              "      <th>YearBuilt</th>\n",
              "      <th>YearRemodAdd</th>\n",
              "      <th>RoofStyle</th>\n",
              "      <th>RoofMatl</th>\n",
              "      <th>Exterior1st</th>\n",
              "      <th>Exterior2nd</th>\n",
              "      <th>MasVnrType</th>\n",
              "      <th>MasVnrArea</th>\n",
              "      <th>ExterQual</th>\n",
              "      <th>ExterCond</th>\n",
              "      <th>Foundation</th>\n",
              "      <th>BsmtQual</th>\n",
              "      <th>BsmtCond</th>\n",
              "      <th>BsmtExposure</th>\n",
              "      <th>BsmtFinType1</th>\n",
              "      <th>BsmtFinSF1</th>\n",
              "      <th>BsmtFinType2</th>\n",
              "      <th>BsmtFinSF2</th>\n",
              "      <th>BsmtUnfSF</th>\n",
              "      <th>TotalBsmtSF</th>\n",
              "      <th>Heating</th>\n",
              "      <th>HeatingQC</th>\n",
              "      <th>CentralAir</th>\n",
              "      <th>Electrical</th>\n",
              "      <th>1stFlrSF</th>\n",
              "      <th>2ndFlrSF</th>\n",
              "      <th>LowQualFinSF</th>\n",
              "      <th>GrLivArea</th>\n",
              "      <th>BsmtFullBath</th>\n",
              "      <th>BsmtHalfBath</th>\n",
              "      <th>FullBath</th>\n",
              "      <th>HalfBath</th>\n",
              "      <th>BedroomAbvGr</th>\n",
              "      <th>KitchenAbvGr</th>\n",
              "      <th>KitchenQual</th>\n",
              "      <th>TotRmsAbvGrd</th>\n",
              "      <th>Functional</th>\n",
              "      <th>Fireplaces</th>\n",
              "      <th>FireplaceQu</th>\n",
              "      <th>GarageType</th>\n",
              "      <th>GarageYrBlt</th>\n",
              "      <th>GarageFinish</th>\n",
              "      <th>GarageCars</th>\n",
              "      <th>GarageArea</th>\n",
              "      <th>GarageQual</th>\n",
              "      <th>GarageCond</th>\n",
              "      <th>PavedDrive</th>\n",
              "      <th>WoodDeckSF</th>\n",
              "      <th>OpenPorchSF</th>\n",
              "      <th>EnclosedPorch</th>\n",
              "      <th>3SsnPorch</th>\n",
              "      <th>ScreenPorch</th>\n",
              "      <th>PoolArea</th>\n",
              "      <th>PoolQC</th>\n",
              "      <th>Fence</th>\n",
              "      <th>MiscFeature</th>\n",
              "      <th>MiscVal</th>\n",
              "      <th>MoSold</th>\n",
              "      <th>YrSold</th>\n",
              "      <th>SaleType</th>\n",
              "      <th>SaleCondition</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Id</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>60</td>\n",
              "      <td>RL</td>\n",
              "      <td>65.0</td>\n",
              "      <td>8450</td>\n",
              "      <td>Pave</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Reg</td>\n",
              "      <td>Lvl</td>\n",
              "      <td>AllPub</td>\n",
              "      <td>Inside</td>\n",
              "      <td>Gtl</td>\n",
              "      <td>CollgCr</td>\n",
              "      <td>Norm</td>\n",
              "      <td>Norm</td>\n",
              "      <td>1Fam</td>\n",
              "      <td>2Story</td>\n",
              "      <td>7</td>\n",
              "      <td>5</td>\n",
              "      <td>2003</td>\n",
              "      <td>2003</td>\n",
              "      <td>Gable</td>\n",
              "      <td>CompShg</td>\n",
              "      <td>VinylSd</td>\n",
              "      <td>VinylSd</td>\n",
              "      <td>BrkFace</td>\n",
              "      <td>196.0</td>\n",
              "      <td>Gd</td>\n",
              "      <td>TA</td>\n",
              "      <td>PConc</td>\n",
              "      <td>Gd</td>\n",
              "      <td>TA</td>\n",
              "      <td>No</td>\n",
              "      <td>GLQ</td>\n",
              "      <td>706.0</td>\n",
              "      <td>Unf</td>\n",
              "      <td>0.0</td>\n",
              "      <td>150.0</td>\n",
              "      <td>856.0</td>\n",
              "      <td>GasA</td>\n",
              "      <td>Ex</td>\n",
              "      <td>Y</td>\n",
              "      <td>SBrkr</td>\n",
              "      <td>856</td>\n",
              "      <td>854</td>\n",
              "      <td>0</td>\n",
              "      <td>1710</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>Gd</td>\n",
              "      <td>8</td>\n",
              "      <td>Typ</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Attchd</td>\n",
              "      <td>2003.0</td>\n",
              "      <td>RFn</td>\n",
              "      <td>2.0</td>\n",
              "      <td>548.0</td>\n",
              "      <td>TA</td>\n",
              "      <td>TA</td>\n",
              "      <td>Y</td>\n",
              "      <td>0</td>\n",
              "      <td>61</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>2008</td>\n",
              "      <td>WD</td>\n",
              "      <td>Normal</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>20</td>\n",
              "      <td>RL</td>\n",
              "      <td>80.0</td>\n",
              "      <td>9600</td>\n",
              "      <td>Pave</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Reg</td>\n",
              "      <td>Lvl</td>\n",
              "      <td>AllPub</td>\n",
              "      <td>FR2</td>\n",
              "      <td>Gtl</td>\n",
              "      <td>Veenker</td>\n",
              "      <td>Feedr</td>\n",
              "      <td>Norm</td>\n",
              "      <td>1Fam</td>\n",
              "      <td>1Story</td>\n",
              "      <td>6</td>\n",
              "      <td>8</td>\n",
              "      <td>1976</td>\n",
              "      <td>1976</td>\n",
              "      <td>Gable</td>\n",
              "      <td>CompShg</td>\n",
              "      <td>MetalSd</td>\n",
              "      <td>MetalSd</td>\n",
              "      <td>None</td>\n",
              "      <td>0.0</td>\n",
              "      <td>TA</td>\n",
              "      <td>TA</td>\n",
              "      <td>CBlock</td>\n",
              "      <td>Gd</td>\n",
              "      <td>TA</td>\n",
              "      <td>Gd</td>\n",
              "      <td>ALQ</td>\n",
              "      <td>978.0</td>\n",
              "      <td>Unf</td>\n",
              "      <td>0.0</td>\n",
              "      <td>284.0</td>\n",
              "      <td>1262.0</td>\n",
              "      <td>GasA</td>\n",
              "      <td>Ex</td>\n",
              "      <td>Y</td>\n",
              "      <td>SBrkr</td>\n",
              "      <td>1262</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1262</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>TA</td>\n",
              "      <td>6</td>\n",
              "      <td>Typ</td>\n",
              "      <td>1</td>\n",
              "      <td>TA</td>\n",
              "      <td>Attchd</td>\n",
              "      <td>1976.0</td>\n",
              "      <td>RFn</td>\n",
              "      <td>2.0</td>\n",
              "      <td>460.0</td>\n",
              "      <td>TA</td>\n",
              "      <td>TA</td>\n",
              "      <td>Y</td>\n",
              "      <td>298</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>2007</td>\n",
              "      <td>WD</td>\n",
              "      <td>Normal</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>60</td>\n",
              "      <td>RL</td>\n",
              "      <td>68.0</td>\n",
              "      <td>11250</td>\n",
              "      <td>Pave</td>\n",
              "      <td>NaN</td>\n",
              "      <td>IR1</td>\n",
              "      <td>Lvl</td>\n",
              "      <td>AllPub</td>\n",
              "      <td>Inside</td>\n",
              "      <td>Gtl</td>\n",
              "      <td>CollgCr</td>\n",
              "      <td>Norm</td>\n",
              "      <td>Norm</td>\n",
              "      <td>1Fam</td>\n",
              "      <td>2Story</td>\n",
              "      <td>7</td>\n",
              "      <td>5</td>\n",
              "      <td>2001</td>\n",
              "      <td>2002</td>\n",
              "      <td>Gable</td>\n",
              "      <td>CompShg</td>\n",
              "      <td>VinylSd</td>\n",
              "      <td>VinylSd</td>\n",
              "      <td>BrkFace</td>\n",
              "      <td>162.0</td>\n",
              "      <td>Gd</td>\n",
              "      <td>TA</td>\n",
              "      <td>PConc</td>\n",
              "      <td>Gd</td>\n",
              "      <td>TA</td>\n",
              "      <td>Mn</td>\n",
              "      <td>GLQ</td>\n",
              "      <td>486.0</td>\n",
              "      <td>Unf</td>\n",
              "      <td>0.0</td>\n",
              "      <td>434.0</td>\n",
              "      <td>920.0</td>\n",
              "      <td>GasA</td>\n",
              "      <td>Ex</td>\n",
              "      <td>Y</td>\n",
              "      <td>SBrkr</td>\n",
              "      <td>920</td>\n",
              "      <td>866</td>\n",
              "      <td>0</td>\n",
              "      <td>1786</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>Gd</td>\n",
              "      <td>6</td>\n",
              "      <td>Typ</td>\n",
              "      <td>1</td>\n",
              "      <td>TA</td>\n",
              "      <td>Attchd</td>\n",
              "      <td>2001.0</td>\n",
              "      <td>RFn</td>\n",
              "      <td>2.0</td>\n",
              "      <td>608.0</td>\n",
              "      <td>TA</td>\n",
              "      <td>TA</td>\n",
              "      <td>Y</td>\n",
              "      <td>0</td>\n",
              "      <td>42</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>9</td>\n",
              "      <td>2008</td>\n",
              "      <td>WD</td>\n",
              "      <td>Normal</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>70</td>\n",
              "      <td>RL</td>\n",
              "      <td>60.0</td>\n",
              "      <td>9550</td>\n",
              "      <td>Pave</td>\n",
              "      <td>NaN</td>\n",
              "      <td>IR1</td>\n",
              "      <td>Lvl</td>\n",
              "      <td>AllPub</td>\n",
              "      <td>Corner</td>\n",
              "      <td>Gtl</td>\n",
              "      <td>Crawfor</td>\n",
              "      <td>Norm</td>\n",
              "      <td>Norm</td>\n",
              "      <td>1Fam</td>\n",
              "      <td>2Story</td>\n",
              "      <td>7</td>\n",
              "      <td>5</td>\n",
              "      <td>1915</td>\n",
              "      <td>1970</td>\n",
              "      <td>Gable</td>\n",
              "      <td>CompShg</td>\n",
              "      <td>Wd Sdng</td>\n",
              "      <td>Wd Shng</td>\n",
              "      <td>None</td>\n",
              "      <td>0.0</td>\n",
              "      <td>TA</td>\n",
              "      <td>TA</td>\n",
              "      <td>BrkTil</td>\n",
              "      <td>TA</td>\n",
              "      <td>Gd</td>\n",
              "      <td>No</td>\n",
              "      <td>ALQ</td>\n",
              "      <td>216.0</td>\n",
              "      <td>Unf</td>\n",
              "      <td>0.0</td>\n",
              "      <td>540.0</td>\n",
              "      <td>756.0</td>\n",
              "      <td>GasA</td>\n",
              "      <td>Gd</td>\n",
              "      <td>Y</td>\n",
              "      <td>SBrkr</td>\n",
              "      <td>961</td>\n",
              "      <td>756</td>\n",
              "      <td>0</td>\n",
              "      <td>1717</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>Gd</td>\n",
              "      <td>7</td>\n",
              "      <td>Typ</td>\n",
              "      <td>1</td>\n",
              "      <td>Gd</td>\n",
              "      <td>Detchd</td>\n",
              "      <td>1998.0</td>\n",
              "      <td>Unf</td>\n",
              "      <td>3.0</td>\n",
              "      <td>642.0</td>\n",
              "      <td>TA</td>\n",
              "      <td>TA</td>\n",
              "      <td>Y</td>\n",
              "      <td>0</td>\n",
              "      <td>35</td>\n",
              "      <td>272</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>2006</td>\n",
              "      <td>WD</td>\n",
              "      <td>Abnorml</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>60</td>\n",
              "      <td>RL</td>\n",
              "      <td>84.0</td>\n",
              "      <td>14260</td>\n",
              "      <td>Pave</td>\n",
              "      <td>NaN</td>\n",
              "      <td>IR1</td>\n",
              "      <td>Lvl</td>\n",
              "      <td>AllPub</td>\n",
              "      <td>FR2</td>\n",
              "      <td>Gtl</td>\n",
              "      <td>NoRidge</td>\n",
              "      <td>Norm</td>\n",
              "      <td>Norm</td>\n",
              "      <td>1Fam</td>\n",
              "      <td>2Story</td>\n",
              "      <td>8</td>\n",
              "      <td>5</td>\n",
              "      <td>2000</td>\n",
              "      <td>2000</td>\n",
              "      <td>Gable</td>\n",
              "      <td>CompShg</td>\n",
              "      <td>VinylSd</td>\n",
              "      <td>VinylSd</td>\n",
              "      <td>BrkFace</td>\n",
              "      <td>350.0</td>\n",
              "      <td>Gd</td>\n",
              "      <td>TA</td>\n",
              "      <td>PConc</td>\n",
              "      <td>Gd</td>\n",
              "      <td>TA</td>\n",
              "      <td>Av</td>\n",
              "      <td>GLQ</td>\n",
              "      <td>655.0</td>\n",
              "      <td>Unf</td>\n",
              "      <td>0.0</td>\n",
              "      <td>490.0</td>\n",
              "      <td>1145.0</td>\n",
              "      <td>GasA</td>\n",
              "      <td>Ex</td>\n",
              "      <td>Y</td>\n",
              "      <td>SBrkr</td>\n",
              "      <td>1145</td>\n",
              "      <td>1053</td>\n",
              "      <td>0</td>\n",
              "      <td>2198</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>Gd</td>\n",
              "      <td>9</td>\n",
              "      <td>Typ</td>\n",
              "      <td>1</td>\n",
              "      <td>TA</td>\n",
              "      <td>Attchd</td>\n",
              "      <td>2000.0</td>\n",
              "      <td>RFn</td>\n",
              "      <td>3.0</td>\n",
              "      <td>836.0</td>\n",
              "      <td>TA</td>\n",
              "      <td>TA</td>\n",
              "      <td>Y</td>\n",
              "      <td>192</td>\n",
              "      <td>84</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>12</td>\n",
              "      <td>2008</td>\n",
              "      <td>WD</td>\n",
              "      <td>Normal</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    MSSubClass MSZoning  LotFrontage  ...  YrSold SaleType SaleCondition\n",
              "Id                                    ...                               \n",
              "1           60       RL         65.0  ...    2008       WD        Normal\n",
              "2           20       RL         80.0  ...    2007       WD        Normal\n",
              "3           60       RL         68.0  ...    2008       WD        Normal\n",
              "4           70       RL         60.0  ...    2006       WD       Abnorml\n",
              "5           60       RL         84.0  ...    2008       WD        Normal\n",
              "\n",
              "[5 rows x 79 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "e855caca0dd4c3318ebe493114e7ebe8d3333fdd",
        "id": "9BhNwNRYHbex",
        "colab_type": "code",
        "outputId": "2e09a079-9054-4d20-d1c6-b3f5d8f61fe6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 183
        }
      },
      "source": [
        "# Categorical Feature\n",
        "cat_feats = all_data.dtypes[all_data.dtypes == \"object\"].index\n",
        "cat_feats"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['MSZoning', 'Street', 'Alley', 'LotShape', 'LandContour', 'Utilities',\n",
              "       'LotConfig', 'LandSlope', 'Neighborhood', 'Condition1', 'Condition2',\n",
              "       'BldgType', 'HouseStyle', 'RoofStyle', 'RoofMatl', 'Exterior1st',\n",
              "       'Exterior2nd', 'MasVnrType', 'ExterQual', 'ExterCond', 'Foundation',\n",
              "       'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2',\n",
              "       'Heating', 'HeatingQC', 'CentralAir', 'Electrical', 'KitchenQual',\n",
              "       'Functional', 'FireplaceQu', 'GarageType', 'GarageFinish', 'GarageQual',\n",
              "       'GarageCond', 'PavedDrive', 'PoolQC', 'Fence', 'MiscFeature',\n",
              "       'SaleType', 'SaleCondition'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "860ce534626177f29ec5e5268e74dff4a3f722f7",
        "id": "PGTmviU-Hbe1",
        "colab_type": "code",
        "outputId": "4fa7945f-02c7-402a-932d-f3e827311c58",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 247
        }
      },
      "source": [
        "all_data[cat_feats].head()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>MSZoning</th>\n",
              "      <th>Street</th>\n",
              "      <th>Alley</th>\n",
              "      <th>LotShape</th>\n",
              "      <th>LandContour</th>\n",
              "      <th>Utilities</th>\n",
              "      <th>LotConfig</th>\n",
              "      <th>LandSlope</th>\n",
              "      <th>Neighborhood</th>\n",
              "      <th>Condition1</th>\n",
              "      <th>Condition2</th>\n",
              "      <th>BldgType</th>\n",
              "      <th>HouseStyle</th>\n",
              "      <th>RoofStyle</th>\n",
              "      <th>RoofMatl</th>\n",
              "      <th>Exterior1st</th>\n",
              "      <th>Exterior2nd</th>\n",
              "      <th>MasVnrType</th>\n",
              "      <th>ExterQual</th>\n",
              "      <th>ExterCond</th>\n",
              "      <th>Foundation</th>\n",
              "      <th>BsmtQual</th>\n",
              "      <th>BsmtCond</th>\n",
              "      <th>BsmtExposure</th>\n",
              "      <th>BsmtFinType1</th>\n",
              "      <th>BsmtFinType2</th>\n",
              "      <th>Heating</th>\n",
              "      <th>HeatingQC</th>\n",
              "      <th>CentralAir</th>\n",
              "      <th>Electrical</th>\n",
              "      <th>KitchenQual</th>\n",
              "      <th>Functional</th>\n",
              "      <th>FireplaceQu</th>\n",
              "      <th>GarageType</th>\n",
              "      <th>GarageFinish</th>\n",
              "      <th>GarageQual</th>\n",
              "      <th>GarageCond</th>\n",
              "      <th>PavedDrive</th>\n",
              "      <th>PoolQC</th>\n",
              "      <th>Fence</th>\n",
              "      <th>MiscFeature</th>\n",
              "      <th>SaleType</th>\n",
              "      <th>SaleCondition</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Id</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>RL</td>\n",
              "      <td>Pave</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Reg</td>\n",
              "      <td>Lvl</td>\n",
              "      <td>AllPub</td>\n",
              "      <td>Inside</td>\n",
              "      <td>Gtl</td>\n",
              "      <td>CollgCr</td>\n",
              "      <td>Norm</td>\n",
              "      <td>Norm</td>\n",
              "      <td>1Fam</td>\n",
              "      <td>2Story</td>\n",
              "      <td>Gable</td>\n",
              "      <td>CompShg</td>\n",
              "      <td>VinylSd</td>\n",
              "      <td>VinylSd</td>\n",
              "      <td>BrkFace</td>\n",
              "      <td>Gd</td>\n",
              "      <td>TA</td>\n",
              "      <td>PConc</td>\n",
              "      <td>Gd</td>\n",
              "      <td>TA</td>\n",
              "      <td>No</td>\n",
              "      <td>GLQ</td>\n",
              "      <td>Unf</td>\n",
              "      <td>GasA</td>\n",
              "      <td>Ex</td>\n",
              "      <td>Y</td>\n",
              "      <td>SBrkr</td>\n",
              "      <td>Gd</td>\n",
              "      <td>Typ</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Attchd</td>\n",
              "      <td>RFn</td>\n",
              "      <td>TA</td>\n",
              "      <td>TA</td>\n",
              "      <td>Y</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>WD</td>\n",
              "      <td>Normal</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>RL</td>\n",
              "      <td>Pave</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Reg</td>\n",
              "      <td>Lvl</td>\n",
              "      <td>AllPub</td>\n",
              "      <td>FR2</td>\n",
              "      <td>Gtl</td>\n",
              "      <td>Veenker</td>\n",
              "      <td>Feedr</td>\n",
              "      <td>Norm</td>\n",
              "      <td>1Fam</td>\n",
              "      <td>1Story</td>\n",
              "      <td>Gable</td>\n",
              "      <td>CompShg</td>\n",
              "      <td>MetalSd</td>\n",
              "      <td>MetalSd</td>\n",
              "      <td>None</td>\n",
              "      <td>TA</td>\n",
              "      <td>TA</td>\n",
              "      <td>CBlock</td>\n",
              "      <td>Gd</td>\n",
              "      <td>TA</td>\n",
              "      <td>Gd</td>\n",
              "      <td>ALQ</td>\n",
              "      <td>Unf</td>\n",
              "      <td>GasA</td>\n",
              "      <td>Ex</td>\n",
              "      <td>Y</td>\n",
              "      <td>SBrkr</td>\n",
              "      <td>TA</td>\n",
              "      <td>Typ</td>\n",
              "      <td>TA</td>\n",
              "      <td>Attchd</td>\n",
              "      <td>RFn</td>\n",
              "      <td>TA</td>\n",
              "      <td>TA</td>\n",
              "      <td>Y</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>WD</td>\n",
              "      <td>Normal</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>RL</td>\n",
              "      <td>Pave</td>\n",
              "      <td>NaN</td>\n",
              "      <td>IR1</td>\n",
              "      <td>Lvl</td>\n",
              "      <td>AllPub</td>\n",
              "      <td>Inside</td>\n",
              "      <td>Gtl</td>\n",
              "      <td>CollgCr</td>\n",
              "      <td>Norm</td>\n",
              "      <td>Norm</td>\n",
              "      <td>1Fam</td>\n",
              "      <td>2Story</td>\n",
              "      <td>Gable</td>\n",
              "      <td>CompShg</td>\n",
              "      <td>VinylSd</td>\n",
              "      <td>VinylSd</td>\n",
              "      <td>BrkFace</td>\n",
              "      <td>Gd</td>\n",
              "      <td>TA</td>\n",
              "      <td>PConc</td>\n",
              "      <td>Gd</td>\n",
              "      <td>TA</td>\n",
              "      <td>Mn</td>\n",
              "      <td>GLQ</td>\n",
              "      <td>Unf</td>\n",
              "      <td>GasA</td>\n",
              "      <td>Ex</td>\n",
              "      <td>Y</td>\n",
              "      <td>SBrkr</td>\n",
              "      <td>Gd</td>\n",
              "      <td>Typ</td>\n",
              "      <td>TA</td>\n",
              "      <td>Attchd</td>\n",
              "      <td>RFn</td>\n",
              "      <td>TA</td>\n",
              "      <td>TA</td>\n",
              "      <td>Y</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>WD</td>\n",
              "      <td>Normal</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>RL</td>\n",
              "      <td>Pave</td>\n",
              "      <td>NaN</td>\n",
              "      <td>IR1</td>\n",
              "      <td>Lvl</td>\n",
              "      <td>AllPub</td>\n",
              "      <td>Corner</td>\n",
              "      <td>Gtl</td>\n",
              "      <td>Crawfor</td>\n",
              "      <td>Norm</td>\n",
              "      <td>Norm</td>\n",
              "      <td>1Fam</td>\n",
              "      <td>2Story</td>\n",
              "      <td>Gable</td>\n",
              "      <td>CompShg</td>\n",
              "      <td>Wd Sdng</td>\n",
              "      <td>Wd Shng</td>\n",
              "      <td>None</td>\n",
              "      <td>TA</td>\n",
              "      <td>TA</td>\n",
              "      <td>BrkTil</td>\n",
              "      <td>TA</td>\n",
              "      <td>Gd</td>\n",
              "      <td>No</td>\n",
              "      <td>ALQ</td>\n",
              "      <td>Unf</td>\n",
              "      <td>GasA</td>\n",
              "      <td>Gd</td>\n",
              "      <td>Y</td>\n",
              "      <td>SBrkr</td>\n",
              "      <td>Gd</td>\n",
              "      <td>Typ</td>\n",
              "      <td>Gd</td>\n",
              "      <td>Detchd</td>\n",
              "      <td>Unf</td>\n",
              "      <td>TA</td>\n",
              "      <td>TA</td>\n",
              "      <td>Y</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>WD</td>\n",
              "      <td>Abnorml</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>RL</td>\n",
              "      <td>Pave</td>\n",
              "      <td>NaN</td>\n",
              "      <td>IR1</td>\n",
              "      <td>Lvl</td>\n",
              "      <td>AllPub</td>\n",
              "      <td>FR2</td>\n",
              "      <td>Gtl</td>\n",
              "      <td>NoRidge</td>\n",
              "      <td>Norm</td>\n",
              "      <td>Norm</td>\n",
              "      <td>1Fam</td>\n",
              "      <td>2Story</td>\n",
              "      <td>Gable</td>\n",
              "      <td>CompShg</td>\n",
              "      <td>VinylSd</td>\n",
              "      <td>VinylSd</td>\n",
              "      <td>BrkFace</td>\n",
              "      <td>Gd</td>\n",
              "      <td>TA</td>\n",
              "      <td>PConc</td>\n",
              "      <td>Gd</td>\n",
              "      <td>TA</td>\n",
              "      <td>Av</td>\n",
              "      <td>GLQ</td>\n",
              "      <td>Unf</td>\n",
              "      <td>GasA</td>\n",
              "      <td>Ex</td>\n",
              "      <td>Y</td>\n",
              "      <td>SBrkr</td>\n",
              "      <td>Gd</td>\n",
              "      <td>Typ</td>\n",
              "      <td>TA</td>\n",
              "      <td>Attchd</td>\n",
              "      <td>RFn</td>\n",
              "      <td>TA</td>\n",
              "      <td>TA</td>\n",
              "      <td>Y</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>WD</td>\n",
              "      <td>Normal</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   MSZoning Street Alley LotShape  ... Fence MiscFeature SaleType SaleCondition\n",
              "Id                                 ...                                         \n",
              "1        RL   Pave   NaN      Reg  ...   NaN         NaN       WD        Normal\n",
              "2        RL   Pave   NaN      Reg  ...   NaN         NaN       WD        Normal\n",
              "3        RL   Pave   NaN      IR1  ...   NaN         NaN       WD        Normal\n",
              "4        RL   Pave   NaN      IR1  ...   NaN         NaN       WD       Abnorml\n",
              "5        RL   Pave   NaN      IR1  ...   NaN         NaN       WD        Normal\n",
              "\n",
              "[5 rows x 43 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "d35018cd07229c21e50989be3f5a2b282e1f5be1",
        "id": "QfooNfoUHbe4",
        "colab_type": "code",
        "outputId": "3e003097-345b-42b0-f823-b144af7c25f4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 166
        }
      },
      "source": [
        "# Numeric Feature\n",
        "numeric_feats = all_data.dtypes[all_data.dtypes != \"object\"].index\n",
        "numeric_feats"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['MSSubClass', 'LotFrontage', 'LotArea', 'OverallQual', 'OverallCond',\n",
              "       'YearBuilt', 'YearRemodAdd', 'MasVnrArea', 'BsmtFinSF1', 'BsmtFinSF2',\n",
              "       'BsmtUnfSF', 'TotalBsmtSF', '1stFlrSF', '2ndFlrSF', 'LowQualFinSF',\n",
              "       'GrLivArea', 'BsmtFullBath', 'BsmtHalfBath', 'FullBath', 'HalfBath',\n",
              "       'BedroomAbvGr', 'KitchenAbvGr', 'TotRmsAbvGrd', 'Fireplaces',\n",
              "       'GarageYrBlt', 'GarageCars', 'GarageArea', 'WoodDeckSF', 'OpenPorchSF',\n",
              "       'EnclosedPorch', '3SsnPorch', 'ScreenPorch', 'PoolArea', 'MiscVal',\n",
              "       'MoSold', 'YrSold'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "c6882a5093880b95e3cc2cc60b60c276dddabf35",
        "id": "f5Kodmy5Hbe8",
        "colab_type": "code",
        "outputId": "cf082466-83a1-4c28-9e94-d833a2964163",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 247
        }
      },
      "source": [
        "all_data[numeric_feats].head()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>MSSubClass</th>\n",
              "      <th>LotFrontage</th>\n",
              "      <th>LotArea</th>\n",
              "      <th>OverallQual</th>\n",
              "      <th>OverallCond</th>\n",
              "      <th>YearBuilt</th>\n",
              "      <th>YearRemodAdd</th>\n",
              "      <th>MasVnrArea</th>\n",
              "      <th>BsmtFinSF1</th>\n",
              "      <th>BsmtFinSF2</th>\n",
              "      <th>BsmtUnfSF</th>\n",
              "      <th>TotalBsmtSF</th>\n",
              "      <th>1stFlrSF</th>\n",
              "      <th>2ndFlrSF</th>\n",
              "      <th>LowQualFinSF</th>\n",
              "      <th>GrLivArea</th>\n",
              "      <th>BsmtFullBath</th>\n",
              "      <th>BsmtHalfBath</th>\n",
              "      <th>FullBath</th>\n",
              "      <th>HalfBath</th>\n",
              "      <th>BedroomAbvGr</th>\n",
              "      <th>KitchenAbvGr</th>\n",
              "      <th>TotRmsAbvGrd</th>\n",
              "      <th>Fireplaces</th>\n",
              "      <th>GarageYrBlt</th>\n",
              "      <th>GarageCars</th>\n",
              "      <th>GarageArea</th>\n",
              "      <th>WoodDeckSF</th>\n",
              "      <th>OpenPorchSF</th>\n",
              "      <th>EnclosedPorch</th>\n",
              "      <th>3SsnPorch</th>\n",
              "      <th>ScreenPorch</th>\n",
              "      <th>PoolArea</th>\n",
              "      <th>MiscVal</th>\n",
              "      <th>MoSold</th>\n",
              "      <th>YrSold</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Id</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>60</td>\n",
              "      <td>65.0</td>\n",
              "      <td>8450</td>\n",
              "      <td>7</td>\n",
              "      <td>5</td>\n",
              "      <td>2003</td>\n",
              "      <td>2003</td>\n",
              "      <td>196.0</td>\n",
              "      <td>706.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>150.0</td>\n",
              "      <td>856.0</td>\n",
              "      <td>856</td>\n",
              "      <td>854</td>\n",
              "      <td>0</td>\n",
              "      <td>1710</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>8</td>\n",
              "      <td>0</td>\n",
              "      <td>2003.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>548.0</td>\n",
              "      <td>0</td>\n",
              "      <td>61</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>2008</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>20</td>\n",
              "      <td>80.0</td>\n",
              "      <td>9600</td>\n",
              "      <td>6</td>\n",
              "      <td>8</td>\n",
              "      <td>1976</td>\n",
              "      <td>1976</td>\n",
              "      <td>0.0</td>\n",
              "      <td>978.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>284.0</td>\n",
              "      <td>1262.0</td>\n",
              "      <td>1262</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1262</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>1976.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>460.0</td>\n",
              "      <td>298</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>2007</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>60</td>\n",
              "      <td>68.0</td>\n",
              "      <td>11250</td>\n",
              "      <td>7</td>\n",
              "      <td>5</td>\n",
              "      <td>2001</td>\n",
              "      <td>2002</td>\n",
              "      <td>162.0</td>\n",
              "      <td>486.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>434.0</td>\n",
              "      <td>920.0</td>\n",
              "      <td>920</td>\n",
              "      <td>866</td>\n",
              "      <td>0</td>\n",
              "      <td>1786</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>2001.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>608.0</td>\n",
              "      <td>0</td>\n",
              "      <td>42</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>9</td>\n",
              "      <td>2008</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>70</td>\n",
              "      <td>60.0</td>\n",
              "      <td>9550</td>\n",
              "      <td>7</td>\n",
              "      <td>5</td>\n",
              "      <td>1915</td>\n",
              "      <td>1970</td>\n",
              "      <td>0.0</td>\n",
              "      <td>216.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>540.0</td>\n",
              "      <td>756.0</td>\n",
              "      <td>961</td>\n",
              "      <td>756</td>\n",
              "      <td>0</td>\n",
              "      <td>1717</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>1998.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>642.0</td>\n",
              "      <td>0</td>\n",
              "      <td>35</td>\n",
              "      <td>272</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>2006</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>60</td>\n",
              "      <td>84.0</td>\n",
              "      <td>14260</td>\n",
              "      <td>8</td>\n",
              "      <td>5</td>\n",
              "      <td>2000</td>\n",
              "      <td>2000</td>\n",
              "      <td>350.0</td>\n",
              "      <td>655.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>490.0</td>\n",
              "      <td>1145.0</td>\n",
              "      <td>1145</td>\n",
              "      <td>1053</td>\n",
              "      <td>0</td>\n",
              "      <td>2198</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>9</td>\n",
              "      <td>1</td>\n",
              "      <td>2000.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>836.0</td>\n",
              "      <td>192</td>\n",
              "      <td>84</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>12</td>\n",
              "      <td>2008</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    MSSubClass  LotFrontage  LotArea  ...  MiscVal  MoSold  YrSold\n",
              "Id                                    ...                         \n",
              "1           60         65.0     8450  ...        0       2    2008\n",
              "2           20         80.0     9600  ...        0       5    2007\n",
              "3           60         68.0    11250  ...        0       9    2008\n",
              "4           70         60.0     9550  ...        0       2    2006\n",
              "5           60         84.0    14260  ...        0      12    2008\n",
              "\n",
              "[5 rows x 36 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "b080a9f9b488e9efc644f1436325fbb66fb5c14e",
        "id": "omzmSAeEHbe_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Ordinal Feature\n",
        "ordinal_features = ['YrSold']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "0f71ddf54c312d8d0b14289657770429688c2562",
        "id": "F82LJVSaHbfD",
        "colab_type": "text"
      },
      "source": [
        "## Preprocessing and Data Transformation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "e47049918da2af1b3f5b73ce437419042711923b",
        "trusted": true,
        "id": "0BbJw4tFHbfE",
        "colab_type": "code",
        "outputId": "5d4caa9b-5157-4cd9-aa25-f13bba2fc4fd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 116
        }
      },
      "source": [
        "#log transform skewed numeric features:\n",
        "\n",
        "skewed_feats = train[numeric_feats].apply(lambda x: skew(x.dropna())) #compute skewness\n",
        "skewed_feats = skewed_feats[skewed_feats > 0.75]\n",
        "skewed_feats = skewed_feats.index\n",
        "print(skewed_feats)\n",
        "\n",
        "all_data[skewed_feats] = np.log1p(all_data[skewed_feats])"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Index(['MSSubClass', 'LotFrontage', 'LotArea', 'MasVnrArea', 'BsmtFinSF1',\n",
            "       'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', '1stFlrSF', '2ndFlrSF',\n",
            "       'LowQualFinSF', 'GrLivArea', 'BsmtHalfBath', 'KitchenAbvGr',\n",
            "       'WoodDeckSF', 'OpenPorchSF', 'EnclosedPorch', '3SsnPorch',\n",
            "       'ScreenPorch', 'PoolArea', 'MiscVal'],\n",
            "      dtype='object')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "b22750f97d9c9cdbe1a26c6a93c291d0c6429f37",
        "id": "zvC5ZnrEHbfJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#log transform the target:\n",
        "train[\"SalePrice\"] = np.log1p(train[\"SalePrice\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "0cfa7ad062368e62123130874b21cbee23c59917",
        "trusted": false,
        "id": "jEFG8Ey7HbfM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# One Hot Encoder\n",
        "all_data = pd.get_dummies(all_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "f31aa08dfe11975e8a0c966bb3ee6e44b93d096c",
        "trusted": false,
        "id": "r28w7g-bHbfQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#filling NA's with the mean of the column:\n",
        "all_data = all_data.fillna(all_data.mean())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "b0c84fa58124b81a8b3e7bd5a207544a34d250e4",
        "trusted": false,
        "id": "Tr9v5LBLHbfT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#creating matrices for sklearn:\n",
        "X_train = all_data[:train.shape[0]]\n",
        "X_test = all_data[train.shape[0]:]\n",
        "y = train.SalePrice"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "8ba692db957325a6f409cfd7202f1eebe84acd3d",
        "trusted": false,
        "id": "FzxgWQArHbfZ",
        "colab_type": "code",
        "outputId": "ba672840-505d-42f5-99c8-b0dad61e7a6f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "X_train = StandardScaler().fit_transform(X_train)\n",
        "X_tr, X_val, y_tr, y_val = train_test_split(X_train, y, random_state = 3)\n",
        "X_tr.shape"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1095, 288)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7VZriE6_NZi0",
        "colab_type": "text"
      },
      "source": [
        "Data obtained as input for neural network to train."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "_uuid": "266489367ab6bb0553b7fbf4c4af9e1a050fbca7",
        "trusted": false,
        "id": "aePEF55fHbfd",
        "colab_type": "code",
        "outputId": "3b5364c2-9f70-4336-dd15-4f508f4be048",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 233
        }
      },
      "source": [
        "X_tr"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 1.00573733,  0.68066137, -0.46001991, ..., -0.11785113,\n",
              "         0.4676514 , -0.30599503],\n",
              "       [-1.12520184,  0.60296111,  0.03113183, ..., -0.11785113,\n",
              "         0.4676514 , -0.30599503],\n",
              "       [-1.12520184, -0.02865265, -0.74027492, ..., -0.11785113,\n",
              "         0.4676514 , -0.30599503],\n",
              "       ...,\n",
              "       [ 0.16426234, -0.87075036, -0.81954431, ..., -0.11785113,\n",
              "        -2.13834494, -0.30599503],\n",
              "       [ 0.92361154, -0.30038284, -0.44275864, ..., -0.11785113,\n",
              "         0.4676514 , -0.30599503],\n",
              "       [ 0.83656519,  1.98505948,  0.46455838, ..., -0.11785113,\n",
              "         0.4676514 , -0.30599503]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "b178af2c10f2b5e0443995d2b069765119c2aa9a",
        "trusted": false,
        "id": "EKvKRTPtHbfk",
        "colab_type": "code",
        "outputId": "57b73949-a675-4df8-c72a-8aa4b557e09e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 233
        }
      },
      "source": [
        "X_val"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-1.12520184,  0.18320317,  0.28666241, ..., -0.11785113,\n",
              "         0.4676514 , -0.30599503],\n",
              "       [-1.12520184, -0.30038284, -0.44275864, ..., -0.11785113,\n",
              "         0.4676514 , -0.30599503],\n",
              "       [ 0.42446233,  0.27169643,  0.43152914, ..., -0.11785113,\n",
              "         0.4676514 , -0.30599503],\n",
              "       ...,\n",
              "       [ 0.64507285, -0.5728182 ,  0.30352832, ..., -0.11785113,\n",
              "         0.4676514 , -0.30599503],\n",
              "       [ 0.64507285, -0.30038284,  0.11341289, ..., -0.11785113,\n",
              "         0.4676514 , -0.30599503],\n",
              "       [ 0.64507285, -0.30038284, -1.78270316, ..., -0.11785113,\n",
              "         0.4676514 , -0.30599503]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "7eb6fb2ea6a6431c49da82d2e79bb2bb54cfe6d3",
        "id": "t-rXfb0jHbfo",
        "colab_type": "text"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "e1a393ace5837a04fe6873895226c970b22ef19d",
        "trusted": false,
        "id": "t0kp746AHbfp",
        "colab_type": "code",
        "outputId": "602564c6-6907-48fd-a844-9f3e34e4a1e6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "#Model1\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(1, input_dim = X_train.shape[1], W_regularizer=l1(0.001)))\n",
        "model.compile(loss = \"mse\", optimizer = \"adam\")"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "862c05647fe35d6cec5d843a90f26a41a419e5e3",
        "trusted": false,
        "id": "F9RA4fPeHbfu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Model2\n",
        "\n",
        "model = Sequential()\n",
        "BatchNormalization()\n",
        "model.add(Dense(1028,input_dim=288,activation='relu'))\n",
        "BatchNormalization()\n",
        "model.add(Dense(1028,input_dim=288,activation='relu'))\n",
        "BatchNormalization()\n",
        "model.add(Dense(100,input_dim=288,activation='relu'))\n",
        "BatchNormalization()\n",
        "model.add(Dense(50))\n",
        "BatchNormalization()\n",
        "model.add(Dense(1))\n",
        "model.compile(optimizer='adam',loss='mse',metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "fa6fb86ae8044dd296da307b8c671c3316c41493",
        "trusted": false,
        "id": "iIfFgNilHbf0",
        "colab_type": "code",
        "outputId": "9d440ed0-c14e-4dcd-aa07-22576a171df0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 316
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_2 (Dense)              (None, 1028)              297092    \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 1028)              1057812   \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 100)               102900    \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 50)                5050      \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 1)                 51        \n",
            "=================================================================\n",
            "Total params: 1,462,905\n",
            "Trainable params: 1,462,905\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9bsv05jeOt9O",
        "colab_type": "code",
        "outputId": "6efa22b5-77fe-4921-b015-a8f148cc1977",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "x_ax = np.arange(1,16,1)\n",
        "y_ax = np.arange(1,150,10)\n",
        "\n",
        "len(x_ax), len(y_ax)"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(15, 15)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "cba6e951fac8bcdf48c9748ddda0b53e8ac0933a",
        "trusted": false,
        "id": "QUnA1pVcHbf5",
        "colab_type": "code",
        "outputId": "1bbafaae-188c-466c-eb86-4795588ec952",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "scores = []\n",
        "\n",
        "for ep in x_ax:\n",
        "  for bat in y_ax:\n",
        "    hist = model.fit(X_tr, y_tr, validation_data = (X_val, y_val), epochs = ep, batch_size = bat)\n",
        "    \n",
        "    scores.append(np.sqrt(model.evaluate(X_val,y_val,verbose=0))[0])\n",
        "    print(ep)\n",
        "    print(bat)"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 1095 samples, validate on 365 samples\n",
            "Epoch 1/1\n",
            "1095/1095 [==============================] - 8s 7ms/step - loss: 0.1858 - acc: 0.0000e+00 - val_loss: 0.1448 - val_acc: 0.0000e+00\n",
            "1\n",
            "1\n",
            "Train on 1095 samples, validate on 365 samples\n",
            "Epoch 1/1\n",
            "1095/1095 [==============================] - 1s 779us/step - loss: 0.1687 - acc: 0.0000e+00 - val_loss: 0.1405 - val_acc: 0.0000e+00\n",
            "1\n",
            "11\n",
            "Train on 1095 samples, validate on 365 samples\n",
            "Epoch 1/1\n",
            "1095/1095 [==============================] - 0s 415us/step - loss: 0.1672 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "1\n",
            "21\n",
            "Train on 1095 samples, validate on 365 samples\n",
            "Epoch 1/1\n",
            "1095/1095 [==============================] - 0s 281us/step - loss: 0.1667 - acc: 0.0000e+00 - val_loss: 0.1388 - val_acc: 0.0000e+00\n",
            "1\n",
            "31\n",
            "Train on 1095 samples, validate on 365 samples\n",
            "Epoch 1/1\n",
            "1095/1095 [==============================] - 0s 212us/step - loss: 0.1679 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "1\n",
            "41\n",
            "Train on 1095 samples, validate on 365 samples\n",
            "Epoch 1/1\n",
            "1095/1095 [==============================] - 0s 188us/step - loss: 0.1667 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "1\n",
            "51\n",
            "Train on 1095 samples, validate on 365 samples\n",
            "Epoch 1/1\n",
            "1095/1095 [==============================] - 0s 143us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "1\n",
            "61\n",
            "Train on 1095 samples, validate on 365 samples\n",
            "Epoch 1/1\n",
            "1095/1095 [==============================] - 0s 137us/step - loss: 0.1667 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "1\n",
            "71\n",
            "Train on 1095 samples, validate on 365 samples\n",
            "Epoch 1/1\n",
            "1095/1095 [==============================] - 0s 121us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "1\n",
            "81\n",
            "Train on 1095 samples, validate on 365 samples\n",
            "Epoch 1/1\n",
            "1095/1095 [==============================] - 0s 108us/step - loss: 0.1665 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "1\n",
            "91\n",
            "Train on 1095 samples, validate on 365 samples\n",
            "Epoch 1/1\n",
            "1095/1095 [==============================] - 0s 99us/step - loss: 0.1669 - acc: 0.0000e+00 - val_loss: 0.1387 - val_acc: 0.0000e+00\n",
            "1\n",
            "101\n",
            "Train on 1095 samples, validate on 365 samples\n",
            "Epoch 1/1\n",
            "1095/1095 [==============================] - 0s 87us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "1\n",
            "111\n",
            "Train on 1095 samples, validate on 365 samples\n",
            "Epoch 1/1\n",
            "1095/1095 [==============================] - 0s 88us/step - loss: 0.1665 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "1\n",
            "121\n",
            "Train on 1095 samples, validate on 365 samples\n",
            "Epoch 1/1\n",
            "1095/1095 [==============================] - 0s 82us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "1\n",
            "131\n",
            "Train on 1095 samples, validate on 365 samples\n",
            "Epoch 1/1\n",
            "1095/1095 [==============================] - 0s 76us/step - loss: 0.1665 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "1\n",
            "141\n",
            "Train on 1095 samples, validate on 365 samples\n",
            "Epoch 1/2\n",
            "1095/1095 [==============================] - 8s 7ms/step - loss: 0.1786 - acc: 0.0000e+00 - val_loss: 0.1400 - val_acc: 0.0000e+00\n",
            "Epoch 2/2\n",
            "1095/1095 [==============================] - 8s 7ms/step - loss: 0.1786 - acc: 0.0000e+00 - val_loss: 0.1388 - val_acc: 0.0000e+00\n",
            "2\n",
            "1\n",
            "Train on 1095 samples, validate on 365 samples\n",
            "Epoch 1/2\n",
            "1095/1095 [==============================] - 1s 763us/step - loss: 0.1676 - acc: 0.0000e+00 - val_loss: 0.1405 - val_acc: 0.0000e+00\n",
            "Epoch 2/2\n",
            "1095/1095 [==============================] - 1s 761us/step - loss: 0.1677 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "2\n",
            "11\n",
            "Train on 1095 samples, validate on 365 samples\n",
            "Epoch 1/2\n",
            "1095/1095 [==============================] - 0s 411us/step - loss: 0.1668 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 2/2\n",
            "1095/1095 [==============================] - 0s 410us/step - loss: 0.1669 - acc: 0.0000e+00 - val_loss: 0.1395 - val_acc: 0.0000e+00\n",
            "2\n",
            "21\n",
            "Train on 1095 samples, validate on 365 samples\n",
            "Epoch 1/2\n",
            "1095/1095 [==============================] - 0s 284us/step - loss: 0.1668 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 2/2\n",
            "1095/1095 [==============================] - 0s 282us/step - loss: 0.1669 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "2\n",
            "31\n",
            "Train on 1095 samples, validate on 365 samples\n",
            "Epoch 1/2\n",
            "1095/1095 [==============================] - 0s 210us/step - loss: 0.1668 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "Epoch 2/2\n",
            "1095/1095 [==============================] - 0s 220us/step - loss: 0.1672 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "2\n",
            "41\n",
            "Train on 1095 samples, validate on 365 samples\n",
            "Epoch 1/2\n",
            "1095/1095 [==============================] - 0s 177us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 2/2\n",
            "1095/1095 [==============================] - 0s 194us/step - loss: 0.1665 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "2\n",
            "51\n",
            "Train on 1095 samples, validate on 365 samples\n",
            "Epoch 1/2\n",
            "1095/1095 [==============================] - 0s 142us/step - loss: 0.1668 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 2/2\n",
            "1095/1095 [==============================] - 0s 149us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "2\n",
            "61\n",
            "Train on 1095 samples, validate on 365 samples\n",
            "Epoch 1/2\n",
            "1095/1095 [==============================] - 0s 144us/step - loss: 0.1665 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 2/2\n",
            "1095/1095 [==============================] - 0s 144us/step - loss: 0.1667 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "2\n",
            "71\n",
            "Train on 1095 samples, validate on 365 samples\n",
            "Epoch 1/2\n",
            "1095/1095 [==============================] - 0s 122us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 2/2\n",
            "1095/1095 [==============================] - 0s 118us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "2\n",
            "81\n",
            "Train on 1095 samples, validate on 365 samples\n",
            "Epoch 1/2\n",
            "1095/1095 [==============================] - 0s 105us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 2/2\n",
            "1095/1095 [==============================] - 0s 110us/step - loss: 0.1670 - acc: 0.0000e+00 - val_loss: 0.1389 - val_acc: 0.0000e+00\n",
            "2\n",
            "91\n",
            "Train on 1095 samples, validate on 365 samples\n",
            "Epoch 1/2\n",
            "1095/1095 [==============================] - 0s 96us/step - loss: 0.1672 - acc: 0.0000e+00 - val_loss: 0.1386 - val_acc: 0.0000e+00\n",
            "Epoch 2/2\n",
            "1095/1095 [==============================] - 0s 93us/step - loss: 0.1667 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "2\n",
            "101\n",
            "Train on 1095 samples, validate on 365 samples\n",
            "Epoch 1/2\n",
            "1095/1095 [==============================] - 0s 86us/step - loss: 0.1665 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "Epoch 2/2\n",
            "1095/1095 [==============================] - 0s 87us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "2\n",
            "111\n",
            "Train on 1095 samples, validate on 365 samples\n",
            "Epoch 1/2\n",
            "1095/1095 [==============================] - 0s 87us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 2/2\n",
            "1095/1095 [==============================] - 0s 95us/step - loss: 0.1664 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "2\n",
            "121\n",
            "Train on 1095 samples, validate on 365 samples\n",
            "Epoch 1/2\n",
            "1095/1095 [==============================] - 0s 83us/step - loss: 0.1665 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 2/2\n",
            "1095/1095 [==============================] - 0s 81us/step - loss: 0.1665 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "2\n",
            "131\n",
            "Train on 1095 samples, validate on 365 samples\n",
            "Epoch 1/2\n",
            "1095/1095 [==============================] - 0s 69us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 2/2\n",
            "1095/1095 [==============================] - 0s 83us/step - loss: 0.1665 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "2\n",
            "141\n",
            "Train on 1095 samples, validate on 365 samples\n",
            "Epoch 1/3\n",
            "1095/1095 [==============================] - 8s 7ms/step - loss: 0.1771 - acc: 0.0000e+00 - val_loss: 0.1511 - val_acc: 0.0000e+00\n",
            "Epoch 2/3\n",
            "1095/1095 [==============================] - 8s 7ms/step - loss: 0.1766 - acc: 0.0000e+00 - val_loss: 0.1511 - val_acc: 0.0000e+00\n",
            "Epoch 3/3\n",
            "1095/1095 [==============================] - 8s 7ms/step - loss: 0.1752 - acc: 0.0000e+00 - val_loss: 0.1435 - val_acc: 0.0000e+00\n",
            "3\n",
            "1\n",
            "Train on 1095 samples, validate on 365 samples\n",
            "Epoch 1/3\n",
            "1095/1095 [==============================] - 1s 768us/step - loss: 0.1677 - acc: 0.0000e+00 - val_loss: 0.1386 - val_acc: 0.0000e+00\n",
            "Epoch 2/3\n",
            "1095/1095 [==============================] - 1s 781us/step - loss: 0.1682 - acc: 0.0000e+00 - val_loss: 0.1392 - val_acc: 0.0000e+00\n",
            "Epoch 3/3\n",
            "1095/1095 [==============================] - 1s 798us/step - loss: 0.1675 - acc: 0.0000e+00 - val_loss: 0.1387 - val_acc: 0.0000e+00\n",
            "3\n",
            "11\n",
            "Train on 1095 samples, validate on 365 samples\n",
            "Epoch 1/3\n",
            "1095/1095 [==============================] - 0s 423us/step - loss: 0.1676 - acc: 0.0000e+00 - val_loss: 0.1387 - val_acc: 0.0000e+00\n",
            "Epoch 2/3\n",
            "1095/1095 [==============================] - 0s 421us/step - loss: 0.1670 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 3/3\n",
            "1095/1095 [==============================] - 1s 464us/step - loss: 0.1674 - acc: 0.0000e+00 - val_loss: 0.1387 - val_acc: 0.0000e+00\n",
            "3\n",
            "21\n",
            "Train on 1095 samples, validate on 365 samples\n",
            "Epoch 1/3\n",
            "1095/1095 [==============================] - 0s 285us/step - loss: 0.1667 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 2/3\n",
            "1095/1095 [==============================] - 0s 293us/step - loss: 0.1667 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 3/3\n",
            "1095/1095 [==============================] - 0s 274us/step - loss: 0.1669 - acc: 0.0000e+00 - val_loss: 0.1388 - val_acc: 0.0000e+00\n",
            "3\n",
            "31\n",
            "Train on 1095 samples, validate on 365 samples\n",
            "Epoch 1/3\n",
            "1095/1095 [==============================] - 0s 211us/step - loss: 0.1668 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 2/3\n",
            "1095/1095 [==============================] - 0s 210us/step - loss: 0.1670 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 3/3\n",
            "1095/1095 [==============================] - 0s 214us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "3\n",
            "41\n",
            "Train on 1095 samples, validate on 365 samples\n",
            "Epoch 1/3\n",
            "1095/1095 [==============================] - 0s 199us/step - loss: 0.1667 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "Epoch 2/3\n",
            "1095/1095 [==============================] - 0s 207us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 3/3\n",
            "1095/1095 [==============================] - 0s 194us/step - loss: 0.1670 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "3\n",
            "51\n",
            "Train on 1095 samples, validate on 365 samples\n",
            "Epoch 1/3\n",
            "1095/1095 [==============================] - 0s 161us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1386 - val_acc: 0.0000e+00\n",
            "Epoch 2/3\n",
            "1095/1095 [==============================] - 0s 159us/step - loss: 0.1665 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 3/3\n",
            "1095/1095 [==============================] - 0s 165us/step - loss: 0.1668 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "3\n",
            "61\n",
            "Train on 1095 samples, validate on 365 samples\n",
            "Epoch 1/3\n",
            "1095/1095 [==============================] - 0s 149us/step - loss: 0.1669 - acc: 0.0000e+00 - val_loss: 0.1386 - val_acc: 0.0000e+00\n",
            "Epoch 2/3\n",
            "1095/1095 [==============================] - 0s 140us/step - loss: 0.1673 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 3/3\n",
            "1095/1095 [==============================] - 0s 139us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1387 - val_acc: 0.0000e+00\n",
            "3\n",
            "71\n",
            "Train on 1095 samples, validate on 365 samples\n",
            "Epoch 1/3\n",
            "1095/1095 [==============================] - 0s 120us/step - loss: 0.1668 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 2/3\n",
            "1095/1095 [==============================] - 0s 120us/step - loss: 0.1665 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 3/3\n",
            "1095/1095 [==============================] - 0s 121us/step - loss: 0.1665 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "3\n",
            "81\n",
            "Train on 1095 samples, validate on 365 samples\n",
            "Epoch 1/3\n",
            "1095/1095 [==============================] - 0s 112us/step - loss: 0.1667 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 2/3\n",
            "1095/1095 [==============================] - 0s 117us/step - loss: 0.1670 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 3/3\n",
            "1095/1095 [==============================] - 0s 109us/step - loss: 0.1665 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "3\n",
            "91\n",
            "Train on 1095 samples, validate on 365 samples\n",
            "Epoch 1/3\n",
            "1095/1095 [==============================] - 0s 94us/step - loss: 0.1667 - acc: 0.0000e+00 - val_loss: 0.1389 - val_acc: 0.0000e+00\n",
            "Epoch 2/3\n",
            "1095/1095 [==============================] - 0s 93us/step - loss: 0.1669 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 3/3\n",
            "1095/1095 [==============================] - 0s 93us/step - loss: 0.1665 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "3\n",
            "101\n",
            "Train on 1095 samples, validate on 365 samples\n",
            "Epoch 1/3\n",
            "1095/1095 [==============================] - 0s 86us/step - loss: 0.1667 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 2/3\n",
            "1095/1095 [==============================] - 0s 90us/step - loss: 0.1665 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "Epoch 3/3\n",
            "1095/1095 [==============================] - 0s 90us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "3\n",
            "111\n",
            "Train on 1095 samples, validate on 365 samples\n",
            "Epoch 1/3\n",
            "1095/1095 [==============================] - 0s 86us/step - loss: 0.1665 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 2/3\n",
            "1095/1095 [==============================] - 0s 93us/step - loss: 0.1665 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 3/3\n",
            "1095/1095 [==============================] - 0s 87us/step - loss: 0.1665 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "3\n",
            "121\n",
            "Train on 1095 samples, validate on 365 samples\n",
            "Epoch 1/3\n",
            "1095/1095 [==============================] - 0s 77us/step - loss: 0.1665 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 2/3\n",
            "1095/1095 [==============================] - 0s 81us/step - loss: 0.1665 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 3/3\n",
            "1095/1095 [==============================] - 0s 79us/step - loss: 0.1665 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "3\n",
            "131\n",
            "Train on 1095 samples, validate on 365 samples\n",
            "Epoch 1/3\n",
            "1095/1095 [==============================] - 0s 81us/step - loss: 0.1665 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 2/3\n",
            "1095/1095 [==============================] - 0s 79us/step - loss: 0.1665 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 3/3\n",
            "1095/1095 [==============================] - 0s 75us/step - loss: 0.1665 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "3\n",
            "141\n",
            "Train on 1095 samples, validate on 365 samples\n",
            "Epoch 1/4\n",
            "1095/1095 [==============================] - 8s 7ms/step - loss: 0.1776 - acc: 0.0000e+00 - val_loss: 0.1529 - val_acc: 0.0000e+00\n",
            "Epoch 2/4\n",
            "1095/1095 [==============================] - 8s 7ms/step - loss: 0.1757 - acc: 0.0000e+00 - val_loss: 0.1402 - val_acc: 0.0000e+00\n",
            "Epoch 3/4\n",
            "1095/1095 [==============================] - 8s 7ms/step - loss: 0.1773 - acc: 0.0000e+00 - val_loss: 0.1396 - val_acc: 0.0000e+00\n",
            "Epoch 4/4\n",
            "1095/1095 [==============================] - 8s 7ms/step - loss: 0.1781 - acc: 0.0000e+00 - val_loss: 0.1845 - val_acc: 0.0000e+00\n",
            "4\n",
            "1\n",
            "Train on 1095 samples, validate on 365 samples\n",
            "Epoch 1/4\n",
            "1095/1095 [==============================] - 1s 782us/step - loss: 0.1704 - acc: 0.0000e+00 - val_loss: 0.1396 - val_acc: 0.0000e+00\n",
            "Epoch 2/4\n",
            "1095/1095 [==============================] - 1s 772us/step - loss: 0.1686 - acc: 0.0000e+00 - val_loss: 0.1405 - val_acc: 0.0000e+00\n",
            "Epoch 3/4\n",
            "1095/1095 [==============================] - 1s 768us/step - loss: 0.1684 - acc: 0.0000e+00 - val_loss: 0.1386 - val_acc: 0.0000e+00\n",
            "Epoch 4/4\n",
            "1095/1095 [==============================] - 1s 767us/step - loss: 0.1687 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "4\n",
            "11\n",
            "Train on 1095 samples, validate on 365 samples\n",
            "Epoch 1/4\n",
            "1095/1095 [==============================] - 0s 420us/step - loss: 0.1673 - acc: 0.0000e+00 - val_loss: 0.1387 - val_acc: 0.0000e+00\n",
            "Epoch 2/4\n",
            "1095/1095 [==============================] - 0s 420us/step - loss: 0.1672 - acc: 0.0000e+00 - val_loss: 0.1398 - val_acc: 0.0000e+00\n",
            "Epoch 3/4\n",
            "1095/1095 [==============================] - 0s 414us/step - loss: 0.1671 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 4/4\n",
            "1095/1095 [==============================] - 0s 409us/step - loss: 0.1670 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "4\n",
            "21\n",
            "Train on 1095 samples, validate on 365 samples\n",
            "Epoch 1/4\n",
            "1095/1095 [==============================] - 0s 291us/step - loss: 0.1667 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 2/4\n",
            "1095/1095 [==============================] - 0s 279us/step - loss: 0.1670 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 3/4\n",
            "1095/1095 [==============================] - 0s 282us/step - loss: 0.1667 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 4/4\n",
            "1095/1095 [==============================] - 0s 288us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "4\n",
            "31\n",
            "Train on 1095 samples, validate on 365 samples\n",
            "Epoch 1/4\n",
            "1095/1095 [==============================] - 0s 209us/step - loss: 0.1675 - acc: 0.0000e+00 - val_loss: 0.1386 - val_acc: 0.0000e+00\n",
            "Epoch 2/4\n",
            "1095/1095 [==============================] - 0s 221us/step - loss: 0.1668 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 3/4\n",
            "1095/1095 [==============================] - 0s 224us/step - loss: 0.1667 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 4/4\n",
            "1095/1095 [==============================] - 0s 235us/step - loss: 0.1667 - acc: 0.0000e+00 - val_loss: 0.1387 - val_acc: 0.0000e+00\n",
            "4\n",
            "41\n",
            "Train on 1095 samples, validate on 365 samples\n",
            "Epoch 1/4\n",
            "1095/1095 [==============================] - 0s 181us/step - loss: 0.1667 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 2/4\n",
            "1095/1095 [==============================] - 0s 195us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 3/4\n",
            "1095/1095 [==============================] - 0s 175us/step - loss: 0.1669 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 4/4\n",
            "1095/1095 [==============================] - 0s 177us/step - loss: 0.1668 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "4\n",
            "51\n",
            "Train on 1095 samples, validate on 365 samples\n",
            "Epoch 1/4\n",
            "1095/1095 [==============================] - 0s 145us/step - loss: 0.1665 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 2/4\n",
            "1095/1095 [==============================] - 0s 146us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 3/4\n",
            "1095/1095 [==============================] - 0s 140us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 4/4\n",
            "1095/1095 [==============================] - 0s 141us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "4\n",
            "61\n",
            "Train on 1095 samples, validate on 365 samples\n",
            "Epoch 1/4\n",
            "1095/1095 [==============================] - 0s 126us/step - loss: 0.1668 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "Epoch 2/4\n",
            "1095/1095 [==============================] - 0s 136us/step - loss: 0.1667 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 3/4\n",
            "1095/1095 [==============================] - 0s 135us/step - loss: 0.1667 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "Epoch 4/4\n",
            "1095/1095 [==============================] - 0s 134us/step - loss: 0.1665 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "4\n",
            "71\n",
            "Train on 1095 samples, validate on 365 samples\n",
            "Epoch 1/4\n",
            "1095/1095 [==============================] - 0s 123us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 2/4\n",
            "1095/1095 [==============================] - 0s 116us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 3/4\n",
            "1095/1095 [==============================] - 0s 117us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 4/4\n",
            "1095/1095 [==============================] - 0s 117us/step - loss: 0.1665 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "4\n",
            "81\n",
            "Train on 1095 samples, validate on 365 samples\n",
            "Epoch 1/4\n",
            "1095/1095 [==============================] - 0s 105us/step - loss: 0.1665 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 2/4\n",
            "1095/1095 [==============================] - 0s 116us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "Epoch 3/4\n",
            "1095/1095 [==============================] - 0s 108us/step - loss: 0.1670 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "Epoch 4/4\n",
            "1095/1095 [==============================] - 0s 107us/step - loss: 0.1667 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "4\n",
            "91\n",
            "Train on 1095 samples, validate on 365 samples\n",
            "Epoch 1/4\n",
            "1095/1095 [==============================] - 0s 94us/step - loss: 0.1668 - acc: 0.0000e+00 - val_loss: 0.1387 - val_acc: 0.0000e+00\n",
            "Epoch 2/4\n",
            "1095/1095 [==============================] - 0s 95us/step - loss: 0.1667 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 3/4\n",
            "1095/1095 [==============================] - 0s 93us/step - loss: 0.1668 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 4/4\n",
            "1095/1095 [==============================] - 0s 98us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "4\n",
            "101\n",
            "Train on 1095 samples, validate on 365 samples\n",
            "Epoch 1/4\n",
            "1095/1095 [==============================] - 0s 85us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 2/4\n",
            "1095/1095 [==============================] - 0s 88us/step - loss: 0.1665 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 3/4\n",
            "1095/1095 [==============================] - 0s 87us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 4/4\n",
            "1095/1095 [==============================] - 0s 93us/step - loss: 0.1665 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "4\n",
            "111\n",
            "Train on 1095 samples, validate on 365 samples\n",
            "Epoch 1/4\n",
            "1095/1095 [==============================] - 0s 87us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 2/4\n",
            "1095/1095 [==============================] - 0s 86us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "Epoch 3/4\n",
            "1095/1095 [==============================] - 0s 94us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1388 - val_acc: 0.0000e+00\n",
            "Epoch 4/4\n",
            "1095/1095 [==============================] - 0s 93us/step - loss: 0.1672 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "4\n",
            "121\n",
            "Train on 1095 samples, validate on 365 samples\n",
            "Epoch 1/4\n",
            "1095/1095 [==============================] - 0s 81us/step - loss: 0.1665 - acc: 0.0000e+00 - val_loss: 0.1387 - val_acc: 0.0000e+00\n",
            "Epoch 2/4\n",
            "1095/1095 [==============================] - 0s 80us/step - loss: 0.1667 - acc: 0.0000e+00 - val_loss: 0.1386 - val_acc: 0.0000e+00\n",
            "Epoch 3/4\n",
            "1095/1095 [==============================] - 0s 82us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "Epoch 4/4\n",
            "1095/1095 [==============================] - 0s 81us/step - loss: 0.1664 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "4\n",
            "131\n",
            "Train on 1095 samples, validate on 365 samples\n",
            "Epoch 1/4\n",
            "1095/1095 [==============================] - 0s 77us/step - loss: 0.1668 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "Epoch 2/4\n",
            "1095/1095 [==============================] - 0s 88us/step - loss: 0.1667 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 3/4\n",
            "1095/1095 [==============================] - 0s 74us/step - loss: 0.1665 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "Epoch 4/4\n",
            "1095/1095 [==============================] - 0s 73us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "4\n",
            "141\n",
            "Train on 1095 samples, validate on 365 samples\n",
            "Epoch 1/5\n",
            "1095/1095 [==============================] - 8s 7ms/step - loss: 0.1818 - acc: 0.0000e+00 - val_loss: 0.1462 - val_acc: 0.0000e+00\n",
            "Epoch 2/5\n",
            "1095/1095 [==============================] - 8s 7ms/step - loss: 0.1788 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 3/5\n",
            "1095/1095 [==============================] - 8s 7ms/step - loss: 0.1729 - acc: 0.0000e+00 - val_loss: 0.1506 - val_acc: 0.0000e+00\n",
            "Epoch 4/5\n",
            "1095/1095 [==============================] - 8s 7ms/step - loss: 0.1794 - acc: 0.0000e+00 - val_loss: 0.1388 - val_acc: 0.0000e+00\n",
            "Epoch 5/5\n",
            "1095/1095 [==============================] - 8s 7ms/step - loss: 0.1773 - acc: 0.0000e+00 - val_loss: 0.1389 - val_acc: 0.0000e+00\n",
            "5\n",
            "1\n",
            "Train on 1095 samples, validate on 365 samples\n",
            "Epoch 1/5\n",
            "1095/1095 [==============================] - 1s 774us/step - loss: 0.1671 - acc: 0.0000e+00 - val_loss: 0.1393 - val_acc: 0.0000e+00\n",
            "Epoch 2/5\n",
            "1095/1095 [==============================] - 1s 768us/step - loss: 0.1682 - acc: 0.0000e+00 - val_loss: 0.1405 - val_acc: 0.0000e+00\n",
            "Epoch 3/5\n",
            "1095/1095 [==============================] - 1s 746us/step - loss: 0.1676 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 4/5\n",
            "1095/1095 [==============================] - 1s 775us/step - loss: 0.1683 - acc: 0.0000e+00 - val_loss: 0.1395 - val_acc: 0.0000e+00\n",
            "Epoch 5/5\n",
            "1095/1095 [==============================] - 1s 745us/step - loss: 0.1677 - acc: 0.0000e+00 - val_loss: 0.1387 - val_acc: 0.0000e+00\n",
            "5\n",
            "11\n",
            "Train on 1095 samples, validate on 365 samples\n",
            "Epoch 1/5\n",
            "1095/1095 [==============================] - 0s 406us/step - loss: 0.1691 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 2/5\n",
            "1095/1095 [==============================] - 0s 413us/step - loss: 0.1671 - acc: 0.0000e+00 - val_loss: 0.1386 - val_acc: 0.0000e+00\n",
            "Epoch 3/5\n",
            "1095/1095 [==============================] - 0s 393us/step - loss: 0.1669 - acc: 0.0000e+00 - val_loss: 0.1397 - val_acc: 0.0000e+00\n",
            "Epoch 4/5\n",
            "1095/1095 [==============================] - 0s 413us/step - loss: 0.1683 - acc: 0.0000e+00 - val_loss: 0.1391 - val_acc: 0.0000e+00\n",
            "Epoch 5/5\n",
            "1095/1095 [==============================] - 0s 411us/step - loss: 0.1671 - acc: 0.0000e+00 - val_loss: 0.1389 - val_acc: 0.0000e+00\n",
            "5\n",
            "21\n",
            "Train on 1095 samples, validate on 365 samples\n",
            "Epoch 1/5\n",
            "1095/1095 [==============================] - 0s 284us/step - loss: 0.1673 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 2/5\n",
            "1095/1095 [==============================] - 0s 277us/step - loss: 0.1671 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 3/5\n",
            "1095/1095 [==============================] - 0s 273us/step - loss: 0.1670 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 4/5\n",
            "1095/1095 [==============================] - 0s 273us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1387 - val_acc: 0.0000e+00\n",
            "Epoch 5/5\n",
            "1095/1095 [==============================] - 0s 292us/step - loss: 0.1669 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "5\n",
            "31\n",
            "Train on 1095 samples, validate on 365 samples\n",
            "Epoch 1/5\n",
            "1095/1095 [==============================] - 0s 207us/step - loss: 0.1667 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 2/5\n",
            "1095/1095 [==============================] - 0s 207us/step - loss: 0.1668 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 3/5\n",
            "1095/1095 [==============================] - 0s 209us/step - loss: 0.1668 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 4/5\n",
            "1095/1095 [==============================] - 0s 238us/step - loss: 0.1671 - acc: 0.0000e+00 - val_loss: 0.1388 - val_acc: 0.0000e+00\n",
            "Epoch 5/5\n",
            "1095/1095 [==============================] - 0s 226us/step - loss: 0.1669 - acc: 0.0000e+00 - val_loss: 0.1387 - val_acc: 0.0000e+00\n",
            "5\n",
            "41\n",
            "Train on 1095 samples, validate on 365 samples\n",
            "Epoch 1/5\n",
            "1095/1095 [==============================] - 0s 184us/step - loss: 0.1671 - acc: 0.0000e+00 - val_loss: 0.1390 - val_acc: 0.0000e+00\n",
            "Epoch 2/5\n",
            "1095/1095 [==============================] - 0s 191us/step - loss: 0.1668 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 3/5\n",
            "1095/1095 [==============================] - 0s 188us/step - loss: 0.1669 - acc: 0.0000e+00 - val_loss: 0.1387 - val_acc: 0.0000e+00\n",
            "Epoch 4/5\n",
            "1095/1095 [==============================] - 0s 173us/step - loss: 0.1667 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 5/5\n",
            "1095/1095 [==============================] - 0s 170us/step - loss: 0.1669 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "5\n",
            "51\n",
            "Train on 1095 samples, validate on 365 samples\n",
            "Epoch 1/5\n",
            "1095/1095 [==============================] - 0s 140us/step - loss: 0.1665 - acc: 0.0000e+00 - val_loss: 0.1386 - val_acc: 0.0000e+00\n",
            "Epoch 2/5\n",
            "1095/1095 [==============================] - 0s 139us/step - loss: 0.1668 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "Epoch 3/5\n",
            "1095/1095 [==============================] - 0s 142us/step - loss: 0.1669 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 4/5\n",
            "1095/1095 [==============================] - 0s 150us/step - loss: 0.1668 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "Epoch 5/5\n",
            "1095/1095 [==============================] - 0s 154us/step - loss: 0.1669 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "5\n",
            "61\n",
            "Train on 1095 samples, validate on 365 samples\n",
            "Epoch 1/5\n",
            "1095/1095 [==============================] - 0s 136us/step - loss: 0.1667 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 2/5\n",
            "1095/1095 [==============================] - 0s 132us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 3/5\n",
            "1095/1095 [==============================] - 0s 134us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 4/5\n",
            "1095/1095 [==============================] - 0s 130us/step - loss: 0.1668 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "Epoch 5/5\n",
            "1095/1095 [==============================] - 0s 136us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "5\n",
            "71\n",
            "Train on 1095 samples, validate on 365 samples\n",
            "Epoch 1/5\n",
            "1095/1095 [==============================] - 0s 125us/step - loss: 0.1668 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 2/5\n",
            "1095/1095 [==============================] - 0s 116us/step - loss: 0.1665 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "Epoch 3/5\n",
            "1095/1095 [==============================] - 0s 115us/step - loss: 0.1673 - acc: 0.0000e+00 - val_loss: 0.1387 - val_acc: 0.0000e+00\n",
            "Epoch 4/5\n",
            "1095/1095 [==============================] - 0s 114us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 5/5\n",
            "1095/1095 [==============================] - 0s 117us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "5\n",
            "81\n",
            "Train on 1095 samples, validate on 365 samples\n",
            "Epoch 1/5\n",
            "1095/1095 [==============================] - 0s 117us/step - loss: 0.1665 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 2/5\n",
            "1095/1095 [==============================] - 0s 111us/step - loss: 0.1676 - acc: 0.0000e+00 - val_loss: 0.1386 - val_acc: 0.0000e+00\n",
            "Epoch 3/5\n",
            "1095/1095 [==============================] - 0s 116us/step - loss: 0.1675 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 4/5\n",
            "1095/1095 [==============================] - 0s 113us/step - loss: 0.1670 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 5/5\n",
            "1095/1095 [==============================] - 0s 110us/step - loss: 0.1667 - acc: 0.0000e+00 - val_loss: 0.1387 - val_acc: 0.0000e+00\n",
            "5\n",
            "91\n",
            "Train on 1095 samples, validate on 365 samples\n",
            "Epoch 1/5\n",
            "1095/1095 [==============================] - 0s 95us/step - loss: 0.1661 - acc: 0.0000e+00 - val_loss: 0.1386 - val_acc: 0.0000e+00\n",
            "Epoch 2/5\n",
            "1095/1095 [==============================] - 0s 98us/step - loss: 0.1671 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "Epoch 3/5\n",
            "1095/1095 [==============================] - 0s 99us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1386 - val_acc: 0.0000e+00\n",
            "Epoch 4/5\n",
            "1095/1095 [==============================] - 0s 95us/step - loss: 0.1667 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 5/5\n",
            "1095/1095 [==============================] - 0s 95us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "5\n",
            "101\n",
            "Train on 1095 samples, validate on 365 samples\n",
            "Epoch 1/5\n",
            "1095/1095 [==============================] - 0s 92us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 2/5\n",
            "1095/1095 [==============================] - 0s 102us/step - loss: 0.1667 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "Epoch 3/5\n",
            "1095/1095 [==============================] - 0s 88us/step - loss: 0.1668 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 4/5\n",
            "1095/1095 [==============================] - 0s 89us/step - loss: 0.1665 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 5/5\n",
            "1095/1095 [==============================] - 0s 93us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "5\n",
            "111\n",
            "Train on 1095 samples, validate on 365 samples\n",
            "Epoch 1/5\n",
            "1095/1095 [==============================] - 0s 87us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 2/5\n",
            "1095/1095 [==============================] - 0s 92us/step - loss: 0.1668 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 3/5\n",
            "1095/1095 [==============================] - 0s 93us/step - loss: 0.1673 - acc: 0.0000e+00 - val_loss: 0.1389 - val_acc: 0.0000e+00\n",
            "Epoch 4/5\n",
            "1095/1095 [==============================] - 0s 88us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1389 - val_acc: 0.0000e+00\n",
            "Epoch 5/5\n",
            "1095/1095 [==============================] - 0s 93us/step - loss: 0.1669 - acc: 0.0000e+00 - val_loss: 0.1386 - val_acc: 0.0000e+00\n",
            "5\n",
            "121\n",
            "Train on 1095 samples, validate on 365 samples\n",
            "Epoch 1/5\n",
            "1095/1095 [==============================] - 0s 85us/step - loss: 0.1663 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 2/5\n",
            "1095/1095 [==============================] - 0s 87us/step - loss: 0.1667 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 3/5\n",
            "1095/1095 [==============================] - 0s 87us/step - loss: 0.1665 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 4/5\n",
            "1095/1095 [==============================] - 0s 84us/step - loss: 0.1669 - acc: 0.0000e+00 - val_loss: 0.1389 - val_acc: 0.0000e+00\n",
            "Epoch 5/5\n",
            "1095/1095 [==============================] - 0s 82us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "5\n",
            "131\n",
            "Train on 1095 samples, validate on 365 samples\n",
            "Epoch 1/5\n",
            "1095/1095 [==============================] - 0s 75us/step - loss: 0.1664 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 2/5\n",
            "1095/1095 [==============================] - 0s 76us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 3/5\n",
            "1095/1095 [==============================] - 0s 77us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 4/5\n",
            "1095/1095 [==============================] - 0s 77us/step - loss: 0.1665 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 5/5\n",
            "1095/1095 [==============================] - 0s 75us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "5\n",
            "141\n",
            "Train on 1095 samples, validate on 365 samples\n",
            "Epoch 1/6\n",
            "1095/1095 [==============================] - 8s 7ms/step - loss: 0.1795 - acc: 0.0000e+00 - val_loss: 0.1388 - val_acc: 0.0000e+00\n",
            "Epoch 2/6\n",
            "1095/1095 [==============================] - 8s 7ms/step - loss: 0.1794 - acc: 0.0000e+00 - val_loss: 0.1388 - val_acc: 0.0000e+00\n",
            "Epoch 3/6\n",
            "1095/1095 [==============================] - 8s 7ms/step - loss: 0.1765 - acc: 0.0000e+00 - val_loss: 0.1431 - val_acc: 0.0000e+00\n",
            "Epoch 4/6\n",
            "1095/1095 [==============================] - 8s 7ms/step - loss: 0.1755 - acc: 0.0000e+00 - val_loss: 0.1652 - val_acc: 0.0000e+00\n",
            "Epoch 5/6\n",
            "1095/1095 [==============================] - 8s 7ms/step - loss: 0.1768 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 6/6\n",
            "1095/1095 [==============================] - 8s 7ms/step - loss: 0.1765 - acc: 0.0000e+00 - val_loss: 0.1672 - val_acc: 0.0000e+00\n",
            "6\n",
            "1\n",
            "Train on 1095 samples, validate on 365 samples\n",
            "Epoch 1/6\n",
            "1095/1095 [==============================] - 1s 779us/step - loss: 0.1684 - acc: 0.0000e+00 - val_loss: 0.1425 - val_acc: 0.0000e+00\n",
            "Epoch 2/6\n",
            "1095/1095 [==============================] - 1s 759us/step - loss: 0.1679 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "Epoch 3/6\n",
            "1095/1095 [==============================] - 1s 778us/step - loss: 0.1674 - acc: 0.0000e+00 - val_loss: 0.1388 - val_acc: 0.0000e+00\n",
            "Epoch 4/6\n",
            "1095/1095 [==============================] - 1s 757us/step - loss: 0.1676 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 5/6\n",
            "1095/1095 [==============================] - 1s 765us/step - loss: 0.1685 - acc: 0.0000e+00 - val_loss: 0.1386 - val_acc: 0.0000e+00\n",
            "Epoch 6/6\n",
            "1095/1095 [==============================] - 1s 782us/step - loss: 0.1679 - acc: 0.0000e+00 - val_loss: 0.1390 - val_acc: 0.0000e+00\n",
            "6\n",
            "11\n",
            "Train on 1095 samples, validate on 365 samples\n",
            "Epoch 1/6\n",
            "1095/1095 [==============================] - 0s 415us/step - loss: 0.1665 - acc: 0.0000e+00 - val_loss: 0.1396 - val_acc: 0.0000e+00\n",
            "Epoch 2/6\n",
            "1095/1095 [==============================] - 0s 414us/step - loss: 0.1682 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "Epoch 3/6\n",
            "1095/1095 [==============================] - 0s 404us/step - loss: 0.1676 - acc: 0.0000e+00 - val_loss: 0.1390 - val_acc: 0.0000e+00\n",
            "Epoch 4/6\n",
            "1095/1095 [==============================] - 0s 411us/step - loss: 0.1672 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "Epoch 5/6\n",
            "1095/1095 [==============================] - 0s 402us/step - loss: 0.1674 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 6/6\n",
            "1095/1095 [==============================] - 0s 423us/step - loss: 0.1671 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "6\n",
            "21\n",
            "Train on 1095 samples, validate on 365 samples\n",
            "Epoch 1/6\n",
            "1095/1095 [==============================] - 0s 281us/step - loss: 0.1667 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 2/6\n",
            "1095/1095 [==============================] - 0s 280us/step - loss: 0.1668 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 3/6\n",
            "1095/1095 [==============================] - 0s 278us/step - loss: 0.1668 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 4/6\n",
            "1095/1095 [==============================] - 0s 271us/step - loss: 0.1668 - acc: 0.0000e+00 - val_loss: 0.1389 - val_acc: 0.0000e+00\n",
            "Epoch 5/6\n",
            "1095/1095 [==============================] - 0s 275us/step - loss: 0.1672 - acc: 0.0000e+00 - val_loss: 0.1395 - val_acc: 0.0000e+00\n",
            "Epoch 6/6\n",
            "1095/1095 [==============================] - 0s 289us/step - loss: 0.1668 - acc: 0.0000e+00 - val_loss: 0.1395 - val_acc: 0.0000e+00\n",
            "6\n",
            "31\n",
            "Train on 1095 samples, validate on 365 samples\n",
            "Epoch 1/6\n",
            "1095/1095 [==============================] - 0s 212us/step - loss: 0.1672 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 2/6\n",
            "1095/1095 [==============================] - 0s 209us/step - loss: 0.1667 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 3/6\n",
            "1095/1095 [==============================] - 0s 212us/step - loss: 0.1667 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 4/6\n",
            "1095/1095 [==============================] - 0s 211us/step - loss: 0.1667 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 5/6\n",
            "1095/1095 [==============================] - 0s 216us/step - loss: 0.1672 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 6/6\n",
            "1095/1095 [==============================] - 0s 208us/step - loss: 0.1670 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "6\n",
            "41\n",
            "Train on 1095 samples, validate on 365 samples\n",
            "Epoch 1/6\n",
            "1095/1095 [==============================] - 0s 169us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 2/6\n",
            "1095/1095 [==============================] - 0s 168us/step - loss: 0.1668 - acc: 0.0000e+00 - val_loss: 0.1386 - val_acc: 0.0000e+00\n",
            "Epoch 3/6\n",
            "1095/1095 [==============================] - 0s 178us/step - loss: 0.1667 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 4/6\n",
            "1095/1095 [==============================] - 0s 182us/step - loss: 0.1672 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 5/6\n",
            "1095/1095 [==============================] - 0s 172us/step - loss: 0.1670 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 6/6\n",
            "1095/1095 [==============================] - 0s 168us/step - loss: 0.1667 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "6\n",
            "51\n",
            "Train on 1095 samples, validate on 365 samples\n",
            "Epoch 1/6\n",
            "1095/1095 [==============================] - 0s 144us/step - loss: 0.1670 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 2/6\n",
            "1095/1095 [==============================] - 0s 144us/step - loss: 0.1667 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 3/6\n",
            "1095/1095 [==============================] - 0s 142us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "Epoch 4/6\n",
            "1095/1095 [==============================] - 0s 145us/step - loss: 0.1667 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 5/6\n",
            "1095/1095 [==============================] - 0s 139us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 6/6\n",
            "1095/1095 [==============================] - 0s 152us/step - loss: 0.1674 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "6\n",
            "61\n",
            "Train on 1095 samples, validate on 365 samples\n",
            "Epoch 1/6\n",
            "1095/1095 [==============================] - 0s 133us/step - loss: 0.1670 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 2/6\n",
            "1095/1095 [==============================] - 0s 129us/step - loss: 0.1680 - acc: 0.0000e+00 - val_loss: 0.1386 - val_acc: 0.0000e+00\n",
            "Epoch 3/6\n",
            "1095/1095 [==============================] - 0s 141us/step - loss: 0.1681 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "Epoch 4/6\n",
            "1095/1095 [==============================] - 0s 130us/step - loss: 0.1663 - acc: 0.0000e+00 - val_loss: 0.1386 - val_acc: 0.0000e+00\n",
            "Epoch 5/6\n",
            "1095/1095 [==============================] - 0s 133us/step - loss: 0.1670 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 6/6\n",
            "1095/1095 [==============================] - 0s 127us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "6\n",
            "71\n",
            "Train on 1095 samples, validate on 365 samples\n",
            "Epoch 1/6\n",
            "1095/1095 [==============================] - 0s 117us/step - loss: 0.1667 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 2/6\n",
            "1095/1095 [==============================] - 0s 110us/step - loss: 0.1665 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "Epoch 3/6\n",
            "1095/1095 [==============================] - 0s 111us/step - loss: 0.1667 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "Epoch 4/6\n",
            "1095/1095 [==============================] - 0s 120us/step - loss: 0.1674 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 5/6\n",
            "1095/1095 [==============================] - 0s 114us/step - loss: 0.1672 - acc: 0.0000e+00 - val_loss: 0.1387 - val_acc: 0.0000e+00\n",
            "Epoch 6/6\n",
            "1095/1095 [==============================] - 0s 115us/step - loss: 0.1665 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "6\n",
            "81\n",
            "Train on 1095 samples, validate on 365 samples\n",
            "Epoch 1/6\n",
            "1095/1095 [==============================] - 0s 109us/step - loss: 0.1667 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 2/6\n",
            "1095/1095 [==============================] - 0s 111us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 3/6\n",
            "1095/1095 [==============================] - 0s 108us/step - loss: 0.1667 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "Epoch 4/6\n",
            "1095/1095 [==============================] - 0s 109us/step - loss: 0.1667 - acc: 0.0000e+00 - val_loss: 0.1387 - val_acc: 0.0000e+00\n",
            "Epoch 5/6\n",
            "1095/1095 [==============================] - 0s 106us/step - loss: 0.1673 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 6/6\n",
            "1095/1095 [==============================] - 0s 110us/step - loss: 0.1669 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "6\n",
            "91\n",
            "Train on 1095 samples, validate on 365 samples\n",
            "Epoch 1/6\n",
            "1095/1095 [==============================] - 0s 94us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 2/6\n",
            "1095/1095 [==============================] - 0s 93us/step - loss: 0.1665 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 3/6\n",
            "1095/1095 [==============================] - 0s 95us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 4/6\n",
            "1095/1095 [==============================] - 0s 96us/step - loss: 0.1665 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 5/6\n",
            "1095/1095 [==============================] - 0s 93us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 6/6\n",
            "1095/1095 [==============================] - 0s 92us/step - loss: 0.1668 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "6\n",
            "101\n",
            "Train on 1095 samples, validate on 365 samples\n",
            "Epoch 1/6\n",
            "1095/1095 [==============================] - 0s 83us/step - loss: 0.1665 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 2/6\n",
            "1095/1095 [==============================] - 0s 85us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 3/6\n",
            "1095/1095 [==============================] - 0s 87us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "Epoch 4/6\n",
            "1095/1095 [==============================] - 0s 98us/step - loss: 0.1668 - acc: 0.0000e+00 - val_loss: 0.1386 - val_acc: 0.0000e+00\n",
            "Epoch 5/6\n",
            "1095/1095 [==============================] - 0s 85us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 6/6\n",
            "1095/1095 [==============================] - 0s 87us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "6\n",
            "111\n",
            "Train on 1095 samples, validate on 365 samples\n",
            "Epoch 1/6\n",
            "1095/1095 [==============================] - 0s 87us/step - loss: 0.1665 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 2/6\n",
            "1095/1095 [==============================] - 0s 85us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "Epoch 3/6\n",
            "1095/1095 [==============================] - 0s 85us/step - loss: 0.1670 - acc: 0.0000e+00 - val_loss: 0.1393 - val_acc: 0.0000e+00\n",
            "Epoch 4/6\n",
            "1095/1095 [==============================] - 0s 87us/step - loss: 0.1670 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 5/6\n",
            "1095/1095 [==============================] - 0s 84us/step - loss: 0.1667 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 6/6\n",
            "1095/1095 [==============================] - 0s 87us/step - loss: 0.1670 - acc: 0.0000e+00 - val_loss: 0.1386 - val_acc: 0.0000e+00\n",
            "6\n",
            "121\n",
            "Train on 1095 samples, validate on 365 samples\n",
            "Epoch 1/6\n",
            "1095/1095 [==============================] - 0s 80us/step - loss: 0.1668 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 2/6\n",
            "1095/1095 [==============================] - 0s 98us/step - loss: 0.1668 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "Epoch 3/6\n",
            "1095/1095 [==============================] - 0s 81us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 4/6\n",
            "1095/1095 [==============================] - 0s 85us/step - loss: 0.1669 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "Epoch 5/6\n",
            "1095/1095 [==============================] - 0s 84us/step - loss: 0.1665 - acc: 0.0000e+00 - val_loss: 0.1386 - val_acc: 0.0000e+00\n",
            "Epoch 6/6\n",
            "1095/1095 [==============================] - 0s 82us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "6\n",
            "131\n",
            "Train on 1095 samples, validate on 365 samples\n",
            "Epoch 1/6\n",
            "1095/1095 [==============================] - 0s 72us/step - loss: 0.1667 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 2/6\n",
            "1095/1095 [==============================] - 0s 74us/step - loss: 0.1665 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 3/6\n",
            "1095/1095 [==============================] - 0s 72us/step - loss: 0.1665 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 4/6\n",
            "1095/1095 [==============================] - 0s 71us/step - loss: 0.1667 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "Epoch 5/6\n",
            "1095/1095 [==============================] - 0s 71us/step - loss: 0.1667 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 6/6\n",
            "1095/1095 [==============================] - 0s 70us/step - loss: 0.1665 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "6\n",
            "141\n",
            "Train on 1095 samples, validate on 365 samples\n",
            "Epoch 1/7\n",
            "1095/1095 [==============================] - 8s 7ms/step - loss: 0.1789 - acc: 0.0000e+00 - val_loss: 0.1493 - val_acc: 0.0000e+00\n",
            "Epoch 2/7\n",
            "1095/1095 [==============================] - 8s 7ms/step - loss: 0.1781 - acc: 0.0000e+00 - val_loss: 0.1446 - val_acc: 0.0000e+00\n",
            "Epoch 3/7\n",
            "1095/1095 [==============================] - 8s 7ms/step - loss: 0.1764 - acc: 0.0000e+00 - val_loss: 0.1510 - val_acc: 0.0000e+00\n",
            "Epoch 4/7\n",
            "1095/1095 [==============================] - 8s 7ms/step - loss: 0.1750 - acc: 0.0000e+00 - val_loss: 0.1391 - val_acc: 0.0000e+00\n",
            "Epoch 5/7\n",
            "1095/1095 [==============================] - 8s 7ms/step - loss: 0.1765 - acc: 0.0000e+00 - val_loss: 0.1399 - val_acc: 0.0000e+00\n",
            "Epoch 6/7\n",
            "1095/1095 [==============================] - 8s 7ms/step - loss: 0.1768 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "Epoch 7/7\n",
            "1095/1095 [==============================] - 8s 7ms/step - loss: 0.1776 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "7\n",
            "1\n",
            "Train on 1095 samples, validate on 365 samples\n",
            "Epoch 1/7\n",
            "1095/1095 [==============================] - 1s 779us/step - loss: 0.1684 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 2/7\n",
            "1095/1095 [==============================] - 1s 753us/step - loss: 0.1675 - acc: 0.0000e+00 - val_loss: 0.1400 - val_acc: 0.0000e+00\n",
            "Epoch 3/7\n",
            "1095/1095 [==============================] - 1s 771us/step - loss: 0.1684 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 4/7\n",
            "1095/1095 [==============================] - 1s 781us/step - loss: 0.1670 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "Epoch 5/7\n",
            "1095/1095 [==============================] - 1s 771us/step - loss: 0.1680 - acc: 0.0000e+00 - val_loss: 0.1390 - val_acc: 0.0000e+00\n",
            "Epoch 6/7\n",
            "1095/1095 [==============================] - 1s 773us/step - loss: 0.1674 - acc: 0.0000e+00 - val_loss: 0.1400 - val_acc: 0.0000e+00\n",
            "Epoch 7/7\n",
            "1095/1095 [==============================] - 1s 795us/step - loss: 0.1687 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "7\n",
            "11\n",
            "Train on 1095 samples, validate on 365 samples\n",
            "Epoch 1/7\n",
            "1095/1095 [==============================] - 0s 408us/step - loss: 0.1673 - acc: 0.0000e+00 - val_loss: 0.1402 - val_acc: 0.0000e+00\n",
            "Epoch 2/7\n",
            "1095/1095 [==============================] - 0s 420us/step - loss: 0.1688 - acc: 0.0000e+00 - val_loss: 0.1394 - val_acc: 0.0000e+00\n",
            "Epoch 3/7\n",
            "1095/1095 [==============================] - 0s 417us/step - loss: 0.1675 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "Epoch 4/7\n",
            "1095/1095 [==============================] - 0s 411us/step - loss: 0.1672 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 5/7\n",
            "1095/1095 [==============================] - 0s 423us/step - loss: 0.1674 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 6/7\n",
            "1095/1095 [==============================] - 0s 415us/step - loss: 0.1672 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 7/7\n",
            "1095/1095 [==============================] - 0s 407us/step - loss: 0.1673 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "7\n",
            "21\n",
            "Train on 1095 samples, validate on 365 samples\n",
            "Epoch 1/7\n",
            "1095/1095 [==============================] - 0s 272us/step - loss: 0.1667 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 2/7\n",
            "1095/1095 [==============================] - 0s 274us/step - loss: 0.1670 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 3/7\n",
            "1095/1095 [==============================] - 0s 277us/step - loss: 0.1667 - acc: 0.0000e+00 - val_loss: 0.1411 - val_acc: 0.0000e+00\n",
            "Epoch 4/7\n",
            "1095/1095 [==============================] - 0s 293us/step - loss: 0.1677 - acc: 0.0000e+00 - val_loss: 0.1399 - val_acc: 0.0000e+00\n",
            "Epoch 5/7\n",
            "1095/1095 [==============================] - 0s 278us/step - loss: 0.1673 - acc: 0.0000e+00 - val_loss: 0.1387 - val_acc: 0.0000e+00\n",
            "Epoch 6/7\n",
            "1095/1095 [==============================] - 0s 284us/step - loss: 0.1674 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 7/7\n",
            "1095/1095 [==============================] - 0s 280us/step - loss: 0.1667 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "7\n",
            "31\n",
            "Train on 1095 samples, validate on 365 samples\n",
            "Epoch 1/7\n",
            "1095/1095 [==============================] - 0s 211us/step - loss: 0.1672 - acc: 0.0000e+00 - val_loss: 0.1386 - val_acc: 0.0000e+00\n",
            "Epoch 2/7\n",
            "1095/1095 [==============================] - 0s 207us/step - loss: 0.1672 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "Epoch 3/7\n",
            "1095/1095 [==============================] - 0s 217us/step - loss: 0.1668 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 4/7\n",
            "1095/1095 [==============================] - 0s 215us/step - loss: 0.1669 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 5/7\n",
            "1095/1095 [==============================] - 0s 209us/step - loss: 0.1668 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "Epoch 6/7\n",
            "1095/1095 [==============================] - 0s 210us/step - loss: 0.1668 - acc: 0.0000e+00 - val_loss: 0.1387 - val_acc: 0.0000e+00\n",
            "Epoch 7/7\n",
            "1095/1095 [==============================] - 0s 212us/step - loss: 0.1669 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "7\n",
            "41\n",
            "Train on 1095 samples, validate on 365 samples\n",
            "Epoch 1/7\n",
            "1095/1095 [==============================] - 0s 170us/step - loss: 0.1669 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 2/7\n",
            "1095/1095 [==============================] - 0s 174us/step - loss: 0.1668 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "Epoch 3/7\n",
            "1095/1095 [==============================] - 0s 173us/step - loss: 0.1668 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 4/7\n",
            "1095/1095 [==============================] - 0s 177us/step - loss: 0.1670 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 5/7\n",
            "1095/1095 [==============================] - 0s 185us/step - loss: 0.1671 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "Epoch 6/7\n",
            "1095/1095 [==============================] - 0s 183us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 7/7\n",
            "1095/1095 [==============================] - 0s 202us/step - loss: 0.1669 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "7\n",
            "51\n",
            "Train on 1095 samples, validate on 365 samples\n",
            "Epoch 1/7\n",
            "1095/1095 [==============================] - 0s 147us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 2/7\n",
            "1095/1095 [==============================] - 0s 141us/step - loss: 0.1667 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "Epoch 3/7\n",
            "1095/1095 [==============================] - 0s 144us/step - loss: 0.1672 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 4/7\n",
            "1095/1095 [==============================] - 0s 147us/step - loss: 0.1668 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 5/7\n",
            "1095/1095 [==============================] - 0s 144us/step - loss: 0.1669 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 6/7\n",
            "1095/1095 [==============================] - 0s 144us/step - loss: 0.1669 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 7/7\n",
            "1095/1095 [==============================] - 0s 141us/step - loss: 0.1667 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "7\n",
            "61\n",
            "Train on 1095 samples, validate on 365 samples\n",
            "Epoch 1/7\n",
            "1095/1095 [==============================] - 0s 135us/step - loss: 0.1665 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 2/7\n",
            "1095/1095 [==============================] - 0s 131us/step - loss: 0.1668 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 3/7\n",
            "1095/1095 [==============================] - 0s 151us/step - loss: 0.1669 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 4/7\n",
            "1095/1095 [==============================] - 0s 138us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1386 - val_acc: 0.0000e+00\n",
            "Epoch 5/7\n",
            "1095/1095 [==============================] - 0s 132us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 6/7\n",
            "1095/1095 [==============================] - 0s 133us/step - loss: 0.1670 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 7/7\n",
            "1095/1095 [==============================] - 0s 130us/step - loss: 0.1671 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "7\n",
            "71\n",
            "Train on 1095 samples, validate on 365 samples\n",
            "Epoch 1/7\n",
            "1095/1095 [==============================] - 0s 111us/step - loss: 0.1669 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "Epoch 2/7\n",
            "1095/1095 [==============================] - 0s 112us/step - loss: 0.1668 - acc: 0.0000e+00 - val_loss: 0.1386 - val_acc: 0.0000e+00\n",
            "Epoch 3/7\n",
            "1095/1095 [==============================] - 0s 125us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 4/7\n",
            "1095/1095 [==============================] - 0s 115us/step - loss: 0.1668 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "Epoch 5/7\n",
            "1095/1095 [==============================] - 0s 117us/step - loss: 0.1665 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 6/7\n",
            "1095/1095 [==============================] - 0s 117us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 7/7\n",
            "1095/1095 [==============================] - 0s 118us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "7\n",
            "81\n",
            "Train on 1095 samples, validate on 365 samples\n",
            "Epoch 1/7\n",
            "1095/1095 [==============================] - 0s 106us/step - loss: 0.1669 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 2/7\n",
            "1095/1095 [==============================] - 0s 111us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 3/7\n",
            "1095/1095 [==============================] - 0s 110us/step - loss: 0.1698 - acc: 0.0000e+00 - val_loss: 0.1404 - val_acc: 0.0000e+00\n",
            "Epoch 4/7\n",
            "1095/1095 [==============================] - 0s 116us/step - loss: 0.1663 - acc: 0.0000e+00 - val_loss: 0.1391 - val_acc: 0.0000e+00\n",
            "Epoch 5/7\n",
            "1095/1095 [==============================] - 0s 107us/step - loss: 0.1667 - acc: 0.0000e+00 - val_loss: 0.1388 - val_acc: 0.0000e+00\n",
            "Epoch 6/7\n",
            "1095/1095 [==============================] - 0s 105us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 7/7\n",
            "1095/1095 [==============================] - 0s 108us/step - loss: 0.1665 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "7\n",
            "91\n",
            "Train on 1095 samples, validate on 365 samples\n",
            "Epoch 1/7\n",
            "1095/1095 [==============================] - 0s 95us/step - loss: 0.1672 - acc: 0.0000e+00 - val_loss: 0.1386 - val_acc: 0.0000e+00\n",
            "Epoch 2/7\n",
            "1095/1095 [==============================] - 0s 90us/step - loss: 0.1664 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 3/7\n",
            "1095/1095 [==============================] - 0s 100us/step - loss: 0.1667 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 4/7\n",
            "1095/1095 [==============================] - 0s 93us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 5/7\n",
            "1095/1095 [==============================] - 0s 91us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 6/7\n",
            "1095/1095 [==============================] - 0s 98us/step - loss: 0.1665 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 7/7\n",
            "1095/1095 [==============================] - 0s 91us/step - loss: 0.1667 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "7\n",
            "101\n",
            "Train on 1095 samples, validate on 365 samples\n",
            "Epoch 1/7\n",
            "1095/1095 [==============================] - 0s 83us/step - loss: 0.1664 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 2/7\n",
            "1095/1095 [==============================] - 0s 83us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 3/7\n",
            "1095/1095 [==============================] - 0s 85us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "Epoch 4/7\n",
            "1095/1095 [==============================] - 0s 86us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 5/7\n",
            "1095/1095 [==============================] - 0s 85us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 6/7\n",
            "1095/1095 [==============================] - 0s 86us/step - loss: 0.1667 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 7/7\n",
            "1095/1095 [==============================] - 0s 88us/step - loss: 0.1665 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "7\n",
            "111\n",
            "Train on 1095 samples, validate on 365 samples\n",
            "Epoch 1/7\n",
            "1095/1095 [==============================] - 0s 93us/step - loss: 0.1673 - acc: 0.0000e+00 - val_loss: 0.1389 - val_acc: 0.0000e+00\n",
            "Epoch 2/7\n",
            "1095/1095 [==============================] - 0s 92us/step - loss: 0.1677 - acc: 0.0000e+00 - val_loss: 0.1395 - val_acc: 0.0000e+00\n",
            "Epoch 3/7\n",
            "1095/1095 [==============================] - 0s 90us/step - loss: 0.1678 - acc: 0.0000e+00 - val_loss: 0.1387 - val_acc: 0.0000e+00\n",
            "Epoch 4/7\n",
            "1095/1095 [==============================] - 0s 90us/step - loss: 0.1676 - acc: 0.0000e+00 - val_loss: 0.1387 - val_acc: 0.0000e+00\n",
            "Epoch 5/7\n",
            "1095/1095 [==============================] - 0s 92us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1387 - val_acc: 0.0000e+00\n",
            "Epoch 6/7\n",
            "1095/1095 [==============================] - 0s 89us/step - loss: 0.1671 - acc: 0.0000e+00 - val_loss: 0.1386 - val_acc: 0.0000e+00\n",
            "Epoch 7/7\n",
            "1095/1095 [==============================] - 0s 89us/step - loss: 0.1669 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "7\n",
            "121\n",
            "Train on 1095 samples, validate on 365 samples\n",
            "Epoch 1/7\n",
            "1095/1095 [==============================] - 0s 77us/step - loss: 0.1664 - acc: 0.0000e+00 - val_loss: 0.1387 - val_acc: 0.0000e+00\n",
            "Epoch 2/7\n",
            "1095/1095 [==============================] - 0s 81us/step - loss: 0.1668 - acc: 0.0000e+00 - val_loss: 0.1388 - val_acc: 0.0000e+00\n",
            "Epoch 3/7\n",
            "1095/1095 [==============================] - 0s 80us/step - loss: 0.1665 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 4/7\n",
            "1095/1095 [==============================] - 0s 78us/step - loss: 0.1667 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "Epoch 5/7\n",
            "1095/1095 [==============================] - 0s 79us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1386 - val_acc: 0.0000e+00\n",
            "Epoch 6/7\n",
            "1095/1095 [==============================] - 0s 86us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 7/7\n",
            "1095/1095 [==============================] - 0s 80us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "7\n",
            "131\n",
            "Train on 1095 samples, validate on 365 samples\n",
            "Epoch 1/7\n",
            "1095/1095 [==============================] - 0s 71us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 2/7\n",
            "1095/1095 [==============================] - 0s 76us/step - loss: 0.1665 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "Epoch 3/7\n",
            "1095/1095 [==============================] - 0s 68us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "Epoch 4/7\n",
            "1095/1095 [==============================] - 0s 72us/step - loss: 0.1665 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 5/7\n",
            "1095/1095 [==============================] - 0s 73us/step - loss: 0.1665 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 6/7\n",
            "1095/1095 [==============================] - 0s 73us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 7/7\n",
            "1095/1095 [==============================] - 0s 79us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "7\n",
            "141\n",
            "Train on 1095 samples, validate on 365 samples\n",
            "Epoch 1/8\n",
            "1095/1095 [==============================] - 8s 7ms/step - loss: 0.1867 - acc: 0.0000e+00 - val_loss: 0.1595 - val_acc: 0.0000e+00\n",
            "Epoch 2/8\n",
            "1095/1095 [==============================] - 8s 7ms/step - loss: 0.1777 - acc: 0.0000e+00 - val_loss: 0.1387 - val_acc: 0.0000e+00\n",
            "Epoch 3/8\n",
            "1095/1095 [==============================] - 8s 8ms/step - loss: 0.1750 - acc: 0.0000e+00 - val_loss: 0.1405 - val_acc: 0.0000e+00\n",
            "Epoch 4/8\n",
            "1095/1095 [==============================] - 8s 7ms/step - loss: 0.1785 - acc: 0.0000e+00 - val_loss: 0.1400 - val_acc: 0.0000e+00\n",
            "Epoch 5/8\n",
            "1095/1095 [==============================] - 8s 7ms/step - loss: 0.1772 - acc: 0.0000e+00 - val_loss: 0.1554 - val_acc: 0.0000e+00\n",
            "Epoch 6/8\n",
            "1095/1095 [==============================] - 8s 7ms/step - loss: 0.1768 - acc: 0.0000e+00 - val_loss: 0.1470 - val_acc: 0.0000e+00\n",
            "Epoch 7/8\n",
            "1095/1095 [==============================] - 8s 8ms/step - loss: 0.1744 - acc: 0.0000e+00 - val_loss: 0.1414 - val_acc: 0.0000e+00\n",
            "Epoch 8/8\n",
            "1095/1095 [==============================] - 8s 8ms/step - loss: 0.1763 - acc: 0.0000e+00 - val_loss: 0.1409 - val_acc: 0.0000e+00\n",
            "8\n",
            "1\n",
            "Train on 1095 samples, validate on 365 samples\n",
            "Epoch 1/8\n",
            "1095/1095 [==============================] - 1s 788us/step - loss: 0.1681 - acc: 0.0000e+00 - val_loss: 0.1392 - val_acc: 0.0000e+00\n",
            "Epoch 2/8\n",
            "1095/1095 [==============================] - 1s 788us/step - loss: 0.1669 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "Epoch 3/8\n",
            "1095/1095 [==============================] - 1s 784us/step - loss: 0.1673 - acc: 0.0000e+00 - val_loss: 0.1392 - val_acc: 0.0000e+00\n",
            "Epoch 4/8\n",
            "1095/1095 [==============================] - 1s 775us/step - loss: 0.1676 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 5/8\n",
            "1095/1095 [==============================] - 1s 763us/step - loss: 0.1676 - acc: 0.0000e+00 - val_loss: 0.1409 - val_acc: 0.0000e+00\n",
            "Epoch 6/8\n",
            "1095/1095 [==============================] - 1s 794us/step - loss: 0.1689 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 7/8\n",
            "1095/1095 [==============================] - 1s 758us/step - loss: 0.1681 - acc: 0.0000e+00 - val_loss: 0.1400 - val_acc: 0.0000e+00\n",
            "Epoch 8/8\n",
            "1095/1095 [==============================] - 1s 769us/step - loss: 0.1674 - acc: 0.0000e+00 - val_loss: 0.1390 - val_acc: 0.0000e+00\n",
            "8\n",
            "11\n",
            "Train on 1095 samples, validate on 365 samples\n",
            "Epoch 1/8\n",
            "1095/1095 [==============================] - 0s 404us/step - loss: 0.1674 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 2/8\n",
            "1095/1095 [==============================] - 0s 401us/step - loss: 0.1669 - acc: 0.0000e+00 - val_loss: 0.1387 - val_acc: 0.0000e+00\n",
            "Epoch 3/8\n",
            "1095/1095 [==============================] - 0s 416us/step - loss: 0.1683 - acc: 0.0000e+00 - val_loss: 0.1388 - val_acc: 0.0000e+00\n",
            "Epoch 4/8\n",
            "1095/1095 [==============================] - 0s 406us/step - loss: 0.1674 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "Epoch 5/8\n",
            "1095/1095 [==============================] - 0s 428us/step - loss: 0.1673 - acc: 0.0000e+00 - val_loss: 0.1404 - val_acc: 0.0000e+00\n",
            "Epoch 6/8\n",
            "1095/1095 [==============================] - 0s 425us/step - loss: 0.1671 - acc: 0.0000e+00 - val_loss: 0.1386 - val_acc: 0.0000e+00\n",
            "Epoch 7/8\n",
            "1095/1095 [==============================] - 0s 416us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1409 - val_acc: 0.0000e+00\n",
            "Epoch 8/8\n",
            "1095/1095 [==============================] - 0s 420us/step - loss: 0.1670 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "8\n",
            "21\n",
            "Train on 1095 samples, validate on 365 samples\n",
            "Epoch 1/8\n",
            "1095/1095 [==============================] - 0s 298us/step - loss: 0.1707 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 2/8\n",
            "1095/1095 [==============================] - 0s 302us/step - loss: 0.1671 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 3/8\n",
            "1095/1095 [==============================] - 0s 271us/step - loss: 0.1671 - acc: 0.0000e+00 - val_loss: 0.1389 - val_acc: 0.0000e+00\n",
            "Epoch 4/8\n",
            "1095/1095 [==============================] - 0s 282us/step - loss: 0.1669 - acc: 0.0000e+00 - val_loss: 0.1386 - val_acc: 0.0000e+00\n",
            "Epoch 5/8\n",
            "1095/1095 [==============================] - 0s 293us/step - loss: 0.1668 - acc: 0.0000e+00 - val_loss: 0.1397 - val_acc: 0.0000e+00\n",
            "Epoch 6/8\n",
            "1095/1095 [==============================] - 0s 279us/step - loss: 0.1672 - acc: 0.0000e+00 - val_loss: 0.1387 - val_acc: 0.0000e+00\n",
            "Epoch 7/8\n",
            "1095/1095 [==============================] - 0s 280us/step - loss: 0.1673 - acc: 0.0000e+00 - val_loss: 0.1396 - val_acc: 0.0000e+00\n",
            "Epoch 8/8\n",
            "1095/1095 [==============================] - 0s 273us/step - loss: 0.1674 - acc: 0.0000e+00 - val_loss: 0.1387 - val_acc: 0.0000e+00\n",
            "8\n",
            "31\n",
            "Train on 1095 samples, validate on 365 samples\n",
            "Epoch 1/8\n",
            "1095/1095 [==============================] - 0s 226us/step - loss: 0.1667 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 2/8\n",
            "1095/1095 [==============================] - 0s 222us/step - loss: 0.1669 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "Epoch 3/8\n",
            "1095/1095 [==============================] - 0s 213us/step - loss: 0.1681 - acc: 0.0000e+00 - val_loss: 0.1398 - val_acc: 0.0000e+00\n",
            "Epoch 4/8\n",
            "1095/1095 [==============================] - 0s 206us/step - loss: 0.1668 - acc: 0.0000e+00 - val_loss: 0.1392 - val_acc: 0.0000e+00\n",
            "Epoch 5/8\n",
            "1095/1095 [==============================] - 0s 220us/step - loss: 0.1674 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "Epoch 6/8\n",
            "1095/1095 [==============================] - 0s 232us/step - loss: 0.1667 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 7/8\n",
            "1095/1095 [==============================] - 0s 209us/step - loss: 0.1667 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 8/8\n",
            "1095/1095 [==============================] - 0s 209us/step - loss: 0.1670 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "8\n",
            "41\n",
            "Train on 1095 samples, validate on 365 samples\n",
            "Epoch 1/8\n",
            "1095/1095 [==============================] - 0s 174us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 2/8\n",
            "1095/1095 [==============================] - 0s 173us/step - loss: 0.1669 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "Epoch 3/8\n",
            "1095/1095 [==============================] - 0s 180us/step - loss: 0.1670 - acc: 0.0000e+00 - val_loss: 0.1388 - val_acc: 0.0000e+00\n",
            "Epoch 4/8\n",
            "1095/1095 [==============================] - 0s 184us/step - loss: 0.1673 - acc: 0.0000e+00 - val_loss: 0.1398 - val_acc: 0.0000e+00\n",
            "Epoch 5/8\n",
            "1095/1095 [==============================] - 0s 177us/step - loss: 0.1682 - acc: 0.0000e+00 - val_loss: 0.1386 - val_acc: 0.0000e+00\n",
            "Epoch 6/8\n",
            "1095/1095 [==============================] - 0s 185us/step - loss: 0.1667 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "Epoch 7/8\n",
            "1095/1095 [==============================] - 0s 189us/step - loss: 0.1667 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 8/8\n",
            "1095/1095 [==============================] - 0s 175us/step - loss: 0.1667 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "8\n",
            "51\n",
            "Train on 1095 samples, validate on 365 samples\n",
            "Epoch 1/8\n",
            "1095/1095 [==============================] - 0s 143us/step - loss: 0.1670 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 2/8\n",
            "1095/1095 [==============================] - 0s 147us/step - loss: 0.1669 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "Epoch 3/8\n",
            "1095/1095 [==============================] - 0s 148us/step - loss: 0.1667 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 4/8\n",
            "1095/1095 [==============================] - 0s 161us/step - loss: 0.1667 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 5/8\n",
            "1095/1095 [==============================] - 0s 157us/step - loss: 0.1668 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 6/8\n",
            "1095/1095 [==============================] - 0s 149us/step - loss: 0.1668 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 7/8\n",
            "1095/1095 [==============================] - 0s 140us/step - loss: 0.1669 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "Epoch 8/8\n",
            "1095/1095 [==============================] - 0s 156us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "8\n",
            "61\n",
            "Train on 1095 samples, validate on 365 samples\n",
            "Epoch 1/8\n",
            "1095/1095 [==============================] - 0s 134us/step - loss: 0.1669 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 2/8\n",
            "1095/1095 [==============================] - 0s 144us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 3/8\n",
            "1095/1095 [==============================] - 0s 129us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 4/8\n",
            "1095/1095 [==============================] - 0s 131us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "Epoch 5/8\n",
            "1095/1095 [==============================] - 0s 132us/step - loss: 0.1668 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 6/8\n",
            "1095/1095 [==============================] - 0s 130us/step - loss: 0.1668 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 7/8\n",
            "1095/1095 [==============================] - 0s 136us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1387 - val_acc: 0.0000e+00\n",
            "Epoch 8/8\n",
            "1095/1095 [==============================] - 0s 136us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "8\n",
            "71\n",
            "Train on 1095 samples, validate on 365 samples\n",
            "Epoch 1/8\n",
            "1095/1095 [==============================] - 0s 124us/step - loss: 0.1667 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 2/8\n",
            "1095/1095 [==============================] - 0s 124us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 3/8\n",
            "1095/1095 [==============================] - 0s 121us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 4/8\n",
            "1095/1095 [==============================] - 0s 116us/step - loss: 0.1665 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "Epoch 5/8\n",
            "1095/1095 [==============================] - 0s 113us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 6/8\n",
            "1095/1095 [==============================] - 0s 113us/step - loss: 0.1673 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "Epoch 7/8\n",
            "1095/1095 [==============================] - 0s 116us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 8/8\n",
            "1095/1095 [==============================] - 0s 113us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "8\n",
            "81\n",
            "Train on 1095 samples, validate on 365 samples\n",
            "Epoch 1/8\n",
            "1095/1095 [==============================] - 0s 111us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 2/8\n",
            "1095/1095 [==============================] - 0s 110us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 3/8\n",
            "1095/1095 [==============================] - 0s 122us/step - loss: 0.1676 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 4/8\n",
            "1095/1095 [==============================] - 0s 117us/step - loss: 0.1681 - acc: 0.0000e+00 - val_loss: 0.1395 - val_acc: 0.0000e+00\n",
            "Epoch 5/8\n",
            "1095/1095 [==============================] - 0s 118us/step - loss: 0.1674 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "Epoch 6/8\n",
            "1095/1095 [==============================] - 0s 116us/step - loss: 0.1668 - acc: 0.0000e+00 - val_loss: 0.1386 - val_acc: 0.0000e+00\n",
            "Epoch 7/8\n",
            "1095/1095 [==============================] - 0s 118us/step - loss: 0.1667 - acc: 0.0000e+00 - val_loss: 0.1386 - val_acc: 0.0000e+00\n",
            "Epoch 8/8\n",
            "1095/1095 [==============================] - 0s 105us/step - loss: 0.1671 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "8\n",
            "91\n",
            "Train on 1095 samples, validate on 365 samples\n",
            "Epoch 1/8\n",
            "1095/1095 [==============================] - 0s 101us/step - loss: 0.1668 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 2/8\n",
            "1095/1095 [==============================] - 0s 93us/step - loss: 0.1665 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 3/8\n",
            "1095/1095 [==============================] - 0s 94us/step - loss: 0.1670 - acc: 0.0000e+00 - val_loss: 0.1386 - val_acc: 0.0000e+00\n",
            "Epoch 4/8\n",
            "1095/1095 [==============================] - 0s 93us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 5/8\n",
            "1095/1095 [==============================] - 0s 93us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 6/8\n",
            "1095/1095 [==============================] - 0s 91us/step - loss: 0.1667 - acc: 0.0000e+00 - val_loss: 0.1387 - val_acc: 0.0000e+00\n",
            "Epoch 7/8\n",
            "1095/1095 [==============================] - 0s 99us/step - loss: 0.1669 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 8/8\n",
            "1095/1095 [==============================] - 0s 98us/step - loss: 0.1668 - acc: 0.0000e+00 - val_loss: 0.1386 - val_acc: 0.0000e+00\n",
            "8\n",
            "101\n",
            "Train on 1095 samples, validate on 365 samples\n",
            "Epoch 1/8\n",
            "1095/1095 [==============================] - 0s 86us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 2/8\n",
            "1095/1095 [==============================] - 0s 96us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 3/8\n",
            "1095/1095 [==============================] - 0s 95us/step - loss: 0.1665 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 4/8\n",
            "1095/1095 [==============================] - 0s 87us/step - loss: 0.1668 - acc: 0.0000e+00 - val_loss: 0.1386 - val_acc: 0.0000e+00\n",
            "Epoch 5/8\n",
            "1095/1095 [==============================] - 0s 87us/step - loss: 0.1665 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 6/8\n",
            "1095/1095 [==============================] - 0s 86us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 7/8\n",
            "1095/1095 [==============================] - 0s 88us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 8/8\n",
            "1095/1095 [==============================] - 0s 86us/step - loss: 0.1665 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "8\n",
            "111\n",
            "Train on 1095 samples, validate on 365 samples\n",
            "Epoch 1/8\n",
            "1095/1095 [==============================] - 0s 87us/step - loss: 0.1667 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "Epoch 2/8\n",
            "1095/1095 [==============================] - 0s 88us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 3/8\n",
            "1095/1095 [==============================] - 0s 92us/step - loss: 0.1667 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 4/8\n",
            "1095/1095 [==============================] - 0s 101us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 5/8\n",
            "1095/1095 [==============================] - 0s 89us/step - loss: 0.1671 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "Epoch 6/8\n",
            "1095/1095 [==============================] - 0s 88us/step - loss: 0.1668 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 7/8\n",
            "1095/1095 [==============================] - 0s 86us/step - loss: 0.1667 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 8/8\n",
            "1095/1095 [==============================] - 0s 87us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "8\n",
            "121\n",
            "Train on 1095 samples, validate on 365 samples\n",
            "Epoch 1/8\n",
            "1095/1095 [==============================] - 0s 80us/step - loss: 0.1675 - acc: 0.0000e+00 - val_loss: 0.1395 - val_acc: 0.0000e+00\n",
            "Epoch 2/8\n",
            "1095/1095 [==============================] - 0s 84us/step - loss: 0.1672 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "Epoch 3/8\n",
            "1095/1095 [==============================] - 0s 80us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1386 - val_acc: 0.0000e+00\n",
            "Epoch 4/8\n",
            "1095/1095 [==============================] - 0s 81us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 5/8\n",
            "1095/1095 [==============================] - 0s 82us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 6/8\n",
            "1095/1095 [==============================] - 0s 81us/step - loss: 0.1665 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "Epoch 7/8\n",
            "1095/1095 [==============================] - 0s 86us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 8/8\n",
            "1095/1095 [==============================] - 0s 79us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "8\n",
            "131\n",
            "Train on 1095 samples, validate on 365 samples\n",
            "Epoch 1/8\n",
            "1095/1095 [==============================] - 0s 74us/step - loss: 0.1667 - acc: 0.0000e+00 - val_loss: 0.1386 - val_acc: 0.0000e+00\n",
            "Epoch 2/8\n",
            "1095/1095 [==============================] - 0s 71us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 3/8\n",
            "1095/1095 [==============================] - 0s 72us/step - loss: 0.1665 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 4/8\n",
            "1095/1095 [==============================] - 0s 72us/step - loss: 0.1667 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 5/8\n",
            "1095/1095 [==============================] - 0s 71us/step - loss: 0.1667 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "Epoch 6/8\n",
            "1095/1095 [==============================] - 0s 71us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 7/8\n",
            "1095/1095 [==============================] - 0s 76us/step - loss: 0.1665 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 8/8\n",
            "1095/1095 [==============================] - 0s 73us/step - loss: 0.1665 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "8\n",
            "141\n",
            "Train on 1095 samples, validate on 365 samples\n",
            "Epoch 1/9\n",
            "1095/1095 [==============================] - 8s 7ms/step - loss: 0.1804 - acc: 0.0000e+00 - val_loss: 0.1407 - val_acc: 0.0000e+00\n",
            "Epoch 2/9\n",
            "1095/1095 [==============================] - 8s 7ms/step - loss: 0.1744 - acc: 0.0000e+00 - val_loss: 0.1558 - val_acc: 0.0000e+00\n",
            "Epoch 3/9\n",
            "1095/1095 [==============================] - 8s 7ms/step - loss: 0.1751 - acc: 0.0000e+00 - val_loss: 0.1714 - val_acc: 0.0000e+00\n",
            "Epoch 4/9\n",
            "1095/1095 [==============================] - 8s 7ms/step - loss: 0.1750 - acc: 0.0000e+00 - val_loss: 0.2208 - val_acc: 0.0000e+00\n",
            "Epoch 5/9\n",
            "1095/1095 [==============================] - 8s 7ms/step - loss: 0.1779 - acc: 0.0000e+00 - val_loss: 0.1763 - val_acc: 0.0000e+00\n",
            "Epoch 6/9\n",
            "1095/1095 [==============================] - 8s 7ms/step - loss: 0.1757 - acc: 0.0000e+00 - val_loss: 0.1386 - val_acc: 0.0000e+00\n",
            "Epoch 7/9\n",
            "1095/1095 [==============================] - 8s 7ms/step - loss: 0.1773 - acc: 0.0000e+00 - val_loss: 0.1417 - val_acc: 0.0000e+00\n",
            "Epoch 8/9\n",
            "1095/1095 [==============================] - 8s 7ms/step - loss: 0.1783 - acc: 0.0000e+00 - val_loss: 0.1389 - val_acc: 0.0000e+00\n",
            "Epoch 9/9\n",
            "1095/1095 [==============================] - 8s 8ms/step - loss: 0.1765 - acc: 0.0000e+00 - val_loss: 0.1577 - val_acc: 0.0000e+00\n",
            "9\n",
            "1\n",
            "Train on 1095 samples, validate on 365 samples\n",
            "Epoch 1/9\n",
            "1095/1095 [==============================] - 1s 786us/step - loss: 0.1691 - acc: 0.0000e+00 - val_loss: 0.1398 - val_acc: 0.0000e+00\n",
            "Epoch 2/9\n",
            "1095/1095 [==============================] - 1s 766us/step - loss: 0.1675 - acc: 0.0000e+00 - val_loss: 0.1387 - val_acc: 0.0000e+00\n",
            "Epoch 3/9\n",
            "1095/1095 [==============================] - 1s 745us/step - loss: 0.1673 - acc: 0.0000e+00 - val_loss: 0.1406 - val_acc: 0.0000e+00\n",
            "Epoch 4/9\n",
            "1095/1095 [==============================] - 1s 771us/step - loss: 0.1678 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 5/9\n",
            "1095/1095 [==============================] - 1s 760us/step - loss: 0.1676 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 6/9\n",
            "1095/1095 [==============================] - 1s 755us/step - loss: 0.1676 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 7/9\n",
            "1095/1095 [==============================] - 1s 761us/step - loss: 0.1675 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "Epoch 8/9\n",
            "1095/1095 [==============================] - 1s 753us/step - loss: 0.1679 - acc: 0.0000e+00 - val_loss: 0.1391 - val_acc: 0.0000e+00\n",
            "Epoch 9/9\n",
            "1095/1095 [==============================] - 1s 762us/step - loss: 0.1672 - acc: 0.0000e+00 - val_loss: 0.1402 - val_acc: 0.0000e+00\n",
            "9\n",
            "11\n",
            "Train on 1095 samples, validate on 365 samples\n",
            "Epoch 1/9\n",
            "1095/1095 [==============================] - 0s 427us/step - loss: 0.1678 - acc: 0.0000e+00 - val_loss: 0.1386 - val_acc: 0.0000e+00\n",
            "Epoch 2/9\n",
            "1095/1095 [==============================] - 0s 421us/step - loss: 0.1671 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "Epoch 3/9\n",
            "1095/1095 [==============================] - 0s 413us/step - loss: 0.1671 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "Epoch 4/9\n",
            "1095/1095 [==============================] - 0s 398us/step - loss: 0.1671 - acc: 0.0000e+00 - val_loss: 0.1389 - val_acc: 0.0000e+00\n",
            "Epoch 5/9\n",
            "1095/1095 [==============================] - 0s 404us/step - loss: 0.1669 - acc: 0.0000e+00 - val_loss: 0.1391 - val_acc: 0.0000e+00\n",
            "Epoch 6/9\n",
            "1095/1095 [==============================] - 0s 408us/step - loss: 0.1674 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 7/9\n",
            "1095/1095 [==============================] - 0s 410us/step - loss: 0.1668 - acc: 0.0000e+00 - val_loss: 0.1393 - val_acc: 0.0000e+00\n",
            "Epoch 8/9\n",
            "1095/1095 [==============================] - 0s 414us/step - loss: 0.1680 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 9/9\n",
            "1095/1095 [==============================] - 0s 416us/step - loss: 0.1660 - acc: 0.0000e+00 - val_loss: 0.1423 - val_acc: 0.0000e+00\n",
            "9\n",
            "21\n",
            "Train on 1095 samples, validate on 365 samples\n",
            "Epoch 1/9\n",
            "1095/1095 [==============================] - 0s 291us/step - loss: 0.1675 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 2/9\n",
            "1095/1095 [==============================] - 0s 268us/step - loss: 0.1674 - acc: 0.0000e+00 - val_loss: 0.1388 - val_acc: 0.0000e+00\n",
            "Epoch 3/9\n",
            "1095/1095 [==============================] - 0s 295us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 4/9\n",
            "1095/1095 [==============================] - 0s 287us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1387 - val_acc: 0.0000e+00\n",
            "Epoch 5/9\n",
            "1095/1095 [==============================] - 0s 283us/step - loss: 0.1674 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 6/9\n",
            "1095/1095 [==============================] - 0s 292us/step - loss: 0.1667 - acc: 0.0000e+00 - val_loss: 0.1388 - val_acc: 0.0000e+00\n",
            "Epoch 7/9\n",
            "1095/1095 [==============================] - 0s 270us/step - loss: 0.1671 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 8/9\n",
            "1095/1095 [==============================] - 0s 290us/step - loss: 0.1673 - acc: 0.0000e+00 - val_loss: 0.1389 - val_acc: 0.0000e+00\n",
            "Epoch 9/9\n",
            "1095/1095 [==============================] - 0s 297us/step - loss: 0.1671 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "9\n",
            "31\n",
            "Train on 1095 samples, validate on 365 samples\n",
            "Epoch 1/9\n",
            "1095/1095 [==============================] - 0s 205us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 2/9\n",
            "1095/1095 [==============================] - 0s 206us/step - loss: 0.1667 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "Epoch 3/9\n",
            "1095/1095 [==============================] - 0s 218us/step - loss: 0.1672 - acc: 0.0000e+00 - val_loss: 0.1391 - val_acc: 0.0000e+00\n",
            "Epoch 4/9\n",
            "1095/1095 [==============================] - 0s 220us/step - loss: 0.1677 - acc: 0.0000e+00 - val_loss: 0.1397 - val_acc: 0.0000e+00\n",
            "Epoch 5/9\n",
            "1095/1095 [==============================] - 0s 225us/step - loss: 0.1667 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "Epoch 6/9\n",
            "1095/1095 [==============================] - 0s 210us/step - loss: 0.1667 - acc: 0.0000e+00 - val_loss: 0.1388 - val_acc: 0.0000e+00\n",
            "Epoch 7/9\n",
            "1095/1095 [==============================] - 0s 213us/step - loss: 0.1680 - acc: 0.0000e+00 - val_loss: 0.1386 - val_acc: 0.0000e+00\n",
            "Epoch 8/9\n",
            "1095/1095 [==============================] - 0s 213us/step - loss: 0.1663 - acc: 0.0000e+00 - val_loss: 0.1391 - val_acc: 0.0000e+00\n",
            "Epoch 9/9\n",
            "1095/1095 [==============================] - 0s 207us/step - loss: 0.1673 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "9\n",
            "41\n",
            "Train on 1095 samples, validate on 365 samples\n",
            "Epoch 1/9\n",
            "1095/1095 [==============================] - 0s 177us/step - loss: 0.1669 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 2/9\n",
            "1095/1095 [==============================] - 0s 180us/step - loss: 0.1668 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "Epoch 3/9\n",
            "1095/1095 [==============================] - 0s 182us/step - loss: 0.1668 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 4/9\n",
            "1095/1095 [==============================] - 0s 180us/step - loss: 0.1669 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 5/9\n",
            "1095/1095 [==============================] - 0s 191us/step - loss: 0.1673 - acc: 0.0000e+00 - val_loss: 0.1388 - val_acc: 0.0000e+00\n",
            "Epoch 6/9\n",
            "1095/1095 [==============================] - 0s 185us/step - loss: 0.1670 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 7/9\n",
            "1095/1095 [==============================] - 0s 174us/step - loss: 0.1668 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 8/9\n",
            "1095/1095 [==============================] - 0s 179us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "Epoch 9/9\n",
            "1095/1095 [==============================] - 0s 171us/step - loss: 0.1671 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "9\n",
            "51\n",
            "Train on 1095 samples, validate on 365 samples\n",
            "Epoch 1/9\n",
            "1095/1095 [==============================] - 0s 142us/step - loss: 0.1669 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 2/9\n",
            "1095/1095 [==============================] - 0s 158us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 3/9\n",
            "1095/1095 [==============================] - 0s 156us/step - loss: 0.1670 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "Epoch 4/9\n",
            "1095/1095 [==============================] - 0s 148us/step - loss: 0.1669 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 5/9\n",
            "1095/1095 [==============================] - 0s 142us/step - loss: 0.1668 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "Epoch 6/9\n",
            "1095/1095 [==============================] - 0s 145us/step - loss: 0.1669 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 7/9\n",
            "1095/1095 [==============================] - 0s 143us/step - loss: 0.1674 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "Epoch 8/9\n",
            "1095/1095 [==============================] - 0s 146us/step - loss: 0.1680 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 9/9\n",
            "1095/1095 [==============================] - 0s 147us/step - loss: 0.1674 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "9\n",
            "61\n",
            "Train on 1095 samples, validate on 365 samples\n",
            "Epoch 1/9\n",
            "1095/1095 [==============================] - 0s 132us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 2/9\n",
            "1095/1095 [==============================] - 0s 145us/step - loss: 0.1670 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 3/9\n",
            "1095/1095 [==============================] - 0s 135us/step - loss: 0.1672 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "Epoch 4/9\n",
            "1095/1095 [==============================] - 0s 138us/step - loss: 0.1672 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 5/9\n",
            "1095/1095 [==============================] - 0s 138us/step - loss: 0.1669 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "Epoch 6/9\n",
            "1095/1095 [==============================] - 0s 134us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 7/9\n",
            "1095/1095 [==============================] - 0s 130us/step - loss: 0.1670 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 8/9\n",
            "1095/1095 [==============================] - 0s 142us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "Epoch 9/9\n",
            "1095/1095 [==============================] - 0s 131us/step - loss: 0.1668 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "9\n",
            "71\n",
            "Train on 1095 samples, validate on 365 samples\n",
            "Epoch 1/9\n",
            "1095/1095 [==============================] - 0s 115us/step - loss: 0.1667 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 2/9\n",
            "1095/1095 [==============================] - 0s 116us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "Epoch 3/9\n",
            "1095/1095 [==============================] - 0s 116us/step - loss: 0.1667 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 4/9\n",
            "1095/1095 [==============================] - 0s 116us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 5/9\n",
            "1095/1095 [==============================] - 0s 115us/step - loss: 0.1667 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 6/9\n",
            "1095/1095 [==============================] - 0s 115us/step - loss: 0.1669 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 7/9\n",
            "1095/1095 [==============================] - 0s 124us/step - loss: 0.1671 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "Epoch 8/9\n",
            "1095/1095 [==============================] - 0s 118us/step - loss: 0.1680 - acc: 0.0000e+00 - val_loss: 0.1388 - val_acc: 0.0000e+00\n",
            "Epoch 9/9\n",
            "1095/1095 [==============================] - 0s 120us/step - loss: 0.1669 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "9\n",
            "81\n",
            "Train on 1095 samples, validate on 365 samples\n",
            "Epoch 1/9\n",
            "1095/1095 [==============================] - 0s 112us/step - loss: 0.1667 - acc: 0.0000e+00 - val_loss: 0.1389 - val_acc: 0.0000e+00\n",
            "Epoch 2/9\n",
            "1095/1095 [==============================] - 0s 114us/step - loss: 0.1687 - acc: 0.0000e+00 - val_loss: 0.1391 - val_acc: 0.0000e+00\n",
            "Epoch 3/9\n",
            "1095/1095 [==============================] - 0s 116us/step - loss: 0.1669 - acc: 0.0000e+00 - val_loss: 0.1387 - val_acc: 0.0000e+00\n",
            "Epoch 4/9\n",
            "1095/1095 [==============================] - 0s 108us/step - loss: 0.1668 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 5/9\n",
            "1095/1095 [==============================] - 0s 106us/step - loss: 0.1673 - acc: 0.0000e+00 - val_loss: 0.1386 - val_acc: 0.0000e+00\n",
            "Epoch 6/9\n",
            "1095/1095 [==============================] - 0s 117us/step - loss: 0.1670 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 7/9\n",
            "1095/1095 [==============================] - 0s 105us/step - loss: 0.1676 - acc: 0.0000e+00 - val_loss: 0.1398 - val_acc: 0.0000e+00\n",
            "Epoch 8/9\n",
            "1095/1095 [==============================] - 0s 110us/step - loss: 0.1681 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 9/9\n",
            "1095/1095 [==============================] - 0s 106us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "9\n",
            "91\n",
            "Train on 1095 samples, validate on 365 samples\n",
            "Epoch 1/9\n",
            "1095/1095 [==============================] - 0s 98us/step - loss: 0.1674 - acc: 0.0000e+00 - val_loss: 0.1390 - val_acc: 0.0000e+00\n",
            "Epoch 2/9\n",
            "1095/1095 [==============================] - 0s 93us/step - loss: 0.1665 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "Epoch 3/9\n",
            "1095/1095 [==============================] - 0s 95us/step - loss: 0.1667 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 4/9\n",
            "1095/1095 [==============================] - 0s 98us/step - loss: 0.1667 - acc: 0.0000e+00 - val_loss: 0.1386 - val_acc: 0.0000e+00\n",
            "Epoch 5/9\n",
            "1095/1095 [==============================] - 0s 92us/step - loss: 0.1667 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 6/9\n",
            "1095/1095 [==============================] - 0s 102us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "Epoch 7/9\n",
            "1095/1095 [==============================] - 0s 95us/step - loss: 0.1668 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "Epoch 8/9\n",
            "1095/1095 [==============================] - 0s 91us/step - loss: 0.1671 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 9/9\n",
            "1095/1095 [==============================] - 0s 90us/step - loss: 0.1669 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "9\n",
            "101\n",
            "Train on 1095 samples, validate on 365 samples\n",
            "Epoch 1/9\n",
            "1095/1095 [==============================] - 0s 90us/step - loss: 0.1665 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 2/9\n",
            "1095/1095 [==============================] - 0s 84us/step - loss: 0.1667 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "Epoch 3/9\n",
            "1095/1095 [==============================] - 0s 89us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 4/9\n",
            "1095/1095 [==============================] - 0s 89us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 5/9\n",
            "1095/1095 [==============================] - 0s 93us/step - loss: 0.1669 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 6/9\n",
            "1095/1095 [==============================] - 0s 91us/step - loss: 0.1665 - acc: 0.0000e+00 - val_loss: 0.1386 - val_acc: 0.0000e+00\n",
            "Epoch 7/9\n",
            "1095/1095 [==============================] - 0s 95us/step - loss: 0.1667 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "Epoch 8/9\n",
            "1095/1095 [==============================] - 0s 87us/step - loss: 0.1668 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 9/9\n",
            "1095/1095 [==============================] - 0s 97us/step - loss: 0.1665 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "9\n",
            "111\n",
            "Train on 1095 samples, validate on 365 samples\n",
            "Epoch 1/9\n",
            "1095/1095 [==============================] - 0s 84us/step - loss: 0.1667 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "Epoch 2/9\n",
            "1095/1095 [==============================] - 0s 86us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1386 - val_acc: 0.0000e+00\n",
            "Epoch 3/9\n",
            "1095/1095 [==============================] - 0s 91us/step - loss: 0.1680 - acc: 0.0000e+00 - val_loss: 0.1392 - val_acc: 0.0000e+00\n",
            "Epoch 4/9\n",
            "1095/1095 [==============================] - 0s 88us/step - loss: 0.1669 - acc: 0.0000e+00 - val_loss: 0.1386 - val_acc: 0.0000e+00\n",
            "Epoch 5/9\n",
            "1095/1095 [==============================] - 0s 89us/step - loss: 0.1667 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "Epoch 6/9\n",
            "1095/1095 [==============================] - 0s 88us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 7/9\n",
            "1095/1095 [==============================] - 0s 87us/step - loss: 0.1669 - acc: 0.0000e+00 - val_loss: 0.1386 - val_acc: 0.0000e+00\n",
            "Epoch 8/9\n",
            "1095/1095 [==============================] - 0s 91us/step - loss: 0.1669 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 9/9\n",
            "1095/1095 [==============================] - 0s 86us/step - loss: 0.1694 - acc: 0.0000e+00 - val_loss: 0.1416 - val_acc: 0.0000e+00\n",
            "9\n",
            "121\n",
            "Train on 1095 samples, validate on 365 samples\n",
            "Epoch 1/9\n",
            "1095/1095 [==============================] - 0s 79us/step - loss: 0.1673 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 2/9\n",
            "1095/1095 [==============================] - 0s 81us/step - loss: 0.1670 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "Epoch 3/9\n",
            "1095/1095 [==============================] - 0s 81us/step - loss: 0.1665 - acc: 0.0000e+00 - val_loss: 0.1388 - val_acc: 0.0000e+00\n",
            "Epoch 4/9\n",
            "1095/1095 [==============================] - 0s 80us/step - loss: 0.1667 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 5/9\n",
            "1095/1095 [==============================] - 0s 77us/step - loss: 0.1669 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 6/9\n",
            "1095/1095 [==============================] - 0s 78us/step - loss: 0.1665 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "Epoch 7/9\n",
            "1095/1095 [==============================] - 0s 80us/step - loss: 0.1667 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "Epoch 8/9\n",
            "1095/1095 [==============================] - 0s 79us/step - loss: 0.1668 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 9/9\n",
            "1095/1095 [==============================] - 0s 80us/step - loss: 0.1672 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "9\n",
            "131\n",
            "Train on 1095 samples, validate on 365 samples\n",
            "Epoch 1/9\n",
            "1095/1095 [==============================] - 0s 78us/step - loss: 0.1665 - acc: 0.0000e+00 - val_loss: 0.1386 - val_acc: 0.0000e+00\n",
            "Epoch 2/9\n",
            "1095/1095 [==============================] - 0s 73us/step - loss: 0.1669 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 3/9\n",
            "1095/1095 [==============================] - 0s 78us/step - loss: 0.1680 - acc: 0.0000e+00 - val_loss: 0.1394 - val_acc: 0.0000e+00\n",
            "Epoch 4/9\n",
            "1095/1095 [==============================] - 0s 80us/step - loss: 0.1667 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 5/9\n",
            "1095/1095 [==============================] - 0s 77us/step - loss: 0.1667 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 6/9\n",
            "1095/1095 [==============================] - 0s 75us/step - loss: 0.1671 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 7/9\n",
            "1095/1095 [==============================] - 0s 77us/step - loss: 0.1667 - acc: 0.0000e+00 - val_loss: 0.1386 - val_acc: 0.0000e+00\n",
            "Epoch 8/9\n",
            "1095/1095 [==============================] - 0s 73us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 9/9\n",
            "1095/1095 [==============================] - 0s 74us/step - loss: 0.1665 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "9\n",
            "141\n",
            "Train on 1095 samples, validate on 365 samples\n",
            "Epoch 1/10\n",
            "1095/1095 [==============================] - 8s 7ms/step - loss: 0.1866 - acc: 0.0000e+00 - val_loss: 0.2142 - val_acc: 0.0000e+00\n",
            "Epoch 2/10\n",
            "1095/1095 [==============================] - 8s 7ms/step - loss: 0.1774 - acc: 0.0000e+00 - val_loss: 0.1487 - val_acc: 0.0000e+00\n",
            "Epoch 3/10\n",
            "1095/1095 [==============================] - 8s 7ms/step - loss: 0.1713 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "Epoch 4/10\n",
            "1095/1095 [==============================] - 8s 7ms/step - loss: 0.1737 - acc: 0.0000e+00 - val_loss: 0.1440 - val_acc: 0.0000e+00\n",
            "Epoch 5/10\n",
            "1095/1095 [==============================] - 8s 7ms/step - loss: 0.1763 - acc: 0.0000e+00 - val_loss: 0.1580 - val_acc: 0.0000e+00\n",
            "Epoch 6/10\n",
            "1095/1095 [==============================] - 8s 7ms/step - loss: 0.1751 - acc: 0.0000e+00 - val_loss: 0.1389 - val_acc: 0.0000e+00\n",
            "Epoch 7/10\n",
            "1095/1095 [==============================] - 8s 7ms/step - loss: 0.1759 - acc: 0.0000e+00 - val_loss: 0.1421 - val_acc: 0.0000e+00\n",
            "Epoch 8/10\n",
            "1095/1095 [==============================] - 8s 7ms/step - loss: 0.1757 - acc: 0.0000e+00 - val_loss: 0.1432 - val_acc: 0.0000e+00\n",
            "Epoch 9/10\n",
            "1095/1095 [==============================] - 8s 7ms/step - loss: 0.1770 - acc: 0.0000e+00 - val_loss: 0.1417 - val_acc: 0.0000e+00\n",
            "Epoch 10/10\n",
            "1095/1095 [==============================] - 8s 7ms/step - loss: 0.1752 - acc: 0.0000e+00 - val_loss: 0.1412 - val_acc: 0.0000e+00\n",
            "10\n",
            "1\n",
            "Train on 1095 samples, validate on 365 samples\n",
            "Epoch 1/10\n",
            "1095/1095 [==============================] - 1s 783us/step - loss: 0.1689 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 2/10\n",
            "1095/1095 [==============================] - 1s 772us/step - loss: 0.1675 - acc: 0.0000e+00 - val_loss: 0.1386 - val_acc: 0.0000e+00\n",
            "Epoch 3/10\n",
            "1095/1095 [==============================] - 1s 761us/step - loss: 0.1675 - acc: 0.0000e+00 - val_loss: 0.1387 - val_acc: 0.0000e+00\n",
            "Epoch 4/10\n",
            "1095/1095 [==============================] - 1s 767us/step - loss: 0.1678 - acc: 0.0000e+00 - val_loss: 0.1389 - val_acc: 0.0000e+00\n",
            "Epoch 5/10\n",
            "1095/1095 [==============================] - 1s 746us/step - loss: 0.1673 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 6/10\n",
            "1095/1095 [==============================] - 1s 775us/step - loss: 0.1683 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "Epoch 7/10\n",
            "1095/1095 [==============================] - 1s 769us/step - loss: 0.1674 - acc: 0.0000e+00 - val_loss: 0.1433 - val_acc: 0.0000e+00\n",
            "Epoch 8/10\n",
            "1095/1095 [==============================] - 1s 784us/step - loss: 0.1692 - acc: 0.0000e+00 - val_loss: 0.1390 - val_acc: 0.0000e+00\n",
            "Epoch 9/10\n",
            "1095/1095 [==============================] - 1s 770us/step - loss: 0.1676 - acc: 0.0000e+00 - val_loss: 0.1411 - val_acc: 0.0000e+00\n",
            "Epoch 10/10\n",
            "1095/1095 [==============================] - 1s 752us/step - loss: 0.1692 - acc: 0.0000e+00 - val_loss: 0.1402 - val_acc: 0.0000e+00\n",
            "10\n",
            "11\n",
            "Train on 1095 samples, validate on 365 samples\n",
            "Epoch 1/10\n",
            "1095/1095 [==============================] - 0s 397us/step - loss: 0.1675 - acc: 0.0000e+00 - val_loss: 0.1395 - val_acc: 0.0000e+00\n",
            "Epoch 2/10\n",
            "1095/1095 [==============================] - 0s 408us/step - loss: 0.1680 - acc: 0.0000e+00 - val_loss: 0.1389 - val_acc: 0.0000e+00\n",
            "Epoch 3/10\n",
            "1095/1095 [==============================] - 0s 419us/step - loss: 0.1667 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 4/10\n",
            "1095/1095 [==============================] - 0s 414us/step - loss: 0.1679 - acc: 0.0000e+00 - val_loss: 0.1386 - val_acc: 0.0000e+00\n",
            "Epoch 5/10\n",
            "1095/1095 [==============================] - 0s 406us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1400 - val_acc: 0.0000e+00\n",
            "Epoch 6/10\n",
            "1095/1095 [==============================] - 0s 393us/step - loss: 0.1671 - acc: 0.0000e+00 - val_loss: 0.1389 - val_acc: 0.0000e+00\n",
            "Epoch 7/10\n",
            "1095/1095 [==============================] - 0s 409us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1390 - val_acc: 0.0000e+00\n",
            "Epoch 8/10\n",
            "1095/1095 [==============================] - 0s 410us/step - loss: 0.1671 - acc: 0.0000e+00 - val_loss: 0.1409 - val_acc: 0.0000e+00\n",
            "Epoch 9/10\n",
            "1095/1095 [==============================] - 0s 410us/step - loss: 0.1681 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 10/10\n",
            "1095/1095 [==============================] - 0s 410us/step - loss: 0.1658 - acc: 0.0000e+00 - val_loss: 0.1445 - val_acc: 0.0000e+00\n",
            "10\n",
            "21\n",
            "Train on 1095 samples, validate on 365 samples\n",
            "Epoch 1/10\n",
            "1095/1095 [==============================] - 0s 275us/step - loss: 0.1690 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "Epoch 2/10\n",
            "1095/1095 [==============================] - 0s 290us/step - loss: 0.1673 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 3/10\n",
            "1095/1095 [==============================] - 0s 269us/step - loss: 0.1670 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 4/10\n",
            "1095/1095 [==============================] - 0s 291us/step - loss: 0.1669 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 5/10\n",
            "1095/1095 [==============================] - 0s 303us/step - loss: 0.1667 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 6/10\n",
            "1095/1095 [==============================] - 0s 284us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1397 - val_acc: 0.0000e+00\n",
            "Epoch 7/10\n",
            "1095/1095 [==============================] - 0s 287us/step - loss: 0.1676 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "Epoch 8/10\n",
            "1095/1095 [==============================] - 0s 286us/step - loss: 0.1685 - acc: 0.0000e+00 - val_loss: 0.1389 - val_acc: 0.0000e+00\n",
            "Epoch 9/10\n",
            "1095/1095 [==============================] - 0s 288us/step - loss: 0.1675 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "Epoch 10/10\n",
            "1095/1095 [==============================] - 0s 279us/step - loss: 0.1668 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "10\n",
            "31\n",
            "Train on 1095 samples, validate on 365 samples\n",
            "Epoch 1/10\n",
            "1095/1095 [==============================] - 0s 210us/step - loss: 0.1668 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 2/10\n",
            "1095/1095 [==============================] - 0s 222us/step - loss: 0.1676 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 3/10\n",
            "1095/1095 [==============================] - 0s 215us/step - loss: 0.1667 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "Epoch 4/10\n",
            "1095/1095 [==============================] - 0s 211us/step - loss: 0.1669 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 5/10\n",
            "1095/1095 [==============================] - 0s 215us/step - loss: 0.1671 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 6/10\n",
            "1095/1095 [==============================] - 0s 213us/step - loss: 0.1668 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 7/10\n",
            "1095/1095 [==============================] - 0s 220us/step - loss: 0.1670 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "Epoch 8/10\n",
            "1095/1095 [==============================] - 0s 206us/step - loss: 0.1669 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 9/10\n",
            "1095/1095 [==============================] - 0s 214us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "Epoch 10/10\n",
            "1095/1095 [==============================] - 0s 211us/step - loss: 0.1672 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "10\n",
            "41\n",
            "Train on 1095 samples, validate on 365 samples\n",
            "Epoch 1/10\n",
            "1095/1095 [==============================] - 0s 176us/step - loss: 0.1670 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 2/10\n",
            "1095/1095 [==============================] - 0s 178us/step - loss: 0.1668 - acc: 0.0000e+00 - val_loss: 0.1387 - val_acc: 0.0000e+00\n",
            "Epoch 3/10\n",
            "1095/1095 [==============================] - 0s 173us/step - loss: 0.1668 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 4/10\n",
            "1095/1095 [==============================] - 0s 178us/step - loss: 0.1668 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 5/10\n",
            "1095/1095 [==============================] - 0s 173us/step - loss: 0.1667 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 6/10\n",
            "1095/1095 [==============================] - 0s 178us/step - loss: 0.1669 - acc: 0.0000e+00 - val_loss: 0.1387 - val_acc: 0.0000e+00\n",
            "Epoch 7/10\n",
            "1095/1095 [==============================] - 0s 174us/step - loss: 0.1669 - acc: 0.0000e+00 - val_loss: 0.1387 - val_acc: 0.0000e+00\n",
            "Epoch 8/10\n",
            "1095/1095 [==============================] - 0s 176us/step - loss: 0.1668 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "Epoch 9/10\n",
            "1095/1095 [==============================] - 0s 172us/step - loss: 0.1668 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 10/10\n",
            "1095/1095 [==============================] - 0s 168us/step - loss: 0.1668 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "10\n",
            "51\n",
            "Train on 1095 samples, validate on 365 samples\n",
            "Epoch 1/10\n",
            "1095/1095 [==============================] - 0s 143us/step - loss: 0.1674 - acc: 0.0000e+00 - val_loss: 0.1387 - val_acc: 0.0000e+00\n",
            "Epoch 2/10\n",
            "1095/1095 [==============================] - 0s 148us/step - loss: 0.1673 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 3/10\n",
            "1095/1095 [==============================] - 0s 143us/step - loss: 0.1669 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "Epoch 4/10\n",
            "1095/1095 [==============================] - 0s 144us/step - loss: 0.1668 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "Epoch 5/10\n",
            "1095/1095 [==============================] - 0s 145us/step - loss: 0.1668 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 6/10\n",
            "1095/1095 [==============================] - 0s 140us/step - loss: 0.1667 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 7/10\n",
            "1095/1095 [==============================] - 0s 147us/step - loss: 0.1669 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 8/10\n",
            "1095/1095 [==============================] - 0s 147us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1386 - val_acc: 0.0000e+00\n",
            "Epoch 9/10\n",
            "1095/1095 [==============================] - 0s 140us/step - loss: 0.1669 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 10/10\n",
            "1095/1095 [==============================] - 0s 139us/step - loss: 0.1665 - acc: 0.0000e+00 - val_loss: 0.1387 - val_acc: 0.0000e+00\n",
            "10\n",
            "61\n",
            "Train on 1095 samples, validate on 365 samples\n",
            "Epoch 1/10\n",
            "1095/1095 [==============================] - 0s 130us/step - loss: 0.1671 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 2/10\n",
            "1095/1095 [==============================] - 0s 131us/step - loss: 0.1671 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 3/10\n",
            "1095/1095 [==============================] - 0s 133us/step - loss: 0.1670 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 4/10\n",
            "1095/1095 [==============================] - 0s 128us/step - loss: 0.1667 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 5/10\n",
            "1095/1095 [==============================] - 0s 138us/step - loss: 0.1668 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "Epoch 6/10\n",
            "1095/1095 [==============================] - 0s 132us/step - loss: 0.1673 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 7/10\n",
            "1095/1095 [==============================] - 0s 138us/step - loss: 0.1669 - acc: 0.0000e+00 - val_loss: 0.1390 - val_acc: 0.0000e+00\n",
            "Epoch 8/10\n",
            "1095/1095 [==============================] - 0s 131us/step - loss: 0.1673 - acc: 0.0000e+00 - val_loss: 0.1389 - val_acc: 0.0000e+00\n",
            "Epoch 9/10\n",
            "1095/1095 [==============================] - 0s 135us/step - loss: 0.1693 - acc: 0.0000e+00 - val_loss: 0.1389 - val_acc: 0.0000e+00\n",
            "Epoch 10/10\n",
            "1095/1095 [==============================] - 0s 134us/step - loss: 0.1667 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "10\n",
            "71\n",
            "Train on 1095 samples, validate on 365 samples\n",
            "Epoch 1/10\n",
            "1095/1095 [==============================] - 0s 115us/step - loss: 0.1667 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 2/10\n",
            "1095/1095 [==============================] - 0s 123us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 3/10\n",
            "1095/1095 [==============================] - 0s 118us/step - loss: 0.1668 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 4/10\n",
            "1095/1095 [==============================] - 0s 119us/step - loss: 0.1668 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 5/10\n",
            "1095/1095 [==============================] - 0s 131us/step - loss: 0.1667 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 6/10\n",
            "1095/1095 [==============================] - 0s 129us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 7/10\n",
            "1095/1095 [==============================] - 0s 121us/step - loss: 0.1669 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 8/10\n",
            "1095/1095 [==============================] - 0s 114us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 9/10\n",
            "1095/1095 [==============================] - 0s 117us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 10/10\n",
            "1095/1095 [==============================] - 0s 130us/step - loss: 0.1670 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "10\n",
            "81\n",
            "Train on 1095 samples, validate on 365 samples\n",
            "Epoch 1/10\n",
            "1095/1095 [==============================] - 0s 105us/step - loss: 0.1668 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 2/10\n",
            "1095/1095 [==============================] - 0s 108us/step - loss: 0.1693 - acc: 0.0000e+00 - val_loss: 0.1386 - val_acc: 0.0000e+00\n",
            "Epoch 3/10\n",
            "1095/1095 [==============================] - 0s 108us/step - loss: 0.1675 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 4/10\n",
            "1095/1095 [==============================] - 0s 109us/step - loss: 0.1711 - acc: 0.0000e+00 - val_loss: 0.1387 - val_acc: 0.0000e+00\n",
            "Epoch 5/10\n",
            "1095/1095 [==============================] - 0s 109us/step - loss: 0.1682 - acc: 0.0000e+00 - val_loss: 0.1398 - val_acc: 0.0000e+00\n",
            "Epoch 6/10\n",
            "1095/1095 [==============================] - 0s 108us/step - loss: 0.1686 - acc: 0.0000e+00 - val_loss: 0.1386 - val_acc: 0.0000e+00\n",
            "Epoch 7/10\n",
            "1095/1095 [==============================] - 0s 115us/step - loss: 0.1690 - acc: 0.0000e+00 - val_loss: 0.1413 - val_acc: 0.0000e+00\n",
            "Epoch 8/10\n",
            "1095/1095 [==============================] - 0s 116us/step - loss: 0.1665 - acc: 0.0000e+00 - val_loss: 0.1407 - val_acc: 0.0000e+00\n",
            "Epoch 9/10\n",
            "1095/1095 [==============================] - 0s 107us/step - loss: 0.1690 - acc: 0.0000e+00 - val_loss: 0.1388 - val_acc: 0.0000e+00\n",
            "Epoch 10/10\n",
            "1095/1095 [==============================] - 0s 105us/step - loss: 0.1719 - acc: 0.0000e+00 - val_loss: 0.1400 - val_acc: 0.0000e+00\n",
            "10\n",
            "91\n",
            "Train on 1095 samples, validate on 365 samples\n",
            "Epoch 1/10\n",
            "1095/1095 [==============================] - 0s 93us/step - loss: 0.1667 - acc: 0.0000e+00 - val_loss: 0.1390 - val_acc: 0.0000e+00\n",
            "Epoch 2/10\n",
            "1095/1095 [==============================] - 0s 91us/step - loss: 0.1673 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "Epoch 3/10\n",
            "1095/1095 [==============================] - 0s 94us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 4/10\n",
            "1095/1095 [==============================] - 0s 93us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 5/10\n",
            "1095/1095 [==============================] - 0s 94us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 6/10\n",
            "1095/1095 [==============================] - 0s 94us/step - loss: 0.1668 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 7/10\n",
            "1095/1095 [==============================] - 0s 102us/step - loss: 0.1669 - acc: 0.0000e+00 - val_loss: 0.1389 - val_acc: 0.0000e+00\n",
            "Epoch 8/10\n",
            "1095/1095 [==============================] - 0s 94us/step - loss: 0.1667 - acc: 0.0000e+00 - val_loss: 0.1386 - val_acc: 0.0000e+00\n",
            "Epoch 9/10\n",
            "1095/1095 [==============================] - 0s 94us/step - loss: 0.1670 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "Epoch 10/10\n",
            "1095/1095 [==============================] - 0s 100us/step - loss: 0.1667 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "10\n",
            "101\n",
            "Train on 1095 samples, validate on 365 samples\n",
            "Epoch 1/10\n",
            "1095/1095 [==============================] - 0s 85us/step - loss: 0.1669 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "Epoch 2/10\n",
            "1095/1095 [==============================] - 0s 85us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 3/10\n",
            "1095/1095 [==============================] - 0s 90us/step - loss: 0.1669 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 4/10\n",
            "1095/1095 [==============================] - 0s 91us/step - loss: 0.1669 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 5/10\n",
            "1095/1095 [==============================] - 0s 87us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 6/10\n",
            "1095/1095 [==============================] - 0s 90us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 7/10\n",
            "1095/1095 [==============================] - 0s 100us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 8/10\n",
            "1095/1095 [==============================] - 0s 88us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 9/10\n",
            "1095/1095 [==============================] - 0s 85us/step - loss: 0.1671 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "Epoch 10/10\n",
            "1095/1095 [==============================] - 0s 88us/step - loss: 0.1664 - acc: 0.0000e+00 - val_loss: 0.1388 - val_acc: 0.0000e+00\n",
            "10\n",
            "111\n",
            "Train on 1095 samples, validate on 365 samples\n",
            "Epoch 1/10\n",
            "1095/1095 [==============================] - 0s 86us/step - loss: 0.1668 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 2/10\n",
            "1095/1095 [==============================] - 0s 87us/step - loss: 0.1669 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 3/10\n",
            "1095/1095 [==============================] - 0s 90us/step - loss: 0.1664 - acc: 0.0000e+00 - val_loss: 0.1387 - val_acc: 0.0000e+00\n",
            "Epoch 4/10\n",
            "1095/1095 [==============================] - 0s 88us/step - loss: 0.1671 - acc: 0.0000e+00 - val_loss: 0.1388 - val_acc: 0.0000e+00\n",
            "Epoch 5/10\n",
            "1095/1095 [==============================] - 0s 90us/step - loss: 0.1675 - acc: 0.0000e+00 - val_loss: 0.1386 - val_acc: 0.0000e+00\n",
            "Epoch 6/10\n",
            "1095/1095 [==============================] - 0s 96us/step - loss: 0.1673 - acc: 0.0000e+00 - val_loss: 0.1389 - val_acc: 0.0000e+00\n",
            "Epoch 7/10\n",
            "1095/1095 [==============================] - 0s 94us/step - loss: 0.1674 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 8/10\n",
            "1095/1095 [==============================] - 0s 86us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 9/10\n",
            "1095/1095 [==============================] - 0s 87us/step - loss: 0.1673 - acc: 0.0000e+00 - val_loss: 0.1387 - val_acc: 0.0000e+00\n",
            "Epoch 10/10\n",
            "1095/1095 [==============================] - 0s 86us/step - loss: 0.1690 - acc: 0.0000e+00 - val_loss: 0.1391 - val_acc: 0.0000e+00\n",
            "10\n",
            "121\n",
            "Train on 1095 samples, validate on 365 samples\n",
            "Epoch 1/10\n",
            "1095/1095 [==============================] - 0s 76us/step - loss: 0.1675 - acc: 0.0000e+00 - val_loss: 0.1397 - val_acc: 0.0000e+00\n",
            "Epoch 2/10\n",
            "1095/1095 [==============================] - 0s 85us/step - loss: 0.1671 - acc: 0.0000e+00 - val_loss: 0.1386 - val_acc: 0.0000e+00\n",
            "Epoch 3/10\n",
            "1095/1095 [==============================] - 0s 86us/step - loss: 0.1667 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 4/10\n",
            "1095/1095 [==============================] - 0s 87us/step - loss: 0.1667 - acc: 0.0000e+00 - val_loss: 0.1386 - val_acc: 0.0000e+00\n",
            "Epoch 5/10\n",
            "1095/1095 [==============================] - 0s 84us/step - loss: 0.1667 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 6/10\n",
            "1095/1095 [==============================] - 0s 90us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 7/10\n",
            "1095/1095 [==============================] - 0s 92us/step - loss: 0.1667 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 8/10\n",
            "1095/1095 [==============================] - 0s 84us/step - loss: 0.1670 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 9/10\n",
            "1095/1095 [==============================] - 0s 88us/step - loss: 0.1665 - acc: 0.0000e+00 - val_loss: 0.1388 - val_acc: 0.0000e+00\n",
            "Epoch 10/10\n",
            "1095/1095 [==============================] - 0s 83us/step - loss: 0.1667 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "10\n",
            "131\n",
            "Train on 1095 samples, validate on 365 samples\n",
            "Epoch 1/10\n",
            "1095/1095 [==============================] - 0s 76us/step - loss: 0.1665 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 2/10\n",
            "1095/1095 [==============================] - 0s 76us/step - loss: 0.1665 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 3/10\n",
            "1095/1095 [==============================] - 0s 77us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "Epoch 4/10\n",
            "1095/1095 [==============================] - 0s 75us/step - loss: 0.1665 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 5/10\n",
            "1095/1095 [==============================] - 0s 74us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 6/10\n",
            "1095/1095 [==============================] - 0s 73us/step - loss: 0.1667 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 7/10\n",
            "1095/1095 [==============================] - 0s 75us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 8/10\n",
            "1095/1095 [==============================] - 0s 79us/step - loss: 0.1670 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "Epoch 9/10\n",
            "1095/1095 [==============================] - 0s 74us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 10/10\n",
            "1095/1095 [==============================] - 0s 74us/step - loss: 0.1665 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "10\n",
            "141\n",
            "Train on 1095 samples, validate on 365 samples\n",
            "Epoch 1/11\n",
            "1095/1095 [==============================] - 8s 7ms/step - loss: 0.1792 - acc: 0.0000e+00 - val_loss: 0.1741 - val_acc: 0.0000e+00\n",
            "Epoch 2/11\n",
            "1095/1095 [==============================] - 8s 7ms/step - loss: 0.1757 - acc: 0.0000e+00 - val_loss: 0.1498 - val_acc: 0.0000e+00\n",
            "Epoch 3/11\n",
            "1095/1095 [==============================] - 8s 7ms/step - loss: 0.1760 - acc: 0.0000e+00 - val_loss: 0.1561 - val_acc: 0.0000e+00\n",
            "Epoch 4/11\n",
            "1095/1095 [==============================] - 8s 7ms/step - loss: 0.1785 - acc: 0.0000e+00 - val_loss: 0.1428 - val_acc: 0.0000e+00\n",
            "Epoch 5/11\n",
            "1095/1095 [==============================] - 8s 7ms/step - loss: 0.1771 - acc: 0.0000e+00 - val_loss: 0.1714 - val_acc: 0.0000e+00\n",
            "Epoch 6/11\n",
            "1095/1095 [==============================] - 8s 7ms/step - loss: 0.1755 - acc: 0.0000e+00 - val_loss: 0.1509 - val_acc: 0.0000e+00\n",
            "Epoch 7/11\n",
            "1095/1095 [==============================] - 8s 7ms/step - loss: 0.1776 - acc: 0.0000e+00 - val_loss: 0.1420 - val_acc: 0.0000e+00\n",
            "Epoch 8/11\n",
            "1095/1095 [==============================] - 8s 7ms/step - loss: 0.1750 - acc: 0.0000e+00 - val_loss: 0.1393 - val_acc: 0.0000e+00\n",
            "Epoch 9/11\n",
            "1095/1095 [==============================] - 9s 8ms/step - loss: 0.1765 - acc: 0.0000e+00 - val_loss: 0.1490 - val_acc: 0.0000e+00\n",
            "Epoch 10/11\n",
            "1095/1095 [==============================] - 8s 7ms/step - loss: 0.1756 - acc: 0.0000e+00 - val_loss: 0.1646 - val_acc: 0.0000e+00\n",
            "Epoch 11/11\n",
            "1095/1095 [==============================] - 8s 7ms/step - loss: 0.1776 - acc: 0.0000e+00 - val_loss: 0.1446 - val_acc: 0.0000e+00\n",
            "11\n",
            "1\n",
            "Train on 1095 samples, validate on 365 samples\n",
            "Epoch 1/11\n",
            "1095/1095 [==============================] - 1s 766us/step - loss: 0.1700 - acc: 0.0000e+00 - val_loss: 0.1395 - val_acc: 0.0000e+00\n",
            "Epoch 2/11\n",
            "1095/1095 [==============================] - 1s 760us/step - loss: 0.1687 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 3/11\n",
            "1095/1095 [==============================] - 1s 777us/step - loss: 0.1675 - acc: 0.0000e+00 - val_loss: 0.1395 - val_acc: 0.0000e+00\n",
            "Epoch 4/11\n",
            "1095/1095 [==============================] - 1s 784us/step - loss: 0.1674 - acc: 0.0000e+00 - val_loss: 0.1400 - val_acc: 0.0000e+00\n",
            "Epoch 5/11\n",
            "1095/1095 [==============================] - 1s 755us/step - loss: 0.1673 - acc: 0.0000e+00 - val_loss: 0.1396 - val_acc: 0.0000e+00\n",
            "Epoch 6/11\n",
            "1095/1095 [==============================] - 1s 747us/step - loss: 0.1681 - acc: 0.0000e+00 - val_loss: 0.1387 - val_acc: 0.0000e+00\n",
            "Epoch 7/11\n",
            "1095/1095 [==============================] - 1s 788us/step - loss: 0.1675 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 8/11\n",
            "1095/1095 [==============================] - 1s 766us/step - loss: 0.1673 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 9/11\n",
            "1095/1095 [==============================] - 1s 758us/step - loss: 0.1672 - acc: 0.0000e+00 - val_loss: 0.1449 - val_acc: 0.0000e+00\n",
            "Epoch 10/11\n",
            "1095/1095 [==============================] - 1s 768us/step - loss: 0.1683 - acc: 0.0000e+00 - val_loss: 0.1388 - val_acc: 0.0000e+00\n",
            "Epoch 11/11\n",
            "1095/1095 [==============================] - 1s 768us/step - loss: 0.1672 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "11\n",
            "11\n",
            "Train on 1095 samples, validate on 365 samples\n",
            "Epoch 1/11\n",
            "1095/1095 [==============================] - 0s 395us/step - loss: 0.1670 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 2/11\n",
            "1095/1095 [==============================] - 0s 411us/step - loss: 0.1669 - acc: 0.0000e+00 - val_loss: 0.1392 - val_acc: 0.0000e+00\n",
            "Epoch 3/11\n",
            "1095/1095 [==============================] - 0s 392us/step - loss: 0.1672 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "Epoch 4/11\n",
            "1095/1095 [==============================] - 0s 408us/step - loss: 0.1667 - acc: 0.0000e+00 - val_loss: 0.1390 - val_acc: 0.0000e+00\n",
            "Epoch 5/11\n",
            "1095/1095 [==============================] - 0s 403us/step - loss: 0.1671 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 6/11\n",
            "1095/1095 [==============================] - 0s 407us/step - loss: 0.1674 - acc: 0.0000e+00 - val_loss: 0.1389 - val_acc: 0.0000e+00\n",
            "Epoch 7/11\n",
            "1095/1095 [==============================] - 0s 414us/step - loss: 0.1673 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 8/11\n",
            "1095/1095 [==============================] - 0s 424us/step - loss: 0.1668 - acc: 0.0000e+00 - val_loss: 0.1386 - val_acc: 0.0000e+00\n",
            "Epoch 9/11\n",
            "1095/1095 [==============================] - 0s 425us/step - loss: 0.1678 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "Epoch 10/11\n",
            "1095/1095 [==============================] - 0s 402us/step - loss: 0.1675 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 11/11\n",
            "1095/1095 [==============================] - 0s 412us/step - loss: 0.1671 - acc: 0.0000e+00 - val_loss: 0.1389 - val_acc: 0.0000e+00\n",
            "11\n",
            "21\n",
            "Train on 1095 samples, validate on 365 samples\n",
            "Epoch 1/11\n",
            "1095/1095 [==============================] - 0s 296us/step - loss: 0.1669 - acc: 0.0000e+00 - val_loss: 0.1387 - val_acc: 0.0000e+00\n",
            "Epoch 2/11\n",
            "1095/1095 [==============================] - 0s 288us/step - loss: 0.1672 - acc: 0.0000e+00 - val_loss: 0.1395 - val_acc: 0.0000e+00\n",
            "Epoch 3/11\n",
            "1095/1095 [==============================] - 0s 290us/step - loss: 0.1678 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "Epoch 4/11\n",
            "1095/1095 [==============================] - 0s 283us/step - loss: 0.1668 - acc: 0.0000e+00 - val_loss: 0.1388 - val_acc: 0.0000e+00\n",
            "Epoch 5/11\n",
            "1095/1095 [==============================] - 0s 281us/step - loss: 0.1670 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 6/11\n",
            "1095/1095 [==============================] - 0s 284us/step - loss: 0.1671 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 7/11\n",
            "1095/1095 [==============================] - 0s 294us/step - loss: 0.1673 - acc: 0.0000e+00 - val_loss: 0.1386 - val_acc: 0.0000e+00\n",
            "Epoch 8/11\n",
            "1095/1095 [==============================] - 0s 291us/step - loss: 0.1670 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "Epoch 9/11\n",
            "1095/1095 [==============================] - 0s 280us/step - loss: 0.1669 - acc: 0.0000e+00 - val_loss: 0.1407 - val_acc: 0.0000e+00\n",
            "Epoch 10/11\n",
            "1095/1095 [==============================] - 0s 291us/step - loss: 0.1673 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 11/11\n",
            "1095/1095 [==============================] - 0s 287us/step - loss: 0.1673 - acc: 0.0000e+00 - val_loss: 0.1390 - val_acc: 0.0000e+00\n",
            "11\n",
            "31\n",
            "Train on 1095 samples, validate on 365 samples\n",
            "Epoch 1/11\n",
            "1095/1095 [==============================] - 0s 214us/step - loss: 0.1667 - acc: 0.0000e+00 - val_loss: 0.1390 - val_acc: 0.0000e+00\n",
            "Epoch 2/11\n",
            "1095/1095 [==============================] - 0s 220us/step - loss: 0.1671 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 3/11\n",
            "1095/1095 [==============================] - 0s 216us/step - loss: 0.1669 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 4/11\n",
            "1095/1095 [==============================] - 0s 221us/step - loss: 0.1673 - acc: 0.0000e+00 - val_loss: 0.1411 - val_acc: 0.0000e+00\n",
            "Epoch 5/11\n",
            "1095/1095 [==============================] - 0s 211us/step - loss: 0.1681 - acc: 0.0000e+00 - val_loss: 0.1387 - val_acc: 0.0000e+00\n",
            "Epoch 6/11\n",
            "1095/1095 [==============================] - 0s 210us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 7/11\n",
            "1095/1095 [==============================] - 0s 208us/step - loss: 0.1668 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "Epoch 8/11\n",
            "1095/1095 [==============================] - 0s 230us/step - loss: 0.1671 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 9/11\n",
            "1095/1095 [==============================] - 0s 214us/step - loss: 0.1670 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 10/11\n",
            "1095/1095 [==============================] - 0s 219us/step - loss: 0.1669 - acc: 0.0000e+00 - val_loss: 0.1390 - val_acc: 0.0000e+00\n",
            "Epoch 11/11\n",
            "1095/1095 [==============================] - 0s 226us/step - loss: 0.1670 - acc: 0.0000e+00 - val_loss: 0.1394 - val_acc: 0.0000e+00\n",
            "11\n",
            "41\n",
            "Train on 1095 samples, validate on 365 samples\n",
            "Epoch 1/11\n",
            "1095/1095 [==============================] - 0s 189us/step - loss: 0.1672 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 2/11\n",
            "1095/1095 [==============================] - 0s 181us/step - loss: 0.1669 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 3/11\n",
            "1095/1095 [==============================] - 0s 185us/step - loss: 0.1669 - acc: 0.0000e+00 - val_loss: 0.1390 - val_acc: 0.0000e+00\n",
            "Epoch 4/11\n",
            "1095/1095 [==============================] - 0s 185us/step - loss: 0.1677 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 5/11\n",
            "1095/1095 [==============================] - 0s 187us/step - loss: 0.1673 - acc: 0.0000e+00 - val_loss: 0.1386 - val_acc: 0.0000e+00\n",
            "Epoch 6/11\n",
            "1095/1095 [==============================] - 0s 176us/step - loss: 0.1672 - acc: 0.0000e+00 - val_loss: 0.1386 - val_acc: 0.0000e+00\n",
            "Epoch 7/11\n",
            "1095/1095 [==============================] - 0s 176us/step - loss: 0.1669 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "Epoch 8/11\n",
            "1095/1095 [==============================] - 0s 174us/step - loss: 0.1668 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "Epoch 9/11\n",
            "1095/1095 [==============================] - 0s 181us/step - loss: 0.1679 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "Epoch 10/11\n",
            "1095/1095 [==============================] - 0s 189us/step - loss: 0.1672 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 11/11\n",
            "1095/1095 [==============================] - 0s 170us/step - loss: 0.1668 - acc: 0.0000e+00 - val_loss: 0.1391 - val_acc: 0.0000e+00\n",
            "11\n",
            "51\n",
            "Train on 1095 samples, validate on 365 samples\n",
            "Epoch 1/11\n",
            "1095/1095 [==============================] - 0s 139us/step - loss: 0.1667 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 2/11\n",
            "1095/1095 [==============================] - 0s 140us/step - loss: 0.1674 - acc: 0.0000e+00 - val_loss: 0.1389 - val_acc: 0.0000e+00\n",
            "Epoch 3/11\n",
            "1095/1095 [==============================] - 0s 144us/step - loss: 0.1667 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 4/11\n",
            "1095/1095 [==============================] - 0s 150us/step - loss: 0.1667 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 5/11\n",
            "1095/1095 [==============================] - 0s 145us/step - loss: 0.1671 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 6/11\n",
            "1095/1095 [==============================] - 0s 139us/step - loss: 0.1668 - acc: 0.0000e+00 - val_loss: 0.1388 - val_acc: 0.0000e+00\n",
            "Epoch 7/11\n",
            "1095/1095 [==============================] - 0s 147us/step - loss: 0.1669 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 8/11\n",
            "1095/1095 [==============================] - 0s 144us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "Epoch 9/11\n",
            "1095/1095 [==============================] - 0s 145us/step - loss: 0.1667 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "Epoch 10/11\n",
            "1095/1095 [==============================] - 0s 157us/step - loss: 0.1670 - acc: 0.0000e+00 - val_loss: 0.1387 - val_acc: 0.0000e+00\n",
            "Epoch 11/11\n",
            "1095/1095 [==============================] - 0s 139us/step - loss: 0.1669 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "11\n",
            "61\n",
            "Train on 1095 samples, validate on 365 samples\n",
            "Epoch 1/11\n",
            "1095/1095 [==============================] - 0s 130us/step - loss: 0.1667 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 2/11\n",
            "1095/1095 [==============================] - 0s 133us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 3/11\n",
            "1095/1095 [==============================] - 0s 133us/step - loss: 0.1669 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 4/11\n",
            "1095/1095 [==============================] - 0s 136us/step - loss: 0.1667 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 5/11\n",
            "1095/1095 [==============================] - 0s 133us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 6/11\n",
            "1095/1095 [==============================] - 0s 147us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 7/11\n",
            "1095/1095 [==============================] - 0s 133us/step - loss: 0.1668 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 8/11\n",
            "1095/1095 [==============================] - 0s 135us/step - loss: 0.1667 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 9/11\n",
            "1095/1095 [==============================] - 0s 134us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "Epoch 10/11\n",
            "1095/1095 [==============================] - 0s 128us/step - loss: 0.1667 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "Epoch 11/11\n",
            "1095/1095 [==============================] - 0s 141us/step - loss: 0.1672 - acc: 0.0000e+00 - val_loss: 0.1386 - val_acc: 0.0000e+00\n",
            "11\n",
            "71\n",
            "Train on 1095 samples, validate on 365 samples\n",
            "Epoch 1/11\n",
            "1095/1095 [==============================] - 0s 111us/step - loss: 0.1668 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 2/11\n",
            "1095/1095 [==============================] - 0s 128us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 3/11\n",
            "1095/1095 [==============================] - 0s 118us/step - loss: 0.1668 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 4/11\n",
            "1095/1095 [==============================] - 0s 120us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 5/11\n",
            "1095/1095 [==============================] - 0s 118us/step - loss: 0.1668 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "Epoch 6/11\n",
            "1095/1095 [==============================] - 0s 116us/step - loss: 0.1671 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "Epoch 7/11\n",
            "1095/1095 [==============================] - 0s 115us/step - loss: 0.1675 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 8/11\n",
            "1095/1095 [==============================] - 0s 116us/step - loss: 0.1673 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 9/11\n",
            "1095/1095 [==============================] - 0s 121us/step - loss: 0.1672 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 10/11\n",
            "1095/1095 [==============================] - 0s 128us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "Epoch 11/11\n",
            "1095/1095 [==============================] - 0s 114us/step - loss: 0.1668 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "11\n",
            "81\n",
            "Train on 1095 samples, validate on 365 samples\n",
            "Epoch 1/11\n",
            "1095/1095 [==============================] - 0s 103us/step - loss: 0.1669 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 2/11\n",
            "1095/1095 [==============================] - 0s 112us/step - loss: 0.1667 - acc: 0.0000e+00 - val_loss: 0.1386 - val_acc: 0.0000e+00\n",
            "Epoch 3/11\n",
            "1095/1095 [==============================] - 0s 106us/step - loss: 0.1674 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 4/11\n",
            "1095/1095 [==============================] - 0s 107us/step - loss: 0.1677 - acc: 0.0000e+00 - val_loss: 0.1387 - val_acc: 0.0000e+00\n",
            "Epoch 5/11\n",
            "1095/1095 [==============================] - 0s 104us/step - loss: 0.1673 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 6/11\n",
            "1095/1095 [==============================] - 0s 108us/step - loss: 0.1675 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "Epoch 7/11\n",
            "1095/1095 [==============================] - 0s 114us/step - loss: 0.1670 - acc: 0.0000e+00 - val_loss: 0.1386 - val_acc: 0.0000e+00\n",
            "Epoch 8/11\n",
            "1095/1095 [==============================] - 0s 109us/step - loss: 0.1670 - acc: 0.0000e+00 - val_loss: 0.1387 - val_acc: 0.0000e+00\n",
            "Epoch 9/11\n",
            "1095/1095 [==============================] - 0s 111us/step - loss: 0.1676 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "Epoch 10/11\n",
            "1095/1095 [==============================] - 0s 108us/step - loss: 0.1671 - acc: 0.0000e+00 - val_loss: 0.1387 - val_acc: 0.0000e+00\n",
            "Epoch 11/11\n",
            "1095/1095 [==============================] - 0s 110us/step - loss: 0.1669 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "11\n",
            "91\n",
            "Train on 1095 samples, validate on 365 samples\n",
            "Epoch 1/11\n",
            "1095/1095 [==============================] - 0s 91us/step - loss: 0.1667 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "Epoch 2/11\n",
            "1095/1095 [==============================] - 0s 94us/step - loss: 0.1669 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 3/11\n",
            "1095/1095 [==============================] - 0s 92us/step - loss: 0.1674 - acc: 0.0000e+00 - val_loss: 0.1388 - val_acc: 0.0000e+00\n",
            "Epoch 4/11\n",
            "1095/1095 [==============================] - 0s 92us/step - loss: 0.1668 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "Epoch 5/11\n",
            "1095/1095 [==============================] - 0s 109us/step - loss: 0.1668 - acc: 0.0000e+00 - val_loss: 0.1386 - val_acc: 0.0000e+00\n",
            "Epoch 6/11\n",
            "1095/1095 [==============================] - 0s 98us/step - loss: 0.1669 - acc: 0.0000e+00 - val_loss: 0.1386 - val_acc: 0.0000e+00\n",
            "Epoch 7/11\n",
            "1095/1095 [==============================] - 0s 95us/step - loss: 0.1665 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 8/11\n",
            "1095/1095 [==============================] - 0s 94us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 9/11\n",
            "1095/1095 [==============================] - 0s 97us/step - loss: 0.1671 - acc: 0.0000e+00 - val_loss: 0.1386 - val_acc: 0.0000e+00\n",
            "Epoch 10/11\n",
            "1095/1095 [==============================] - 0s 93us/step - loss: 0.1668 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 11/11\n",
            "1095/1095 [==============================] - 0s 92us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "11\n",
            "101\n",
            "Train on 1095 samples, validate on 365 samples\n",
            "Epoch 1/11\n",
            "1095/1095 [==============================] - 0s 85us/step - loss: 0.1669 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "Epoch 2/11\n",
            "1095/1095 [==============================] - 0s 85us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 3/11\n",
            "1095/1095 [==============================] - 0s 90us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 4/11\n",
            "1095/1095 [==============================] - 0s 97us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 5/11\n",
            "1095/1095 [==============================] - 0s 84us/step - loss: 0.1667 - acc: 0.0000e+00 - val_loss: 0.1386 - val_acc: 0.0000e+00\n",
            "Epoch 6/11\n",
            "1095/1095 [==============================] - 0s 85us/step - loss: 0.1669 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "Epoch 7/11\n",
            "1095/1095 [==============================] - 0s 85us/step - loss: 0.1665 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 8/11\n",
            "1095/1095 [==============================] - 0s 85us/step - loss: 0.1669 - acc: 0.0000e+00 - val_loss: 0.1387 - val_acc: 0.0000e+00\n",
            "Epoch 9/11\n",
            "1095/1095 [==============================] - 0s 85us/step - loss: 0.1667 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "Epoch 10/11\n",
            "1095/1095 [==============================] - 0s 86us/step - loss: 0.1665 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 11/11\n",
            "1095/1095 [==============================] - 0s 83us/step - loss: 0.1665 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "11\n",
            "111\n",
            "Train on 1095 samples, validate on 365 samples\n",
            "Epoch 1/11\n",
            "1095/1095 [==============================] - 0s 82us/step - loss: 0.1667 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 2/11\n",
            "1095/1095 [==============================] - 0s 86us/step - loss: 0.1675 - acc: 0.0000e+00 - val_loss: 0.1388 - val_acc: 0.0000e+00\n",
            "Epoch 3/11\n",
            "1095/1095 [==============================] - 0s 94us/step - loss: 0.1673 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "Epoch 4/11\n",
            "1095/1095 [==============================] - 0s 87us/step - loss: 0.1669 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "Epoch 5/11\n",
            "1095/1095 [==============================] - 0s 89us/step - loss: 0.1704 - acc: 0.0000e+00 - val_loss: 0.1406 - val_acc: 0.0000e+00\n",
            "Epoch 6/11\n",
            "1095/1095 [==============================] - 0s 88us/step - loss: 0.1691 - acc: 0.0000e+00 - val_loss: 0.1392 - val_acc: 0.0000e+00\n",
            "Epoch 7/11\n",
            "1095/1095 [==============================] - 0s 90us/step - loss: 0.1692 - acc: 0.0000e+00 - val_loss: 0.1402 - val_acc: 0.0000e+00\n",
            "Epoch 8/11\n",
            "1095/1095 [==============================] - 0s 89us/step - loss: 0.1669 - acc: 0.0000e+00 - val_loss: 0.1398 - val_acc: 0.0000e+00\n",
            "Epoch 9/11\n",
            "1095/1095 [==============================] - 0s 87us/step - loss: 0.1691 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 10/11\n",
            "1095/1095 [==============================] - 0s 89us/step - loss: 0.1669 - acc: 0.0000e+00 - val_loss: 0.1403 - val_acc: 0.0000e+00\n",
            "Epoch 11/11\n",
            "1095/1095 [==============================] - 0s 99us/step - loss: 0.1697 - acc: 0.0000e+00 - val_loss: 0.1393 - val_acc: 0.0000e+00\n",
            "11\n",
            "121\n",
            "Train on 1095 samples, validate on 365 samples\n",
            "Epoch 1/11\n",
            "1095/1095 [==============================] - 0s 81us/step - loss: 0.1668 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 2/11\n",
            "1095/1095 [==============================] - 0s 87us/step - loss: 0.1668 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 3/11\n",
            "1095/1095 [==============================] - 0s 79us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 4/11\n",
            "1095/1095 [==============================] - 0s 81us/step - loss: 0.1665 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 5/11\n",
            "1095/1095 [==============================] - 0s 80us/step - loss: 0.1665 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 6/11\n",
            "1095/1095 [==============================] - 0s 82us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 7/11\n",
            "1095/1095 [==============================] - 0s 81us/step - loss: 0.1667 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 8/11\n",
            "1095/1095 [==============================] - 0s 78us/step - loss: 0.1667 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "Epoch 9/11\n",
            "1095/1095 [==============================] - 0s 81us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 10/11\n",
            "1095/1095 [==============================] - 0s 77us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 11/11\n",
            "1095/1095 [==============================] - 0s 80us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "11\n",
            "131\n",
            "Train on 1095 samples, validate on 365 samples\n",
            "Epoch 1/11\n",
            "1095/1095 [==============================] - 0s 74us/step - loss: 0.1665 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "Epoch 2/11\n",
            "1095/1095 [==============================] - 0s 75us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 3/11\n",
            "1095/1095 [==============================] - 0s 76us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 4/11\n",
            "1095/1095 [==============================] - 0s 72us/step - loss: 0.1668 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 5/11\n",
            "1095/1095 [==============================] - 0s 73us/step - loss: 0.1664 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 6/11\n",
            "1095/1095 [==============================] - 0s 72us/step - loss: 0.1671 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 7/11\n",
            "1095/1095 [==============================] - 0s 73us/step - loss: 0.1667 - acc: 0.0000e+00 - val_loss: 0.1388 - val_acc: 0.0000e+00\n",
            "Epoch 8/11\n",
            "1095/1095 [==============================] - 0s 72us/step - loss: 0.1667 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 9/11\n",
            "1095/1095 [==============================] - 0s 72us/step - loss: 0.1667 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 10/11\n",
            "1095/1095 [==============================] - 0s 72us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 11/11\n",
            "1095/1095 [==============================] - 0s 75us/step - loss: 0.1668 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "11\n",
            "141\n",
            "Train on 1095 samples, validate on 365 samples\n",
            "Epoch 1/12\n",
            "1095/1095 [==============================] - 8s 7ms/step - loss: 0.1830 - acc: 0.0000e+00 - val_loss: 0.1408 - val_acc: 0.0000e+00\n",
            "Epoch 2/12\n",
            "1095/1095 [==============================] - 8s 7ms/step - loss: 0.1745 - acc: 0.0000e+00 - val_loss: 0.1397 - val_acc: 0.0000e+00\n",
            "Epoch 3/12\n",
            "1095/1095 [==============================] - 8s 7ms/step - loss: 0.1735 - acc: 0.0000e+00 - val_loss: 0.1505 - val_acc: 0.0000e+00\n",
            "Epoch 4/12\n",
            "1095/1095 [==============================] - 8s 7ms/step - loss: 0.1791 - acc: 0.0000e+00 - val_loss: 0.1853 - val_acc: 0.0000e+00\n",
            "Epoch 5/12\n",
            "1095/1095 [==============================] - 8s 7ms/step - loss: 0.1776 - acc: 0.0000e+00 - val_loss: 0.1397 - val_acc: 0.0000e+00\n",
            "Epoch 6/12\n",
            "1095/1095 [==============================] - 8s 8ms/step - loss: 0.1742 - acc: 0.0000e+00 - val_loss: 0.1421 - val_acc: 0.0000e+00\n",
            "Epoch 7/12\n",
            "1095/1095 [==============================] - 8s 7ms/step - loss: 0.1747 - acc: 0.0000e+00 - val_loss: 0.1405 - val_acc: 0.0000e+00\n",
            "Epoch 8/12\n",
            "1095/1095 [==============================] - 8s 7ms/step - loss: 0.1739 - acc: 0.0000e+00 - val_loss: 0.1536 - val_acc: 0.0000e+00\n",
            "Epoch 9/12\n",
            "1095/1095 [==============================] - 8s 7ms/step - loss: 0.1763 - acc: 0.0000e+00 - val_loss: 0.1393 - val_acc: 0.0000e+00\n",
            "Epoch 10/12\n",
            "1095/1095 [==============================] - 8s 7ms/step - loss: 0.1752 - acc: 0.0000e+00 - val_loss: 0.1544 - val_acc: 0.0000e+00\n",
            "Epoch 11/12\n",
            "1095/1095 [==============================] - 8s 7ms/step - loss: 0.1755 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 12/12\n",
            "1095/1095 [==============================] - 8s 7ms/step - loss: 0.1756 - acc: 0.0000e+00 - val_loss: 0.1396 - val_acc: 0.0000e+00\n",
            "12\n",
            "1\n",
            "Train on 1095 samples, validate on 365 samples\n",
            "Epoch 1/12\n",
            "1095/1095 [==============================] - 1s 775us/step - loss: 0.1683 - acc: 0.0000e+00 - val_loss: 0.1386 - val_acc: 0.0000e+00\n",
            "Epoch 2/12\n",
            "1095/1095 [==============================] - 1s 760us/step - loss: 0.1672 - acc: 0.0000e+00 - val_loss: 0.1387 - val_acc: 0.0000e+00\n",
            "Epoch 3/12\n",
            "1095/1095 [==============================] - 1s 779us/step - loss: 0.1672 - acc: 0.0000e+00 - val_loss: 0.1413 - val_acc: 0.0000e+00\n",
            "Epoch 4/12\n",
            "1095/1095 [==============================] - 1s 761us/step - loss: 0.1675 - acc: 0.0000e+00 - val_loss: 0.1387 - val_acc: 0.0000e+00\n",
            "Epoch 5/12\n",
            "1095/1095 [==============================] - 1s 762us/step - loss: 0.1672 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 6/12\n",
            "1095/1095 [==============================] - 1s 780us/step - loss: 0.1669 - acc: 0.0000e+00 - val_loss: 0.1404 - val_acc: 0.0000e+00\n",
            "Epoch 7/12\n",
            "1095/1095 [==============================] - 1s 765us/step - loss: 0.1672 - acc: 0.0000e+00 - val_loss: 0.1395 - val_acc: 0.0000e+00\n",
            "Epoch 8/12\n",
            "1095/1095 [==============================] - 1s 769us/step - loss: 0.1676 - acc: 0.0000e+00 - val_loss: 0.1398 - val_acc: 0.0000e+00\n",
            "Epoch 9/12\n",
            "1095/1095 [==============================] - 1s 733us/step - loss: 0.1689 - acc: 0.0000e+00 - val_loss: 0.1405 - val_acc: 0.0000e+00\n",
            "Epoch 10/12\n",
            "1095/1095 [==============================] - 1s 768us/step - loss: 0.1673 - acc: 0.0000e+00 - val_loss: 0.1402 - val_acc: 0.0000e+00\n",
            "Epoch 11/12\n",
            "1095/1095 [==============================] - 1s 769us/step - loss: 0.1679 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "Epoch 12/12\n",
            "1095/1095 [==============================] - 1s 782us/step - loss: 0.1669 - acc: 0.0000e+00 - val_loss: 0.1420 - val_acc: 0.0000e+00\n",
            "12\n",
            "11\n",
            "Train on 1095 samples, validate on 365 samples\n",
            "Epoch 1/12\n",
            "1095/1095 [==============================] - 0s 402us/step - loss: 0.1683 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 2/12\n",
            "1095/1095 [==============================] - 0s 410us/step - loss: 0.1672 - acc: 0.0000e+00 - val_loss: 0.1400 - val_acc: 0.0000e+00\n",
            "Epoch 3/12\n",
            "1095/1095 [==============================] - 0s 417us/step - loss: 0.1663 - acc: 0.0000e+00 - val_loss: 0.1405 - val_acc: 0.0000e+00\n",
            "Epoch 4/12\n",
            "1095/1095 [==============================] - 0s 404us/step - loss: 0.1696 - acc: 0.0000e+00 - val_loss: 0.1393 - val_acc: 0.0000e+00\n",
            "Epoch 5/12\n",
            "1095/1095 [==============================] - 0s 402us/step - loss: 0.1670 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 6/12\n",
            "1095/1095 [==============================] - 0s 419us/step - loss: 0.1674 - acc: 0.0000e+00 - val_loss: 0.1434 - val_acc: 0.0000e+00\n",
            "Epoch 7/12\n",
            "1095/1095 [==============================] - 0s 410us/step - loss: 0.1673 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "Epoch 8/12\n",
            "1095/1095 [==============================] - 0s 408us/step - loss: 0.1671 - acc: 0.0000e+00 - val_loss: 0.1390 - val_acc: 0.0000e+00\n",
            "Epoch 9/12\n",
            "1095/1095 [==============================] - 0s 414us/step - loss: 0.1672 - acc: 0.0000e+00 - val_loss: 0.1389 - val_acc: 0.0000e+00\n",
            "Epoch 10/12\n",
            "1095/1095 [==============================] - 0s 394us/step - loss: 0.1669 - acc: 0.0000e+00 - val_loss: 0.1394 - val_acc: 0.0000e+00\n",
            "Epoch 11/12\n",
            "1095/1095 [==============================] - 0s 414us/step - loss: 0.1678 - acc: 0.0000e+00 - val_loss: 0.1388 - val_acc: 0.0000e+00\n",
            "Epoch 12/12\n",
            "1095/1095 [==============================] - 0s 408us/step - loss: 0.1672 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "12\n",
            "21\n",
            "Train on 1095 samples, validate on 365 samples\n",
            "Epoch 1/12\n",
            "1095/1095 [==============================] - 0s 288us/step - loss: 0.1671 - acc: 0.0000e+00 - val_loss: 0.1390 - val_acc: 0.0000e+00\n",
            "Epoch 2/12\n",
            "1095/1095 [==============================] - 0s 279us/step - loss: 0.1669 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 3/12\n",
            "1095/1095 [==============================] - 0s 275us/step - loss: 0.1668 - acc: 0.0000e+00 - val_loss: 0.1393 - val_acc: 0.0000e+00\n",
            "Epoch 4/12\n",
            "1095/1095 [==============================] - 0s 288us/step - loss: 0.1671 - acc: 0.0000e+00 - val_loss: 0.1387 - val_acc: 0.0000e+00\n",
            "Epoch 5/12\n",
            "1095/1095 [==============================] - 0s 299us/step - loss: 0.1676 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 6/12\n",
            "1095/1095 [==============================] - 0s 277us/step - loss: 0.1673 - acc: 0.0000e+00 - val_loss: 0.1393 - val_acc: 0.0000e+00\n",
            "Epoch 7/12\n",
            "1095/1095 [==============================] - 0s 283us/step - loss: 0.1685 - acc: 0.0000e+00 - val_loss: 0.1395 - val_acc: 0.0000e+00\n",
            "Epoch 8/12\n",
            "1095/1095 [==============================] - 0s 294us/step - loss: 0.1680 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 9/12\n",
            "1095/1095 [==============================] - 0s 286us/step - loss: 0.1669 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 10/12\n",
            "1095/1095 [==============================] - 0s 285us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1408 - val_acc: 0.0000e+00\n",
            "Epoch 11/12\n",
            "1095/1095 [==============================] - 0s 280us/step - loss: 0.1674 - acc: 0.0000e+00 - val_loss: 0.1386 - val_acc: 0.0000e+00\n",
            "Epoch 12/12\n",
            "1095/1095 [==============================] - 0s 279us/step - loss: 0.1668 - acc: 0.0000e+00 - val_loss: 0.1402 - val_acc: 0.0000e+00\n",
            "12\n",
            "31\n",
            "Train on 1095 samples, validate on 365 samples\n",
            "Epoch 1/12\n",
            "1095/1095 [==============================] - 0s 215us/step - loss: 0.1686 - acc: 0.0000e+00 - val_loss: 0.1394 - val_acc: 0.0000e+00\n",
            "Epoch 2/12\n",
            "1095/1095 [==============================] - 0s 225us/step - loss: 0.1682 - acc: 0.0000e+00 - val_loss: 0.1390 - val_acc: 0.0000e+00\n",
            "Epoch 3/12\n",
            "1095/1095 [==============================] - 0s 230us/step - loss: 0.1668 - acc: 0.0000e+00 - val_loss: 0.1390 - val_acc: 0.0000e+00\n",
            "Epoch 4/12\n",
            "1095/1095 [==============================] - 0s 210us/step - loss: 0.1671 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "Epoch 5/12\n",
            "1095/1095 [==============================] - 0s 212us/step - loss: 0.1670 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 6/12\n",
            "1095/1095 [==============================] - 0s 215us/step - loss: 0.1677 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 7/12\n",
            "1095/1095 [==============================] - 0s 217us/step - loss: 0.1693 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 8/12\n",
            "1095/1095 [==============================] - 0s 208us/step - loss: 0.1668 - acc: 0.0000e+00 - val_loss: 0.1388 - val_acc: 0.0000e+00\n",
            "Epoch 9/12\n",
            "1095/1095 [==============================] - 0s 215us/step - loss: 0.1673 - acc: 0.0000e+00 - val_loss: 0.1388 - val_acc: 0.0000e+00\n",
            "Epoch 10/12\n",
            "1095/1095 [==============================] - 0s 216us/step - loss: 0.1678 - acc: 0.0000e+00 - val_loss: 0.1400 - val_acc: 0.0000e+00\n",
            "Epoch 11/12\n",
            "1095/1095 [==============================] - 0s 220us/step - loss: 0.1674 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "Epoch 12/12\n",
            "1095/1095 [==============================] - 0s 217us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "12\n",
            "41\n",
            "Train on 1095 samples, validate on 365 samples\n",
            "Epoch 1/12\n",
            "1095/1095 [==============================] - 0s 179us/step - loss: 0.1667 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "Epoch 2/12\n",
            "1095/1095 [==============================] - 0s 190us/step - loss: 0.1671 - acc: 0.0000e+00 - val_loss: 0.1393 - val_acc: 0.0000e+00\n",
            "Epoch 3/12\n",
            "1095/1095 [==============================] - 0s 174us/step - loss: 0.1662 - acc: 0.0000e+00 - val_loss: 0.1393 - val_acc: 0.0000e+00\n",
            "Epoch 4/12\n",
            "1095/1095 [==============================] - 0s 182us/step - loss: 0.1677 - acc: 0.0000e+00 - val_loss: 0.1388 - val_acc: 0.0000e+00\n",
            "Epoch 5/12\n",
            "1095/1095 [==============================] - 0s 175us/step - loss: 0.1669 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 6/12\n",
            "1095/1095 [==============================] - 0s 168us/step - loss: 0.1668 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 7/12\n",
            "1095/1095 [==============================] - 0s 182us/step - loss: 0.1669 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "Epoch 8/12\n",
            "1095/1095 [==============================] - 0s 176us/step - loss: 0.1679 - acc: 0.0000e+00 - val_loss: 0.1396 - val_acc: 0.0000e+00\n",
            "Epoch 9/12\n",
            "1095/1095 [==============================] - 0s 172us/step - loss: 0.1673 - acc: 0.0000e+00 - val_loss: 0.1386 - val_acc: 0.0000e+00\n",
            "Epoch 10/12\n",
            "1095/1095 [==============================] - 0s 177us/step - loss: 0.1667 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 11/12\n",
            "1095/1095 [==============================] - 0s 174us/step - loss: 0.1672 - acc: 0.0000e+00 - val_loss: 0.1389 - val_acc: 0.0000e+00\n",
            "Epoch 12/12\n",
            "1095/1095 [==============================] - 0s 169us/step - loss: 0.1673 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "12\n",
            "51\n",
            "Train on 1095 samples, validate on 365 samples\n",
            "Epoch 1/12\n",
            "1095/1095 [==============================] - 0s 143us/step - loss: 0.1668 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 2/12\n",
            "1095/1095 [==============================] - 0s 141us/step - loss: 0.1668 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "Epoch 3/12\n",
            "1095/1095 [==============================] - 0s 151us/step - loss: 0.1674 - acc: 0.0000e+00 - val_loss: 0.1387 - val_acc: 0.0000e+00\n",
            "Epoch 4/12\n",
            "1095/1095 [==============================] - 0s 142us/step - loss: 0.1679 - acc: 0.0000e+00 - val_loss: 0.1387 - val_acc: 0.0000e+00\n",
            "Epoch 5/12\n",
            "1095/1095 [==============================] - 0s 143us/step - loss: 0.1672 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 6/12\n",
            "1095/1095 [==============================] - 0s 145us/step - loss: 0.1667 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 7/12\n",
            "1095/1095 [==============================] - 0s 147us/step - loss: 0.1669 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 8/12\n",
            "1095/1095 [==============================] - 0s 145us/step - loss: 0.1672 - acc: 0.0000e+00 - val_loss: 0.1386 - val_acc: 0.0000e+00\n",
            "Epoch 9/12\n",
            "1095/1095 [==============================] - 0s 142us/step - loss: 0.1668 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 10/12\n",
            "1095/1095 [==============================] - 0s 148us/step - loss: 0.1667 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 11/12\n",
            "1095/1095 [==============================] - 0s 142us/step - loss: 0.1677 - acc: 0.0000e+00 - val_loss: 0.1387 - val_acc: 0.0000e+00\n",
            "Epoch 12/12\n",
            "1095/1095 [==============================] - 0s 141us/step - loss: 0.1669 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "12\n",
            "61\n",
            "Train on 1095 samples, validate on 365 samples\n",
            "Epoch 1/12\n",
            "1095/1095 [==============================] - 0s 136us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 2/12\n",
            "1095/1095 [==============================] - 0s 132us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 3/12\n",
            "1095/1095 [==============================] - 0s 136us/step - loss: 0.1672 - acc: 0.0000e+00 - val_loss: 0.1386 - val_acc: 0.0000e+00\n",
            "Epoch 4/12\n",
            "1095/1095 [==============================] - 0s 141us/step - loss: 0.1667 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 5/12\n",
            "1095/1095 [==============================] - 0s 136us/step - loss: 0.1671 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 6/12\n",
            "1095/1095 [==============================] - 0s 138us/step - loss: 0.1669 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 7/12\n",
            "1095/1095 [==============================] - 0s 144us/step - loss: 0.1667 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 8/12\n",
            "1095/1095 [==============================] - 0s 130us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 9/12\n",
            "1095/1095 [==============================] - 0s 135us/step - loss: 0.1671 - acc: 0.0000e+00 - val_loss: 0.1387 - val_acc: 0.0000e+00\n",
            "Epoch 10/12\n",
            "1095/1095 [==============================] - 0s 133us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "Epoch 11/12\n",
            "1095/1095 [==============================] - 0s 147us/step - loss: 0.1670 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 12/12\n",
            "1095/1095 [==============================] - 0s 135us/step - loss: 0.1668 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "12\n",
            "71\n",
            "Train on 1095 samples, validate on 365 samples\n",
            "Epoch 1/12\n",
            "1095/1095 [==============================] - 0s 111us/step - loss: 0.1680 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 2/12\n",
            "1095/1095 [==============================] - 0s 115us/step - loss: 0.1681 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 3/12\n",
            "1095/1095 [==============================] - 0s 115us/step - loss: 0.1668 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 4/12\n",
            "1095/1095 [==============================] - 0s 116us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 5/12\n",
            "1095/1095 [==============================] - 0s 115us/step - loss: 0.1672 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "Epoch 6/12\n",
            "1095/1095 [==============================] - 0s 115us/step - loss: 0.1670 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 7/12\n",
            "1095/1095 [==============================] - 0s 131us/step - loss: 0.1665 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 8/12\n",
            "1095/1095 [==============================] - 0s 123us/step - loss: 0.1667 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 9/12\n",
            "1095/1095 [==============================] - 0s 125us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 10/12\n",
            "1095/1095 [==============================] - 0s 117us/step - loss: 0.1668 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "Epoch 11/12\n",
            "1095/1095 [==============================] - 0s 114us/step - loss: 0.1669 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "Epoch 12/12\n",
            "1095/1095 [==============================] - 0s 116us/step - loss: 0.1667 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "12\n",
            "81\n",
            "Train on 1095 samples, validate on 365 samples\n",
            "Epoch 1/12\n",
            "1095/1095 [==============================] - 0s 108us/step - loss: 0.1668 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 2/12\n",
            "1095/1095 [==============================] - 0s 112us/step - loss: 0.1667 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 3/12\n",
            "1095/1095 [==============================] - 0s 112us/step - loss: 0.1671 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "Epoch 4/12\n",
            "1095/1095 [==============================] - 0s 108us/step - loss: 0.1672 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "Epoch 5/12\n",
            "1095/1095 [==============================] - 0s 107us/step - loss: 0.1673 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 6/12\n",
            "1095/1095 [==============================] - 0s 110us/step - loss: 0.1712 - acc: 0.0000e+00 - val_loss: 0.1388 - val_acc: 0.0000e+00\n",
            "Epoch 7/12\n",
            "1095/1095 [==============================] - 0s 107us/step - loss: 0.1669 - acc: 0.0000e+00 - val_loss: 0.1389 - val_acc: 0.0000e+00\n",
            "Epoch 8/12\n",
            "1095/1095 [==============================] - 0s 111us/step - loss: 0.1673 - acc: 0.0000e+00 - val_loss: 0.1388 - val_acc: 0.0000e+00\n",
            "Epoch 9/12\n",
            "1095/1095 [==============================] - 0s 106us/step - loss: 0.1668 - acc: 0.0000e+00 - val_loss: 0.1387 - val_acc: 0.0000e+00\n",
            "Epoch 10/12\n",
            "1095/1095 [==============================] - 0s 105us/step - loss: 0.1669 - acc: 0.0000e+00 - val_loss: 0.1391 - val_acc: 0.0000e+00\n",
            "Epoch 11/12\n",
            "1095/1095 [==============================] - 0s 116us/step - loss: 0.1721 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "Epoch 12/12\n",
            "1095/1095 [==============================] - 0s 107us/step - loss: 0.1684 - acc: 0.0000e+00 - val_loss: 0.1388 - val_acc: 0.0000e+00\n",
            "12\n",
            "91\n",
            "Train on 1095 samples, validate on 365 samples\n",
            "Epoch 1/12\n",
            "1095/1095 [==============================] - 0s 93us/step - loss: 0.1674 - acc: 0.0000e+00 - val_loss: 0.1391 - val_acc: 0.0000e+00\n",
            "Epoch 2/12\n",
            "1095/1095 [==============================] - 0s 94us/step - loss: 0.1667 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 3/12\n",
            "1095/1095 [==============================] - 0s 107us/step - loss: 0.1667 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "Epoch 4/12\n",
            "1095/1095 [==============================] - 0s 99us/step - loss: 0.1668 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 5/12\n",
            "1095/1095 [==============================] - 0s 94us/step - loss: 0.1668 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 6/12\n",
            "1095/1095 [==============================] - 0s 101us/step - loss: 0.1667 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "Epoch 7/12\n",
            "1095/1095 [==============================] - 0s 91us/step - loss: 0.1665 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 8/12\n",
            "1095/1095 [==============================] - 0s 99us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 9/12\n",
            "1095/1095 [==============================] - 0s 96us/step - loss: 0.1667 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 10/12\n",
            "1095/1095 [==============================] - 0s 94us/step - loss: 0.1667 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 11/12\n",
            "1095/1095 [==============================] - 0s 97us/step - loss: 0.1668 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 12/12\n",
            "1095/1095 [==============================] - 0s 95us/step - loss: 0.1665 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "12\n",
            "101\n",
            "Train on 1095 samples, validate on 365 samples\n",
            "Epoch 1/12\n",
            "1095/1095 [==============================] - 0s 86us/step - loss: 0.1667 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 2/12\n",
            "1095/1095 [==============================] - 0s 84us/step - loss: 0.1668 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 3/12\n",
            "1095/1095 [==============================] - 0s 87us/step - loss: 0.1665 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "Epoch 4/12\n",
            "1095/1095 [==============================] - 0s 91us/step - loss: 0.1677 - acc: 0.0000e+00 - val_loss: 0.1387 - val_acc: 0.0000e+00\n",
            "Epoch 5/12\n",
            "1095/1095 [==============================] - 0s 87us/step - loss: 0.1671 - acc: 0.0000e+00 - val_loss: 0.1386 - val_acc: 0.0000e+00\n",
            "Epoch 6/12\n",
            "1095/1095 [==============================] - 0s 94us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1386 - val_acc: 0.0000e+00\n",
            "Epoch 7/12\n",
            "1095/1095 [==============================] - 0s 91us/step - loss: 0.1670 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "Epoch 8/12\n",
            "1095/1095 [==============================] - 0s 90us/step - loss: 0.1667 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "Epoch 9/12\n",
            "1095/1095 [==============================] - 0s 88us/step - loss: 0.1667 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 10/12\n",
            "1095/1095 [==============================] - 0s 91us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 11/12\n",
            "1095/1095 [==============================] - 0s 93us/step - loss: 0.1665 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 12/12\n",
            "1095/1095 [==============================] - 0s 90us/step - loss: 0.1667 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "12\n",
            "111\n",
            "Train on 1095 samples, validate on 365 samples\n",
            "Epoch 1/12\n",
            "1095/1095 [==============================] - 0s 83us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1387 - val_acc: 0.0000e+00\n",
            "Epoch 2/12\n",
            "1095/1095 [==============================] - 0s 87us/step - loss: 0.1683 - acc: 0.0000e+00 - val_loss: 0.1390 - val_acc: 0.0000e+00\n",
            "Epoch 3/12\n",
            "1095/1095 [==============================] - 0s 88us/step - loss: 0.1676 - acc: 0.0000e+00 - val_loss: 0.1393 - val_acc: 0.0000e+00\n",
            "Epoch 4/12\n",
            "1095/1095 [==============================] - 0s 97us/step - loss: 0.1674 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 5/12\n",
            "1095/1095 [==============================] - 0s 88us/step - loss: 0.1670 - acc: 0.0000e+00 - val_loss: 0.1386 - val_acc: 0.0000e+00\n",
            "Epoch 6/12\n",
            "1095/1095 [==============================] - 0s 90us/step - loss: 0.1667 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 7/12\n",
            "1095/1095 [==============================] - 0s 89us/step - loss: 0.1668 - acc: 0.0000e+00 - val_loss: 0.1395 - val_acc: 0.0000e+00\n",
            "Epoch 8/12\n",
            "1095/1095 [==============================] - 0s 89us/step - loss: 0.1672 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 9/12\n",
            "1095/1095 [==============================] - 0s 89us/step - loss: 0.1692 - acc: 0.0000e+00 - val_loss: 0.1393 - val_acc: 0.0000e+00\n",
            "Epoch 10/12\n",
            "1095/1095 [==============================] - 0s 87us/step - loss: 0.1671 - acc: 0.0000e+00 - val_loss: 0.1396 - val_acc: 0.0000e+00\n",
            "Epoch 11/12\n",
            "1095/1095 [==============================] - 0s 89us/step - loss: 0.1670 - acc: 0.0000e+00 - val_loss: 0.1386 - val_acc: 0.0000e+00\n",
            "Epoch 12/12\n",
            "1095/1095 [==============================] - 0s 90us/step - loss: 0.1704 - acc: 0.0000e+00 - val_loss: 0.1390 - val_acc: 0.0000e+00\n",
            "12\n",
            "121\n",
            "Train on 1095 samples, validate on 365 samples\n",
            "Epoch 1/12\n",
            "1095/1095 [==============================] - 0s 77us/step - loss: 0.1689 - acc: 0.0000e+00 - val_loss: 0.1423 - val_acc: 0.0000e+00\n",
            "Epoch 2/12\n",
            "1095/1095 [==============================] - 0s 87us/step - loss: 0.1668 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "Epoch 3/12\n",
            "1095/1095 [==============================] - 0s 80us/step - loss: 0.1685 - acc: 0.0000e+00 - val_loss: 0.1393 - val_acc: 0.0000e+00\n",
            "Epoch 4/12\n",
            "1095/1095 [==============================] - 0s 83us/step - loss: 0.1675 - acc: 0.0000e+00 - val_loss: 0.1397 - val_acc: 0.0000e+00\n",
            "Epoch 5/12\n",
            "1095/1095 [==============================] - 0s 82us/step - loss: 0.1670 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 6/12\n",
            "1095/1095 [==============================] - 0s 84us/step - loss: 0.1667 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 7/12\n",
            "1095/1095 [==============================] - 0s 78us/step - loss: 0.1665 - acc: 0.0000e+00 - val_loss: 0.1391 - val_acc: 0.0000e+00\n",
            "Epoch 8/12\n",
            "1095/1095 [==============================] - 0s 76us/step - loss: 0.1671 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "Epoch 9/12\n",
            "1095/1095 [==============================] - 0s 79us/step - loss: 0.1667 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 10/12\n",
            "1095/1095 [==============================] - 0s 80us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 11/12\n",
            "1095/1095 [==============================] - 0s 80us/step - loss: 0.1667 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 12/12\n",
            "1095/1095 [==============================] - 0s 80us/step - loss: 0.1665 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "12\n",
            "131\n",
            "Train on 1095 samples, validate on 365 samples\n",
            "Epoch 1/12\n",
            "1095/1095 [==============================] - 0s 74us/step - loss: 0.1665 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 2/12\n",
            "1095/1095 [==============================] - 0s 78us/step - loss: 0.1675 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 3/12\n",
            "1095/1095 [==============================] - 0s 71us/step - loss: 0.1669 - acc: 0.0000e+00 - val_loss: 0.1387 - val_acc: 0.0000e+00\n",
            "Epoch 4/12\n",
            "1095/1095 [==============================] - 0s 74us/step - loss: 0.1665 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 5/12\n",
            "1095/1095 [==============================] - 0s 72us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 6/12\n",
            "1095/1095 [==============================] - 0s 70us/step - loss: 0.1668 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 7/12\n",
            "1095/1095 [==============================] - 0s 72us/step - loss: 0.1676 - acc: 0.0000e+00 - val_loss: 0.1390 - val_acc: 0.0000e+00\n",
            "Epoch 8/12\n",
            "1095/1095 [==============================] - 0s 74us/step - loss: 0.1667 - acc: 0.0000e+00 - val_loss: 0.1386 - val_acc: 0.0000e+00\n",
            "Epoch 9/12\n",
            "1095/1095 [==============================] - 0s 74us/step - loss: 0.1669 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 10/12\n",
            "1095/1095 [==============================] - 0s 71us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 11/12\n",
            "1095/1095 [==============================] - 0s 74us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "Epoch 12/12\n",
            "1095/1095 [==============================] - 0s 74us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "12\n",
            "141\n",
            "Train on 1095 samples, validate on 365 samples\n",
            "Epoch 1/13\n",
            "1095/1095 [==============================] - 8s 7ms/step - loss: 0.1807 - acc: 0.0000e+00 - val_loss: 0.1403 - val_acc: 0.0000e+00\n",
            "Epoch 2/13\n",
            "1095/1095 [==============================] - 8s 7ms/step - loss: 0.1755 - acc: 0.0000e+00 - val_loss: 0.1409 - val_acc: 0.0000e+00\n",
            "Epoch 3/13\n",
            "1095/1095 [==============================] - 8s 7ms/step - loss: 0.1758 - acc: 0.0000e+00 - val_loss: 0.1395 - val_acc: 0.0000e+00\n",
            "Epoch 4/13\n",
            "1095/1095 [==============================] - 8s 7ms/step - loss: 0.1753 - acc: 0.0000e+00 - val_loss: 0.1386 - val_acc: 0.0000e+00\n",
            "Epoch 5/13\n",
            "1095/1095 [==============================] - 8s 7ms/step - loss: 0.1755 - acc: 0.0000e+00 - val_loss: 0.1451 - val_acc: 0.0000e+00\n",
            "Epoch 6/13\n",
            "1095/1095 [==============================] - 8s 7ms/step - loss: 0.1757 - acc: 0.0000e+00 - val_loss: 0.1402 - val_acc: 0.0000e+00\n",
            "Epoch 7/13\n",
            "1095/1095 [==============================] - 8s 7ms/step - loss: 0.1759 - acc: 0.0000e+00 - val_loss: 0.1501 - val_acc: 0.0000e+00\n",
            "Epoch 8/13\n",
            "1095/1095 [==============================] - 8s 7ms/step - loss: 0.1782 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 9/13\n",
            "1095/1095 [==============================] - 8s 7ms/step - loss: 0.1771 - acc: 0.0000e+00 - val_loss: 0.1742 - val_acc: 0.0000e+00\n",
            "Epoch 10/13\n",
            "1095/1095 [==============================] - 8s 7ms/step - loss: 0.1763 - acc: 0.0000e+00 - val_loss: 0.1422 - val_acc: 0.0000e+00\n",
            "Epoch 11/13\n",
            "1095/1095 [==============================] - 8s 7ms/step - loss: 0.1744 - acc: 0.0000e+00 - val_loss: 0.1647 - val_acc: 0.0000e+00\n",
            "Epoch 12/13\n",
            "1095/1095 [==============================] - 8s 7ms/step - loss: 0.1750 - acc: 0.0000e+00 - val_loss: 0.1415 - val_acc: 0.0000e+00\n",
            "Epoch 13/13\n",
            "1095/1095 [==============================] - 8s 7ms/step - loss: 0.1717 - acc: 0.0000e+00 - val_loss: 0.1468 - val_acc: 0.0000e+00\n",
            "13\n",
            "1\n",
            "Train on 1095 samples, validate on 365 samples\n",
            "Epoch 1/13\n",
            "1095/1095 [==============================] - 1s 786us/step - loss: 0.1680 - acc: 0.0000e+00 - val_loss: 0.1397 - val_acc: 0.0000e+00\n",
            "Epoch 2/13\n",
            "1095/1095 [==============================] - 1s 773us/step - loss: 0.1669 - acc: 0.0000e+00 - val_loss: 0.1395 - val_acc: 0.0000e+00\n",
            "Epoch 3/13\n",
            "1095/1095 [==============================] - 1s 761us/step - loss: 0.1673 - acc: 0.0000e+00 - val_loss: 0.1389 - val_acc: 0.0000e+00\n",
            "Epoch 4/13\n",
            "1095/1095 [==============================] - 1s 788us/step - loss: 0.1672 - acc: 0.0000e+00 - val_loss: 0.1391 - val_acc: 0.0000e+00\n",
            "Epoch 5/13\n",
            "1095/1095 [==============================] - 1s 762us/step - loss: 0.1681 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "Epoch 6/13\n",
            "1095/1095 [==============================] - 1s 764us/step - loss: 0.1673 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 7/13\n",
            "1095/1095 [==============================] - 1s 795us/step - loss: 0.1677 - acc: 0.0000e+00 - val_loss: 0.1403 - val_acc: 0.0000e+00\n",
            "Epoch 8/13\n",
            "1095/1095 [==============================] - 1s 765us/step - loss: 0.1681 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "Epoch 9/13\n",
            "1095/1095 [==============================] - 1s 766us/step - loss: 0.1690 - acc: 0.0000e+00 - val_loss: 0.1396 - val_acc: 0.0000e+00\n",
            "Epoch 10/13\n",
            "1095/1095 [==============================] - 1s 779us/step - loss: 0.1677 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 11/13\n",
            "1095/1095 [==============================] - 1s 759us/step - loss: 0.1679 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 12/13\n",
            "1095/1095 [==============================] - 1s 770us/step - loss: 0.1679 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 13/13\n",
            "1095/1095 [==============================] - 1s 756us/step - loss: 0.1679 - acc: 0.0000e+00 - val_loss: 0.1404 - val_acc: 0.0000e+00\n",
            "13\n",
            "11\n",
            "Train on 1095 samples, validate on 365 samples\n",
            "Epoch 1/13\n",
            "1095/1095 [==============================] - 0s 423us/step - loss: 0.1672 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 2/13\n",
            "1095/1095 [==============================] - 0s 406us/step - loss: 0.1669 - acc: 0.0000e+00 - val_loss: 0.1394 - val_acc: 0.0000e+00\n",
            "Epoch 3/13\n",
            "1095/1095 [==============================] - 0s 422us/step - loss: 0.1678 - acc: 0.0000e+00 - val_loss: 0.1387 - val_acc: 0.0000e+00\n",
            "Epoch 4/13\n",
            "1095/1095 [==============================] - 0s 411us/step - loss: 0.1671 - acc: 0.0000e+00 - val_loss: 0.1388 - val_acc: 0.0000e+00\n",
            "Epoch 5/13\n",
            "1095/1095 [==============================] - 0s 416us/step - loss: 0.1675 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 6/13\n",
            "1095/1095 [==============================] - 0s 411us/step - loss: 0.1674 - acc: 0.0000e+00 - val_loss: 0.1387 - val_acc: 0.0000e+00\n",
            "Epoch 7/13\n",
            "1095/1095 [==============================] - 0s 427us/step - loss: 0.1672 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "Epoch 8/13\n",
            "1095/1095 [==============================] - 0s 438us/step - loss: 0.1663 - acc: 0.0000e+00 - val_loss: 0.1411 - val_acc: 0.0000e+00\n",
            "Epoch 9/13\n",
            "1095/1095 [==============================] - 0s 429us/step - loss: 0.1690 - acc: 0.0000e+00 - val_loss: 0.1412 - val_acc: 0.0000e+00\n",
            "Epoch 10/13\n",
            "1095/1095 [==============================] - 0s 454us/step - loss: 0.1674 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 11/13\n",
            "1095/1095 [==============================] - 0s 430us/step - loss: 0.1672 - acc: 0.0000e+00 - val_loss: 0.1386 - val_acc: 0.0000e+00\n",
            "Epoch 12/13\n",
            "1095/1095 [==============================] - 1s 458us/step - loss: 0.1671 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 13/13\n",
            "1095/1095 [==============================] - 1s 463us/step - loss: 0.1673 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "13\n",
            "21\n",
            "Train on 1095 samples, validate on 365 samples\n",
            "Epoch 1/13\n",
            "1095/1095 [==============================] - 0s 313us/step - loss: 0.1669 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 2/13\n",
            "1095/1095 [==============================] - 0s 296us/step - loss: 0.1669 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 3/13\n",
            "1095/1095 [==============================] - 0s 300us/step - loss: 0.1668 - acc: 0.0000e+00 - val_loss: 0.1388 - val_acc: 0.0000e+00\n",
            "Epoch 4/13\n",
            "1095/1095 [==============================] - 0s 297us/step - loss: 0.1679 - acc: 0.0000e+00 - val_loss: 0.1387 - val_acc: 0.0000e+00\n",
            "Epoch 5/13\n",
            "1095/1095 [==============================] - 0s 289us/step - loss: 0.1679 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "Epoch 6/13\n",
            "1095/1095 [==============================] - 0s 278us/step - loss: 0.1670 - acc: 0.0000e+00 - val_loss: 0.1401 - val_acc: 0.0000e+00\n",
            "Epoch 7/13\n",
            "1095/1095 [==============================] - 0s 281us/step - loss: 0.1673 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 8/13\n",
            "1095/1095 [==============================] - 0s 281us/step - loss: 0.1675 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 9/13\n",
            "1095/1095 [==============================] - 0s 272us/step - loss: 0.1676 - acc: 0.0000e+00 - val_loss: 0.1390 - val_acc: 0.0000e+00\n",
            "Epoch 10/13\n",
            "1095/1095 [==============================] - 0s 293us/step - loss: 0.1672 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 11/13\n",
            "1095/1095 [==============================] - 0s 283us/step - loss: 0.1669 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "Epoch 12/13\n",
            "1095/1095 [==============================] - 0s 283us/step - loss: 0.1673 - acc: 0.0000e+00 - val_loss: 0.1388 - val_acc: 0.0000e+00\n",
            "Epoch 13/13\n",
            "1095/1095 [==============================] - 0s 297us/step - loss: 0.1679 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "13\n",
            "31\n",
            "Train on 1095 samples, validate on 365 samples\n",
            "Epoch 1/13\n",
            "1095/1095 [==============================] - 0s 216us/step - loss: 0.1670 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "Epoch 2/13\n",
            "1095/1095 [==============================] - 0s 218us/step - loss: 0.1669 - acc: 0.0000e+00 - val_loss: 0.1387 - val_acc: 0.0000e+00\n",
            "Epoch 3/13\n",
            "1095/1095 [==============================] - 0s 214us/step - loss: 0.1669 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 4/13\n",
            "1095/1095 [==============================] - 0s 216us/step - loss: 0.1673 - acc: 0.0000e+00 - val_loss: 0.1388 - val_acc: 0.0000e+00\n",
            "Epoch 5/13\n",
            "1095/1095 [==============================] - 0s 219us/step - loss: 0.1675 - acc: 0.0000e+00 - val_loss: 0.1393 - val_acc: 0.0000e+00\n",
            "Epoch 6/13\n",
            "1095/1095 [==============================] - 0s 212us/step - loss: 0.1681 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 7/13\n",
            "1095/1095 [==============================] - 0s 210us/step - loss: 0.1670 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 8/13\n",
            "1095/1095 [==============================] - 0s 211us/step - loss: 0.1667 - acc: 0.0000e+00 - val_loss: 0.1386 - val_acc: 0.0000e+00\n",
            "Epoch 9/13\n",
            "1095/1095 [==============================] - 0s 222us/step - loss: 0.1673 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "Epoch 10/13\n",
            "1095/1095 [==============================] - 0s 215us/step - loss: 0.1670 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "Epoch 11/13\n",
            "1095/1095 [==============================] - 0s 207us/step - loss: 0.1671 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "Epoch 12/13\n",
            "1095/1095 [==============================] - 0s 209us/step - loss: 0.1677 - acc: 0.0000e+00 - val_loss: 0.1390 - val_acc: 0.0000e+00\n",
            "Epoch 13/13\n",
            "1095/1095 [==============================] - 0s 217us/step - loss: 0.1684 - acc: 0.0000e+00 - val_loss: 0.1388 - val_acc: 0.0000e+00\n",
            "13\n",
            "41\n",
            "Train on 1095 samples, validate on 365 samples\n",
            "Epoch 1/13\n",
            "1095/1095 [==============================] - 0s 173us/step - loss: 0.1678 - acc: 0.0000e+00 - val_loss: 0.1392 - val_acc: 0.0000e+00\n",
            "Epoch 2/13\n",
            "1095/1095 [==============================] - 0s 178us/step - loss: 0.1687 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 3/13\n",
            "1095/1095 [==============================] - 0s 176us/step - loss: 0.1690 - acc: 0.0000e+00 - val_loss: 0.1417 - val_acc: 0.0000e+00\n",
            "Epoch 4/13\n",
            "1095/1095 [==============================] - 0s 173us/step - loss: 0.1676 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "Epoch 5/13\n",
            "1095/1095 [==============================] - 0s 191us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1386 - val_acc: 0.0000e+00\n",
            "Epoch 6/13\n",
            "1095/1095 [==============================] - 0s 180us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 7/13\n",
            "1095/1095 [==============================] - 0s 178us/step - loss: 0.1667 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "Epoch 8/13\n",
            "1095/1095 [==============================] - 0s 173us/step - loss: 0.1671 - acc: 0.0000e+00 - val_loss: 0.1392 - val_acc: 0.0000e+00\n",
            "Epoch 9/13\n",
            "1095/1095 [==============================] - 0s 172us/step - loss: 0.1670 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "Epoch 10/13\n",
            "1095/1095 [==============================] - 0s 173us/step - loss: 0.1669 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "Epoch 11/13\n",
            "1095/1095 [==============================] - 0s 186us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1391 - val_acc: 0.0000e+00\n",
            "Epoch 12/13\n",
            "1095/1095 [==============================] - 0s 176us/step - loss: 0.1665 - acc: 0.0000e+00 - val_loss: 0.1386 - val_acc: 0.0000e+00\n",
            "Epoch 13/13\n",
            "1095/1095 [==============================] - 0s 179us/step - loss: 0.1670 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "13\n",
            "51\n",
            "Train on 1095 samples, validate on 365 samples\n",
            "Epoch 1/13\n",
            "1095/1095 [==============================] - 0s 149us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 2/13\n",
            "1095/1095 [==============================] - 0s 144us/step - loss: 0.1671 - acc: 0.0000e+00 - val_loss: 0.1392 - val_acc: 0.0000e+00\n",
            "Epoch 3/13\n",
            "1095/1095 [==============================] - 0s 151us/step - loss: 0.1669 - acc: 0.0000e+00 - val_loss: 0.1386 - val_acc: 0.0000e+00\n",
            "Epoch 4/13\n",
            "1095/1095 [==============================] - 0s 143us/step - loss: 0.1677 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "Epoch 5/13\n",
            "1095/1095 [==============================] - 0s 141us/step - loss: 0.1670 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "Epoch 6/13\n",
            "1095/1095 [==============================] - 0s 142us/step - loss: 0.1669 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "Epoch 7/13\n",
            "1095/1095 [==============================] - 0s 145us/step - loss: 0.1673 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 8/13\n",
            "1095/1095 [==============================] - 0s 148us/step - loss: 0.1672 - acc: 0.0000e+00 - val_loss: 0.1386 - val_acc: 0.0000e+00\n",
            "Epoch 9/13\n",
            "1095/1095 [==============================] - 0s 144us/step - loss: 0.1669 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "Epoch 10/13\n",
            "1095/1095 [==============================] - 0s 154us/step - loss: 0.1668 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 11/13\n",
            "1095/1095 [==============================] - 0s 146us/step - loss: 0.1675 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 12/13\n",
            "1095/1095 [==============================] - 0s 141us/step - loss: 0.1675 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "Epoch 13/13\n",
            "1095/1095 [==============================] - 0s 146us/step - loss: 0.1670 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "13\n",
            "61\n",
            "Train on 1095 samples, validate on 365 samples\n",
            "Epoch 1/13\n",
            "1095/1095 [==============================] - 0s 126us/step - loss: 0.1667 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 2/13\n",
            "1095/1095 [==============================] - 0s 134us/step - loss: 0.1667 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 3/13\n",
            "1095/1095 [==============================] - 0s 139us/step - loss: 0.1667 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 4/13\n",
            "1095/1095 [==============================] - 0s 136us/step - loss: 0.1669 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 5/13\n",
            "1095/1095 [==============================] - 0s 134us/step - loss: 0.1672 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "Epoch 6/13\n",
            "1095/1095 [==============================] - 0s 132us/step - loss: 0.1671 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 7/13\n",
            "1095/1095 [==============================] - 0s 134us/step - loss: 0.1669 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 8/13\n",
            "1095/1095 [==============================] - 0s 131us/step - loss: 0.1669 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 9/13\n",
            "1095/1095 [==============================] - 0s 135us/step - loss: 0.1672 - acc: 0.0000e+00 - val_loss: 0.1386 - val_acc: 0.0000e+00\n",
            "Epoch 10/13\n",
            "1095/1095 [==============================] - 0s 137us/step - loss: 0.1673 - acc: 0.0000e+00 - val_loss: 0.1395 - val_acc: 0.0000e+00\n",
            "Epoch 11/13\n",
            "1095/1095 [==============================] - 0s 135us/step - loss: 0.1668 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 12/13\n",
            "1095/1095 [==============================] - 0s 131us/step - loss: 0.1668 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 13/13\n",
            "1095/1095 [==============================] - 0s 130us/step - loss: 0.1671 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "13\n",
            "71\n",
            "Train on 1095 samples, validate on 365 samples\n",
            "Epoch 1/13\n",
            "1095/1095 [==============================] - 0s 117us/step - loss: 0.1669 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 2/13\n",
            "1095/1095 [==============================] - 0s 114us/step - loss: 0.1665 - acc: 0.0000e+00 - val_loss: 0.1386 - val_acc: 0.0000e+00\n",
            "Epoch 3/13\n",
            "1095/1095 [==============================] - 0s 116us/step - loss: 0.1667 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 4/13\n",
            "1095/1095 [==============================] - 0s 122us/step - loss: 0.1671 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 5/13\n",
            "1095/1095 [==============================] - 0s 113us/step - loss: 0.1669 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "Epoch 6/13\n",
            "1095/1095 [==============================] - 0s 112us/step - loss: 0.1668 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "Epoch 7/13\n",
            "1095/1095 [==============================] - 0s 112us/step - loss: 0.1668 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 8/13\n",
            "1095/1095 [==============================] - 0s 116us/step - loss: 0.1669 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 9/13\n",
            "1095/1095 [==============================] - 0s 117us/step - loss: 0.1667 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 10/13\n",
            "1095/1095 [==============================] - 0s 114us/step - loss: 0.1667 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 11/13\n",
            "1095/1095 [==============================] - 0s 115us/step - loss: 0.1671 - acc: 0.0000e+00 - val_loss: 0.1387 - val_acc: 0.0000e+00\n",
            "Epoch 12/13\n",
            "1095/1095 [==============================] - 0s 121us/step - loss: 0.1677 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 13/13\n",
            "1095/1095 [==============================] - 0s 117us/step - loss: 0.1668 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "13\n",
            "81\n",
            "Train on 1095 samples, validate on 365 samples\n",
            "Epoch 1/13\n",
            "1095/1095 [==============================] - 0s 107us/step - loss: 0.1668 - acc: 0.0000e+00 - val_loss: 0.1386 - val_acc: 0.0000e+00\n",
            "Epoch 2/13\n",
            "1095/1095 [==============================] - 0s 113us/step - loss: 0.1668 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 3/13\n",
            "1095/1095 [==============================] - 0s 109us/step - loss: 0.1678 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "Epoch 4/13\n",
            "1095/1095 [==============================] - 0s 106us/step - loss: 0.1675 - acc: 0.0000e+00 - val_loss: 0.1386 - val_acc: 0.0000e+00\n",
            "Epoch 5/13\n",
            "1095/1095 [==============================] - 0s 106us/step - loss: 0.1664 - acc: 0.0000e+00 - val_loss: 0.1387 - val_acc: 0.0000e+00\n",
            "Epoch 6/13\n",
            "1095/1095 [==============================] - 0s 105us/step - loss: 0.1674 - acc: 0.0000e+00 - val_loss: 0.1387 - val_acc: 0.0000e+00\n",
            "Epoch 7/13\n",
            "1095/1095 [==============================] - 0s 104us/step - loss: 0.1698 - acc: 0.0000e+00 - val_loss: 0.1386 - val_acc: 0.0000e+00\n",
            "Epoch 8/13\n",
            "1095/1095 [==============================] - 0s 111us/step - loss: 0.1702 - acc: 0.0000e+00 - val_loss: 0.1390 - val_acc: 0.0000e+00\n",
            "Epoch 9/13\n",
            "1095/1095 [==============================] - 0s 105us/step - loss: 0.1671 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "Epoch 10/13\n",
            "1095/1095 [==============================] - 0s 114us/step - loss: 0.1673 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "Epoch 11/13\n",
            "1095/1095 [==============================] - 0s 107us/step - loss: 0.1677 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 12/13\n",
            "1095/1095 [==============================] - 0s 107us/step - loss: 0.1680 - acc: 0.0000e+00 - val_loss: 0.1386 - val_acc: 0.0000e+00\n",
            "Epoch 13/13\n",
            "1095/1095 [==============================] - 0s 116us/step - loss: 0.1672 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "13\n",
            "91\n",
            "Train on 1095 samples, validate on 365 samples\n",
            "Epoch 1/13\n",
            "1095/1095 [==============================] - 0s 94us/step - loss: 0.1673 - acc: 0.0000e+00 - val_loss: 0.1387 - val_acc: 0.0000e+00\n",
            "Epoch 2/13\n",
            "1095/1095 [==============================] - 0s 94us/step - loss: 0.1669 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 3/13\n",
            "1095/1095 [==============================] - 0s 96us/step - loss: 0.1665 - acc: 0.0000e+00 - val_loss: 0.1386 - val_acc: 0.0000e+00\n",
            "Epoch 4/13\n",
            "1095/1095 [==============================] - 0s 94us/step - loss: 0.1669 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 5/13\n",
            "1095/1095 [==============================] - 0s 93us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "Epoch 6/13\n",
            "1095/1095 [==============================] - 0s 96us/step - loss: 0.1665 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 7/13\n",
            "1095/1095 [==============================] - 0s 93us/step - loss: 0.1671 - acc: 0.0000e+00 - val_loss: 0.1386 - val_acc: 0.0000e+00\n",
            "Epoch 8/13\n",
            "1095/1095 [==============================] - 0s 92us/step - loss: 0.1667 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 9/13\n",
            "1095/1095 [==============================] - 0s 91us/step - loss: 0.1667 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "Epoch 10/13\n",
            "1095/1095 [==============================] - 0s 91us/step - loss: 0.1670 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "Epoch 11/13\n",
            "1095/1095 [==============================] - 0s 96us/step - loss: 0.1667 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 12/13\n",
            "1095/1095 [==============================] - 0s 93us/step - loss: 0.1667 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 13/13\n",
            "1095/1095 [==============================] - 0s 96us/step - loss: 0.1668 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "13\n",
            "101\n",
            "Train on 1095 samples, validate on 365 samples\n",
            "Epoch 1/13\n",
            "1095/1095 [==============================] - 0s 88us/step - loss: 0.1668 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 2/13\n",
            "1095/1095 [==============================] - 0s 87us/step - loss: 0.1668 - acc: 0.0000e+00 - val_loss: 0.1387 - val_acc: 0.0000e+00\n",
            "Epoch 3/13\n",
            "1095/1095 [==============================] - 0s 90us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 4/13\n",
            "1095/1095 [==============================] - 0s 87us/step - loss: 0.1667 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 5/13\n",
            "1095/1095 [==============================] - 0s 86us/step - loss: 0.1677 - acc: 0.0000e+00 - val_loss: 0.1390 - val_acc: 0.0000e+00\n",
            "Epoch 6/13\n",
            "1095/1095 [==============================] - 0s 86us/step - loss: 0.1677 - acc: 0.0000e+00 - val_loss: 0.1388 - val_acc: 0.0000e+00\n",
            "Epoch 7/13\n",
            "1095/1095 [==============================] - 0s 87us/step - loss: 0.1663 - acc: 0.0000e+00 - val_loss: 0.1389 - val_acc: 0.0000e+00\n",
            "Epoch 8/13\n",
            "1095/1095 [==============================] - 0s 88us/step - loss: 0.1667 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "Epoch 9/13\n",
            "1095/1095 [==============================] - 0s 91us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 10/13\n",
            "1095/1095 [==============================] - 0s 93us/step - loss: 0.1667 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 11/13\n",
            "1095/1095 [==============================] - 0s 87us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1386 - val_acc: 0.0000e+00\n",
            "Epoch 12/13\n",
            "1095/1095 [==============================] - 0s 90us/step - loss: 0.1669 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 13/13\n",
            "1095/1095 [==============================] - 0s 87us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "13\n",
            "111\n",
            "Train on 1095 samples, validate on 365 samples\n",
            "Epoch 1/13\n",
            "1095/1095 [==============================] - 0s 86us/step - loss: 0.1667 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 2/13\n",
            "1095/1095 [==============================] - 0s 86us/step - loss: 0.1668 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 3/13\n",
            "1095/1095 [==============================] - 0s 94us/step - loss: 0.1665 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 4/13\n",
            "1095/1095 [==============================] - 0s 88us/step - loss: 0.1667 - acc: 0.0000e+00 - val_loss: 0.1389 - val_acc: 0.0000e+00\n",
            "Epoch 5/13\n",
            "1095/1095 [==============================] - 0s 90us/step - loss: 0.1674 - acc: 0.0000e+00 - val_loss: 0.1393 - val_acc: 0.0000e+00\n",
            "Epoch 6/13\n",
            "1095/1095 [==============================] - 0s 88us/step - loss: 0.1675 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 7/13\n",
            "1095/1095 [==============================] - 0s 93us/step - loss: 0.1668 - acc: 0.0000e+00 - val_loss: 0.1386 - val_acc: 0.0000e+00\n",
            "Epoch 8/13\n",
            "1095/1095 [==============================] - 0s 93us/step - loss: 0.1671 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 9/13\n",
            "1095/1095 [==============================] - 0s 87us/step - loss: 0.1674 - acc: 0.0000e+00 - val_loss: 0.1388 - val_acc: 0.0000e+00\n",
            "Epoch 10/13\n",
            "1095/1095 [==============================] - 0s 89us/step - loss: 0.1670 - acc: 0.0000e+00 - val_loss: 0.1396 - val_acc: 0.0000e+00\n",
            "Epoch 11/13\n",
            "1095/1095 [==============================] - 0s 88us/step - loss: 0.1680 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 12/13\n",
            "1095/1095 [==============================] - 0s 91us/step - loss: 0.1669 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 13/13\n",
            "1095/1095 [==============================] - 0s 89us/step - loss: 0.1667 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "13\n",
            "121\n",
            "Train on 1095 samples, validate on 365 samples\n",
            "Epoch 1/13\n",
            "1095/1095 [==============================] - 0s 81us/step - loss: 0.1669 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "Epoch 2/13\n",
            "1095/1095 [==============================] - 0s 86us/step - loss: 0.1668 - acc: 0.0000e+00 - val_loss: 0.1387 - val_acc: 0.0000e+00\n",
            "Epoch 3/13\n",
            "1095/1095 [==============================] - 0s 87us/step - loss: 0.1671 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 4/13\n",
            "1095/1095 [==============================] - 0s 85us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "Epoch 5/13\n",
            "1095/1095 [==============================] - 0s 95us/step - loss: 0.1668 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 6/13\n",
            "1095/1095 [==============================] - 0s 86us/step - loss: 0.1670 - acc: 0.0000e+00 - val_loss: 0.1386 - val_acc: 0.0000e+00\n",
            "Epoch 7/13\n",
            "1095/1095 [==============================] - 0s 84us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1387 - val_acc: 0.0000e+00\n",
            "Epoch 8/13\n",
            "1095/1095 [==============================] - 0s 79us/step - loss: 0.1668 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 9/13\n",
            "1095/1095 [==============================] - 0s 84us/step - loss: 0.1669 - acc: 0.0000e+00 - val_loss: 0.1386 - val_acc: 0.0000e+00\n",
            "Epoch 10/13\n",
            "1095/1095 [==============================] - 0s 82us/step - loss: 0.1668 - acc: 0.0000e+00 - val_loss: 0.1391 - val_acc: 0.0000e+00\n",
            "Epoch 11/13\n",
            "1095/1095 [==============================] - 0s 80us/step - loss: 0.1677 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 12/13\n",
            "1095/1095 [==============================] - 0s 81us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1388 - val_acc: 0.0000e+00\n",
            "Epoch 13/13\n",
            "1095/1095 [==============================] - 0s 79us/step - loss: 0.1668 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "13\n",
            "131\n",
            "Train on 1095 samples, validate on 365 samples\n",
            "Epoch 1/13\n",
            "1095/1095 [==============================] - 0s 72us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "Epoch 2/13\n",
            "1095/1095 [==============================] - 0s 73us/step - loss: 0.1667 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 3/13\n",
            "1095/1095 [==============================] - 0s 76us/step - loss: 0.1668 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 4/13\n",
            "1095/1095 [==============================] - 0s 72us/step - loss: 0.1669 - acc: 0.0000e+00 - val_loss: 0.1386 - val_acc: 0.0000e+00\n",
            "Epoch 5/13\n",
            "1095/1095 [==============================] - 0s 72us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1386 - val_acc: 0.0000e+00\n",
            "Epoch 6/13\n",
            "1095/1095 [==============================] - 0s 74us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "Epoch 7/13\n",
            "1095/1095 [==============================] - 0s 74us/step - loss: 0.1668 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 8/13\n",
            "1095/1095 [==============================] - 0s 72us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "Epoch 9/13\n",
            "1095/1095 [==============================] - 0s 69us/step - loss: 0.1667 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 10/13\n",
            "1095/1095 [==============================] - 0s 72us/step - loss: 0.1667 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "Epoch 11/13\n",
            "1095/1095 [==============================] - 0s 75us/step - loss: 0.1667 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 12/13\n",
            "1095/1095 [==============================] - 0s 72us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "Epoch 13/13\n",
            "1095/1095 [==============================] - 0s 72us/step - loss: 0.1671 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "13\n",
            "141\n",
            "Train on 1095 samples, validate on 365 samples\n",
            "Epoch 1/14\n",
            "1095/1095 [==============================] - 8s 7ms/step - loss: 0.1857 - acc: 0.0000e+00 - val_loss: 0.1521 - val_acc: 0.0000e+00\n",
            "Epoch 2/14\n",
            "1095/1095 [==============================] - 8s 7ms/step - loss: 0.1785 - acc: 0.0000e+00 - val_loss: 0.1412 - val_acc: 0.0000e+00\n",
            "Epoch 3/14\n",
            "1095/1095 [==============================] - 8s 7ms/step - loss: 0.1740 - acc: 0.0000e+00 - val_loss: 0.1536 - val_acc: 0.0000e+00\n",
            "Epoch 4/14\n",
            "1095/1095 [==============================] - 8s 7ms/step - loss: 0.1742 - acc: 0.0000e+00 - val_loss: 0.1391 - val_acc: 0.0000e+00\n",
            "Epoch 5/14\n",
            "1095/1095 [==============================] - 8s 7ms/step - loss: 0.1762 - acc: 0.0000e+00 - val_loss: 0.1584 - val_acc: 0.0000e+00\n",
            "Epoch 6/14\n",
            "1095/1095 [==============================] - 8s 7ms/step - loss: 0.1765 - acc: 0.0000e+00 - val_loss: 0.1430 - val_acc: 0.0000e+00\n",
            "Epoch 7/14\n",
            "1095/1095 [==============================] - 8s 7ms/step - loss: 0.1746 - acc: 0.0000e+00 - val_loss: 0.1436 - val_acc: 0.0000e+00\n",
            "Epoch 8/14\n",
            "1095/1095 [==============================] - 8s 7ms/step - loss: 0.1758 - acc: 0.0000e+00 - val_loss: 0.1793 - val_acc: 0.0000e+00\n",
            "Epoch 9/14\n",
            "1095/1095 [==============================] - 8s 8ms/step - loss: 0.1726 - acc: 0.0000e+00 - val_loss: 0.1655 - val_acc: 0.0000e+00\n",
            "Epoch 10/14\n",
            "1095/1095 [==============================] - 8s 7ms/step - loss: 0.1763 - acc: 0.0000e+00 - val_loss: 0.1392 - val_acc: 0.0000e+00\n",
            "Epoch 11/14\n",
            "1095/1095 [==============================] - 8s 7ms/step - loss: 0.1755 - acc: 0.0000e+00 - val_loss: 0.1394 - val_acc: 0.0000e+00\n",
            "Epoch 12/14\n",
            "1095/1095 [==============================] - 8s 7ms/step - loss: 0.1758 - acc: 0.0000e+00 - val_loss: 0.1741 - val_acc: 0.0000e+00\n",
            "Epoch 13/14\n",
            "1095/1095 [==============================] - 8s 7ms/step - loss: 0.1738 - acc: 0.0000e+00 - val_loss: 0.1456 - val_acc: 0.0000e+00\n",
            "Epoch 14/14\n",
            "1095/1095 [==============================] - 8s 7ms/step - loss: 0.1754 - acc: 0.0000e+00 - val_loss: 0.1397 - val_acc: 0.0000e+00\n",
            "14\n",
            "1\n",
            "Train on 1095 samples, validate on 365 samples\n",
            "Epoch 1/14\n",
            "1095/1095 [==============================] - 1s 756us/step - loss: 0.1674 - acc: 0.0000e+00 - val_loss: 0.1408 - val_acc: 0.0000e+00\n",
            "Epoch 2/14\n",
            "1095/1095 [==============================] - 1s 800us/step - loss: 0.1677 - acc: 0.0000e+00 - val_loss: 0.1395 - val_acc: 0.0000e+00\n",
            "Epoch 3/14\n",
            "1095/1095 [==============================] - 1s 761us/step - loss: 0.1678 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 4/14\n",
            "1095/1095 [==============================] - 1s 743us/step - loss: 0.1668 - acc: 0.0000e+00 - val_loss: 0.1418 - val_acc: 0.0000e+00\n",
            "Epoch 5/14\n",
            "1095/1095 [==============================] - 1s 754us/step - loss: 0.1690 - acc: 0.0000e+00 - val_loss: 0.1391 - val_acc: 0.0000e+00\n",
            "Epoch 6/14\n",
            "1095/1095 [==============================] - 1s 740us/step - loss: 0.1680 - acc: 0.0000e+00 - val_loss: 0.1412 - val_acc: 0.0000e+00\n",
            "Epoch 7/14\n",
            "1095/1095 [==============================] - 1s 767us/step - loss: 0.1677 - acc: 0.0000e+00 - val_loss: 0.1388 - val_acc: 0.0000e+00\n",
            "Epoch 8/14\n",
            "1095/1095 [==============================] - 1s 780us/step - loss: 0.1689 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "Epoch 9/14\n",
            "1095/1095 [==============================] - 1s 767us/step - loss: 0.1687 - acc: 0.0000e+00 - val_loss: 0.1386 - val_acc: 0.0000e+00\n",
            "Epoch 10/14\n",
            "1095/1095 [==============================] - 1s 792us/step - loss: 0.1677 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 11/14\n",
            "1095/1095 [==============================] - 1s 785us/step - loss: 0.1688 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "Epoch 12/14\n",
            "1095/1095 [==============================] - 1s 766us/step - loss: 0.1682 - acc: 0.0000e+00 - val_loss: 0.1386 - val_acc: 0.0000e+00\n",
            "Epoch 13/14\n",
            "1095/1095 [==============================] - 1s 776us/step - loss: 0.1675 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "Epoch 14/14\n",
            "1095/1095 [==============================] - 1s 765us/step - loss: 0.1668 - acc: 0.0000e+00 - val_loss: 0.1402 - val_acc: 0.0000e+00\n",
            "14\n",
            "11\n",
            "Train on 1095 samples, validate on 365 samples\n",
            "Epoch 1/14\n",
            "1095/1095 [==============================] - 0s 418us/step - loss: 0.1675 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 2/14\n",
            "1095/1095 [==============================] - 0s 415us/step - loss: 0.1675 - acc: 0.0000e+00 - val_loss: 0.1390 - val_acc: 0.0000e+00\n",
            "Epoch 3/14\n",
            "1095/1095 [==============================] - 0s 411us/step - loss: 0.1672 - acc: 0.0000e+00 - val_loss: 0.1386 - val_acc: 0.0000e+00\n",
            "Epoch 4/14\n",
            "1095/1095 [==============================] - 0s 417us/step - loss: 0.1667 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 5/14\n",
            "1095/1095 [==============================] - 0s 413us/step - loss: 0.1660 - acc: 0.0000e+00 - val_loss: 0.1432 - val_acc: 0.0000e+00\n",
            "Epoch 6/14\n",
            "1095/1095 [==============================] - 0s 436us/step - loss: 0.1677 - acc: 0.0000e+00 - val_loss: 0.1390 - val_acc: 0.0000e+00\n",
            "Epoch 7/14\n",
            "1095/1095 [==============================] - 0s 413us/step - loss: 0.1704 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 8/14\n",
            "1095/1095 [==============================] - 0s 444us/step - loss: 0.1685 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 9/14\n",
            "1095/1095 [==============================] - 0s 446us/step - loss: 0.1668 - acc: 0.0000e+00 - val_loss: 0.1396 - val_acc: 0.0000e+00\n",
            "Epoch 10/14\n",
            "1095/1095 [==============================] - 0s 425us/step - loss: 0.1676 - acc: 0.0000e+00 - val_loss: 0.1386 - val_acc: 0.0000e+00\n",
            "Epoch 11/14\n",
            "1095/1095 [==============================] - 0s 397us/step - loss: 0.1677 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 12/14\n",
            "1095/1095 [==============================] - 0s 413us/step - loss: 0.1681 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 13/14\n",
            "1095/1095 [==============================] - 0s 407us/step - loss: 0.1670 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 14/14\n",
            "1095/1095 [==============================] - 0s 402us/step - loss: 0.1671 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "14\n",
            "21\n",
            "Train on 1095 samples, validate on 365 samples\n",
            "Epoch 1/14\n",
            "1095/1095 [==============================] - 0s 285us/step - loss: 0.1668 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 2/14\n",
            "1095/1095 [==============================] - 0s 285us/step - loss: 0.1676 - acc: 0.0000e+00 - val_loss: 0.1388 - val_acc: 0.0000e+00\n",
            "Epoch 3/14\n",
            "1095/1095 [==============================] - 0s 282us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1396 - val_acc: 0.0000e+00\n",
            "Epoch 4/14\n",
            "1095/1095 [==============================] - 0s 292us/step - loss: 0.1691 - acc: 0.0000e+00 - val_loss: 0.1388 - val_acc: 0.0000e+00\n",
            "Epoch 5/14\n",
            "1095/1095 [==============================] - 0s 274us/step - loss: 0.1670 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 6/14\n",
            "1095/1095 [==============================] - 0s 281us/step - loss: 0.1673 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 7/14\n",
            "1095/1095 [==============================] - 0s 301us/step - loss: 0.1672 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "Epoch 8/14\n",
            "1095/1095 [==============================] - 0s 270us/step - loss: 0.1672 - acc: 0.0000e+00 - val_loss: 0.1392 - val_acc: 0.0000e+00\n",
            "Epoch 9/14\n",
            "1095/1095 [==============================] - 0s 287us/step - loss: 0.1690 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 10/14\n",
            "1095/1095 [==============================] - 0s 281us/step - loss: 0.1675 - acc: 0.0000e+00 - val_loss: 0.1394 - val_acc: 0.0000e+00\n",
            "Epoch 11/14\n",
            "1095/1095 [==============================] - 0s 281us/step - loss: 0.1673 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 12/14\n",
            "1095/1095 [==============================] - 0s 281us/step - loss: 0.1671 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 13/14\n",
            "1095/1095 [==============================] - 0s 289us/step - loss: 0.1668 - acc: 0.0000e+00 - val_loss: 0.1390 - val_acc: 0.0000e+00\n",
            "Epoch 14/14\n",
            "1095/1095 [==============================] - 0s 294us/step - loss: 0.1673 - acc: 0.0000e+00 - val_loss: 0.1389 - val_acc: 0.0000e+00\n",
            "14\n",
            "31\n",
            "Train on 1095 samples, validate on 365 samples\n",
            "Epoch 1/14\n",
            "1095/1095 [==============================] - 0s 205us/step - loss: 0.1668 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "Epoch 2/14\n",
            "1095/1095 [==============================] - 0s 212us/step - loss: 0.1677 - acc: 0.0000e+00 - val_loss: 0.1386 - val_acc: 0.0000e+00\n",
            "Epoch 3/14\n",
            "1095/1095 [==============================] - 0s 211us/step - loss: 0.1669 - acc: 0.0000e+00 - val_loss: 0.1389 - val_acc: 0.0000e+00\n",
            "Epoch 4/14\n",
            "1095/1095 [==============================] - 0s 220us/step - loss: 0.1669 - acc: 0.0000e+00 - val_loss: 0.1392 - val_acc: 0.0000e+00\n",
            "Epoch 5/14\n",
            "1095/1095 [==============================] - 0s 209us/step - loss: 0.1673 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 6/14\n",
            "1095/1095 [==============================] - 0s 209us/step - loss: 0.1667 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 7/14\n",
            "1095/1095 [==============================] - 0s 216us/step - loss: 0.1668 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 8/14\n",
            "1095/1095 [==============================] - 0s 213us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 9/14\n",
            "1095/1095 [==============================] - 0s 216us/step - loss: 0.1673 - acc: 0.0000e+00 - val_loss: 0.1416 - val_acc: 0.0000e+00\n",
            "Epoch 10/14\n",
            "1095/1095 [==============================] - 0s 212us/step - loss: 0.1679 - acc: 0.0000e+00 - val_loss: 0.1386 - val_acc: 0.0000e+00\n",
            "Epoch 11/14\n",
            "1095/1095 [==============================] - 0s 207us/step - loss: 0.1671 - acc: 0.0000e+00 - val_loss: 0.1387 - val_acc: 0.0000e+00\n",
            "Epoch 12/14\n",
            "1095/1095 [==============================] - 0s 207us/step - loss: 0.1679 - acc: 0.0000e+00 - val_loss: 0.1387 - val_acc: 0.0000e+00\n",
            "Epoch 13/14\n",
            "1095/1095 [==============================] - 0s 217us/step - loss: 0.1671 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 14/14\n",
            "1095/1095 [==============================] - 0s 197us/step - loss: 0.1669 - acc: 0.0000e+00 - val_loss: 0.1387 - val_acc: 0.0000e+00\n",
            "14\n",
            "41\n",
            "Train on 1095 samples, validate on 365 samples\n",
            "Epoch 1/14\n",
            "1095/1095 [==============================] - 0s 175us/step - loss: 0.1669 - acc: 0.0000e+00 - val_loss: 0.1389 - val_acc: 0.0000e+00\n",
            "Epoch 2/14\n",
            "1095/1095 [==============================] - 0s 177us/step - loss: 0.1668 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 3/14\n",
            "1095/1095 [==============================] - 0s 173us/step - loss: 0.1672 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 4/14\n",
            "1095/1095 [==============================] - 0s 185us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1389 - val_acc: 0.0000e+00\n",
            "Epoch 5/14\n",
            "1095/1095 [==============================] - 0s 183us/step - loss: 0.1669 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 6/14\n",
            "1095/1095 [==============================] - 0s 170us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 7/14\n",
            "1095/1095 [==============================] - 0s 175us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 8/14\n",
            "1095/1095 [==============================] - 0s 170us/step - loss: 0.1672 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 9/14\n",
            "1095/1095 [==============================] - 0s 182us/step - loss: 0.1677 - acc: 0.0000e+00 - val_loss: 0.1398 - val_acc: 0.0000e+00\n",
            "Epoch 10/14\n",
            "1095/1095 [==============================] - 0s 174us/step - loss: 0.1681 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 11/14\n",
            "1095/1095 [==============================] - 0s 188us/step - loss: 0.1676 - acc: 0.0000e+00 - val_loss: 0.1387 - val_acc: 0.0000e+00\n",
            "Epoch 12/14\n",
            "1095/1095 [==============================] - 0s 177us/step - loss: 0.1691 - acc: 0.0000e+00 - val_loss: 0.1409 - val_acc: 0.0000e+00\n",
            "Epoch 13/14\n",
            "1095/1095 [==============================] - 0s 171us/step - loss: 0.1673 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 14/14\n",
            "1095/1095 [==============================] - 0s 178us/step - loss: 0.1676 - acc: 0.0000e+00 - val_loss: 0.1386 - val_acc: 0.0000e+00\n",
            "14\n",
            "51\n",
            "Train on 1095 samples, validate on 365 samples\n",
            "Epoch 1/14\n",
            "1095/1095 [==============================] - 0s 141us/step - loss: 0.1672 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 2/14\n",
            "1095/1095 [==============================] - 0s 143us/step - loss: 0.1667 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 3/14\n",
            "1095/1095 [==============================] - 0s 146us/step - loss: 0.1668 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "Epoch 4/14\n",
            "1095/1095 [==============================] - 0s 144us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 5/14\n",
            "1095/1095 [==============================] - 0s 150us/step - loss: 0.1669 - acc: 0.0000e+00 - val_loss: 0.1386 - val_acc: 0.0000e+00\n",
            "Epoch 6/14\n",
            "1095/1095 [==============================] - 0s 147us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "Epoch 7/14\n",
            "1095/1095 [==============================] - 0s 161us/step - loss: 0.1671 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 8/14\n",
            "1095/1095 [==============================] - 0s 142us/step - loss: 0.1670 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 9/14\n",
            "1095/1095 [==============================] - 0s 144us/step - loss: 0.1671 - acc: 0.0000e+00 - val_loss: 0.1387 - val_acc: 0.0000e+00\n",
            "Epoch 10/14\n",
            "1095/1095 [==============================] - 0s 144us/step - loss: 0.1668 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "Epoch 11/14\n",
            "1095/1095 [==============================] - 0s 145us/step - loss: 0.1667 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 12/14\n",
            "1095/1095 [==============================] - 0s 146us/step - loss: 0.1668 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 13/14\n",
            "1095/1095 [==============================] - 0s 149us/step - loss: 0.1678 - acc: 0.0000e+00 - val_loss: 0.1405 - val_acc: 0.0000e+00\n",
            "Epoch 14/14\n",
            "1095/1095 [==============================] - 0s 143us/step - loss: 0.1676 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "14\n",
            "61\n",
            "Train on 1095 samples, validate on 365 samples\n",
            "Epoch 1/14\n",
            "1095/1095 [==============================] - 0s 137us/step - loss: 0.1667 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 2/14\n",
            "1095/1095 [==============================] - 0s 133us/step - loss: 0.1667 - acc: 0.0000e+00 - val_loss: 0.1386 - val_acc: 0.0000e+00\n",
            "Epoch 3/14\n",
            "1095/1095 [==============================] - 0s 130us/step - loss: 0.1681 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 4/14\n",
            "1095/1095 [==============================] - 0s 133us/step - loss: 0.1677 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 5/14\n",
            "1095/1095 [==============================] - 0s 137us/step - loss: 0.1681 - acc: 0.0000e+00 - val_loss: 0.1390 - val_acc: 0.0000e+00\n",
            "Epoch 6/14\n",
            "1095/1095 [==============================] - 0s 137us/step - loss: 0.1669 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 7/14\n",
            "1095/1095 [==============================] - 0s 132us/step - loss: 0.1668 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 8/14\n",
            "1095/1095 [==============================] - 0s 132us/step - loss: 0.1668 - acc: 0.0000e+00 - val_loss: 0.1388 - val_acc: 0.0000e+00\n",
            "Epoch 9/14\n",
            "1095/1095 [==============================] - 0s 133us/step - loss: 0.1671 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "Epoch 10/14\n",
            "1095/1095 [==============================] - 0s 133us/step - loss: 0.1669 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 11/14\n",
            "1095/1095 [==============================] - 0s 135us/step - loss: 0.1669 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 12/14\n",
            "1095/1095 [==============================] - 0s 135us/step - loss: 0.1668 - acc: 0.0000e+00 - val_loss: 0.1388 - val_acc: 0.0000e+00\n",
            "Epoch 13/14\n",
            "1095/1095 [==============================] - 0s 131us/step - loss: 0.1674 - acc: 0.0000e+00 - val_loss: 0.1388 - val_acc: 0.0000e+00\n",
            "Epoch 14/14\n",
            "1095/1095 [==============================] - 0s 134us/step - loss: 0.1677 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "14\n",
            "71\n",
            "Train on 1095 samples, validate on 365 samples\n",
            "Epoch 1/14\n",
            "1095/1095 [==============================] - 0s 119us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 2/14\n",
            "1095/1095 [==============================] - 0s 115us/step - loss: 0.1670 - acc: 0.0000e+00 - val_loss: 0.1386 - val_acc: 0.0000e+00\n",
            "Epoch 3/14\n",
            "1095/1095 [==============================] - 0s 117us/step - loss: 0.1668 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "Epoch 4/14\n",
            "1095/1095 [==============================] - 0s 115us/step - loss: 0.1667 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 5/14\n",
            "1095/1095 [==============================] - 0s 123us/step - loss: 0.1667 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 6/14\n",
            "1095/1095 [==============================] - 0s 122us/step - loss: 0.1668 - acc: 0.0000e+00 - val_loss: 0.1386 - val_acc: 0.0000e+00\n",
            "Epoch 7/14\n",
            "1095/1095 [==============================] - 0s 121us/step - loss: 0.1670 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 8/14\n",
            "1095/1095 [==============================] - 0s 115us/step - loss: 0.1668 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 9/14\n",
            "1095/1095 [==============================] - 0s 119us/step - loss: 0.1667 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 10/14\n",
            "1095/1095 [==============================] - 0s 115us/step - loss: 0.1668 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 11/14\n",
            "1095/1095 [==============================] - 0s 117us/step - loss: 0.1668 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 12/14\n",
            "1095/1095 [==============================] - 0s 115us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 13/14\n",
            "1095/1095 [==============================] - 0s 114us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 14/14\n",
            "1095/1095 [==============================] - 0s 120us/step - loss: 0.1667 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "14\n",
            "81\n",
            "Train on 1095 samples, validate on 365 samples\n",
            "Epoch 1/14\n",
            "1095/1095 [==============================] - 0s 108us/step - loss: 0.1670 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 2/14\n",
            "1095/1095 [==============================] - 0s 106us/step - loss: 0.1667 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "Epoch 3/14\n",
            "1095/1095 [==============================] - 0s 105us/step - loss: 0.1678 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 4/14\n",
            "1095/1095 [==============================] - 0s 106us/step - loss: 0.1667 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "Epoch 5/14\n",
            "1095/1095 [==============================] - 0s 109us/step - loss: 0.1680 - acc: 0.0000e+00 - val_loss: 0.1388 - val_acc: 0.0000e+00\n",
            "Epoch 6/14\n",
            "1095/1095 [==============================] - 0s 106us/step - loss: 0.1683 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 7/14\n",
            "1095/1095 [==============================] - 0s 107us/step - loss: 0.1713 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "Epoch 8/14\n",
            "1095/1095 [==============================] - 0s 113us/step - loss: 0.1751 - acc: 0.0000e+00 - val_loss: 0.1405 - val_acc: 0.0000e+00\n",
            "Epoch 9/14\n",
            "1095/1095 [==============================] - 0s 107us/step - loss: 0.1680 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "Epoch 10/14\n",
            "1095/1095 [==============================] - 0s 106us/step - loss: 0.1691 - acc: 0.0000e+00 - val_loss: 0.1391 - val_acc: 0.0000e+00\n",
            "Epoch 11/14\n",
            "1095/1095 [==============================] - 0s 106us/step - loss: 0.1675 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "Epoch 12/14\n",
            "1095/1095 [==============================] - 0s 114us/step - loss: 0.1671 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "Epoch 13/14\n",
            "1095/1095 [==============================] - 0s 114us/step - loss: 0.1674 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 14/14\n",
            "1095/1095 [==============================] - 0s 109us/step - loss: 0.1667 - acc: 0.0000e+00 - val_loss: 0.1386 - val_acc: 0.0000e+00\n",
            "14\n",
            "91\n",
            "Train on 1095 samples, validate on 365 samples\n",
            "Epoch 1/14\n",
            "1095/1095 [==============================] - 0s 92us/step - loss: 0.1667 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 2/14\n",
            "1095/1095 [==============================] - 0s 96us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 3/14\n",
            "1095/1095 [==============================] - 0s 94us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "Epoch 4/14\n",
            "1095/1095 [==============================] - 0s 91us/step - loss: 0.1669 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 5/14\n",
            "1095/1095 [==============================] - 0s 93us/step - loss: 0.1664 - acc: 0.0000e+00 - val_loss: 0.1387 - val_acc: 0.0000e+00\n",
            "Epoch 6/14\n",
            "1095/1095 [==============================] - 0s 102us/step - loss: 0.1668 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 7/14\n",
            "1095/1095 [==============================] - 0s 99us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 8/14\n",
            "1095/1095 [==============================] - 0s 95us/step - loss: 0.1675 - acc: 0.0000e+00 - val_loss: 0.1386 - val_acc: 0.0000e+00\n",
            "Epoch 9/14\n",
            "1095/1095 [==============================] - 0s 96us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1386 - val_acc: 0.0000e+00\n",
            "Epoch 10/14\n",
            "1095/1095 [==============================] - 0s 93us/step - loss: 0.1667 - acc: 0.0000e+00 - val_loss: 0.1386 - val_acc: 0.0000e+00\n",
            "Epoch 11/14\n",
            "1095/1095 [==============================] - 0s 95us/step - loss: 0.1667 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 12/14\n",
            "1095/1095 [==============================] - 0s 100us/step - loss: 0.1667 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 13/14\n",
            "1095/1095 [==============================] - 0s 95us/step - loss: 0.1667 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 14/14\n",
            "1095/1095 [==============================] - 0s 93us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "14\n",
            "101\n",
            "Train on 1095 samples, validate on 365 samples\n",
            "Epoch 1/14\n",
            "1095/1095 [==============================] - 0s 88us/step - loss: 0.1670 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 2/14\n",
            "1095/1095 [==============================] - 0s 87us/step - loss: 0.1665 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "Epoch 3/14\n",
            "1095/1095 [==============================] - 0s 87us/step - loss: 0.1669 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 4/14\n",
            "1095/1095 [==============================] - 0s 85us/step - loss: 0.1669 - acc: 0.0000e+00 - val_loss: 0.1386 - val_acc: 0.0000e+00\n",
            "Epoch 5/14\n",
            "1095/1095 [==============================] - 0s 88us/step - loss: 0.1670 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 6/14\n",
            "1095/1095 [==============================] - 0s 90us/step - loss: 0.1671 - acc: 0.0000e+00 - val_loss: 0.1386 - val_acc: 0.0000e+00\n",
            "Epoch 7/14\n",
            "1095/1095 [==============================] - 0s 85us/step - loss: 0.1665 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "Epoch 8/14\n",
            "1095/1095 [==============================] - 0s 96us/step - loss: 0.1667 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "Epoch 9/14\n",
            "1095/1095 [==============================] - 0s 90us/step - loss: 0.1674 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 10/14\n",
            "1095/1095 [==============================] - 0s 86us/step - loss: 0.1669 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 11/14\n",
            "1095/1095 [==============================] - 0s 86us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 12/14\n",
            "1095/1095 [==============================] - 0s 88us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 13/14\n",
            "1095/1095 [==============================] - 0s 87us/step - loss: 0.1665 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 14/14\n",
            "1095/1095 [==============================] - 0s 86us/step - loss: 0.1668 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "14\n",
            "111\n",
            "Train on 1095 samples, validate on 365 samples\n",
            "Epoch 1/14\n",
            "1095/1095 [==============================] - 0s 84us/step - loss: 0.1670 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 2/14\n",
            "1095/1095 [==============================] - 0s 87us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "Epoch 3/14\n",
            "1095/1095 [==============================] - 0s 89us/step - loss: 0.1665 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 4/14\n",
            "1095/1095 [==============================] - 0s 87us/step - loss: 0.1667 - acc: 0.0000e+00 - val_loss: 0.1391 - val_acc: 0.0000e+00\n",
            "Epoch 5/14\n",
            "1095/1095 [==============================] - 0s 109us/step - loss: 0.1682 - acc: 0.0000e+00 - val_loss: 0.1391 - val_acc: 0.0000e+00\n",
            "Epoch 6/14\n",
            "1095/1095 [==============================] - 0s 93us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1387 - val_acc: 0.0000e+00\n",
            "Epoch 7/14\n",
            "1095/1095 [==============================] - 0s 85us/step - loss: 0.1673 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 8/14\n",
            "1095/1095 [==============================] - 0s 89us/step - loss: 0.1671 - acc: 0.0000e+00 - val_loss: 0.1387 - val_acc: 0.0000e+00\n",
            "Epoch 9/14\n",
            "1095/1095 [==============================] - 0s 94us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 10/14\n",
            "1095/1095 [==============================] - 0s 87us/step - loss: 0.1665 - acc: 0.0000e+00 - val_loss: 0.1386 - val_acc: 0.0000e+00\n",
            "Epoch 11/14\n",
            "1095/1095 [==============================] - 0s 89us/step - loss: 0.1668 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 12/14\n",
            "1095/1095 [==============================] - 0s 86us/step - loss: 0.1679 - acc: 0.0000e+00 - val_loss: 0.1388 - val_acc: 0.0000e+00\n",
            "Epoch 13/14\n",
            "1095/1095 [==============================] - 0s 89us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1396 - val_acc: 0.0000e+00\n",
            "Epoch 14/14\n",
            "1095/1095 [==============================] - 0s 90us/step - loss: 0.1675 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "14\n",
            "121\n",
            "Train on 1095 samples, validate on 365 samples\n",
            "Epoch 1/14\n",
            "1095/1095 [==============================] - 0s 79us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 2/14\n",
            "1095/1095 [==============================] - 0s 80us/step - loss: 0.1667 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "Epoch 3/14\n",
            "1095/1095 [==============================] - 0s 77us/step - loss: 0.1665 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 4/14\n",
            "1095/1095 [==============================] - 0s 78us/step - loss: 0.1667 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "Epoch 5/14\n",
            "1095/1095 [==============================] - 0s 82us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 6/14\n",
            "1095/1095 [==============================] - 0s 80us/step - loss: 0.1670 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 7/14\n",
            "1095/1095 [==============================] - 0s 80us/step - loss: 0.1665 - acc: 0.0000e+00 - val_loss: 0.1387 - val_acc: 0.0000e+00\n",
            "Epoch 8/14\n",
            "1095/1095 [==============================] - 0s 81us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 9/14\n",
            "1095/1095 [==============================] - 0s 77us/step - loss: 0.1669 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 10/14\n",
            "1095/1095 [==============================] - 0s 80us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 11/14\n",
            "1095/1095 [==============================] - 0s 84us/step - loss: 0.1667 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "Epoch 12/14\n",
            "1095/1095 [==============================] - 0s 86us/step - loss: 0.1671 - acc: 0.0000e+00 - val_loss: 0.1387 - val_acc: 0.0000e+00\n",
            "Epoch 13/14\n",
            "1095/1095 [==============================] - 0s 86us/step - loss: 0.1669 - acc: 0.0000e+00 - val_loss: 0.1390 - val_acc: 0.0000e+00\n",
            "Epoch 14/14\n",
            "1095/1095 [==============================] - 0s 83us/step - loss: 0.1668 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "14\n",
            "131\n",
            "Train on 1095 samples, validate on 365 samples\n",
            "Epoch 1/14\n",
            "1095/1095 [==============================] - 0s 77us/step - loss: 0.1667 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 2/14\n",
            "1095/1095 [==============================] - 0s 70us/step - loss: 0.1670 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 3/14\n",
            "1095/1095 [==============================] - 0s 76us/step - loss: 0.1664 - acc: 0.0000e+00 - val_loss: 0.1386 - val_acc: 0.0000e+00\n",
            "Epoch 4/14\n",
            "1095/1095 [==============================] - 0s 74us/step - loss: 0.1668 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "Epoch 5/14\n",
            "1095/1095 [==============================] - 0s 73us/step - loss: 0.1670 - acc: 0.0000e+00 - val_loss: 0.1386 - val_acc: 0.0000e+00\n",
            "Epoch 6/14\n",
            "1095/1095 [==============================] - 0s 71us/step - loss: 0.1671 - acc: 0.0000e+00 - val_loss: 0.1387 - val_acc: 0.0000e+00\n",
            "Epoch 7/14\n",
            "1095/1095 [==============================] - 0s 71us/step - loss: 0.1672 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 8/14\n",
            "1095/1095 [==============================] - 0s 73us/step - loss: 0.1664 - acc: 0.0000e+00 - val_loss: 0.1386 - val_acc: 0.0000e+00\n",
            "Epoch 9/14\n",
            "1095/1095 [==============================] - 0s 75us/step - loss: 0.1667 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "Epoch 10/14\n",
            "1095/1095 [==============================] - 0s 78us/step - loss: 0.1669 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 11/14\n",
            "1095/1095 [==============================] - 0s 74us/step - loss: 0.1668 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 12/14\n",
            "1095/1095 [==============================] - 0s 86us/step - loss: 0.1669 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "Epoch 13/14\n",
            "1095/1095 [==============================] - 0s 77us/step - loss: 0.1672 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 14/14\n",
            "1095/1095 [==============================] - 0s 74us/step - loss: 0.1667 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "14\n",
            "141\n",
            "Train on 1095 samples, validate on 365 samples\n",
            "Epoch 1/15\n",
            "1095/1095 [==============================] - 8s 7ms/step - loss: 0.1814 - acc: 0.0000e+00 - val_loss: 0.1465 - val_acc: 0.0000e+00\n",
            "Epoch 2/15\n",
            "1095/1095 [==============================] - 8s 7ms/step - loss: 0.1785 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 3/15\n",
            "1095/1095 [==============================] - 8s 7ms/step - loss: 0.1787 - acc: 0.0000e+00 - val_loss: 0.1624 - val_acc: 0.0000e+00\n",
            "Epoch 4/15\n",
            "1095/1095 [==============================] - 8s 7ms/step - loss: 0.1775 - acc: 0.0000e+00 - val_loss: 0.1413 - val_acc: 0.0000e+00\n",
            "Epoch 5/15\n",
            "1095/1095 [==============================] - 8s 7ms/step - loss: 0.1772 - acc: 0.0000e+00 - val_loss: 0.1635 - val_acc: 0.0000e+00\n",
            "Epoch 6/15\n",
            "1095/1095 [==============================] - 8s 7ms/step - loss: 0.1734 - acc: 0.0000e+00 - val_loss: 0.1498 - val_acc: 0.0000e+00\n",
            "Epoch 7/15\n",
            "1095/1095 [==============================] - 8s 7ms/step - loss: 0.1745 - acc: 0.0000e+00 - val_loss: 0.1386 - val_acc: 0.0000e+00\n",
            "Epoch 8/15\n",
            "1095/1095 [==============================] - 8s 7ms/step - loss: 0.1754 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 9/15\n",
            "1095/1095 [==============================] - 8s 7ms/step - loss: 0.1750 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 10/15\n",
            "1095/1095 [==============================] - 8s 7ms/step - loss: 0.1766 - acc: 0.0000e+00 - val_loss: 0.1434 - val_acc: 0.0000e+00\n",
            "Epoch 11/15\n",
            "1095/1095 [==============================] - 8s 7ms/step - loss: 0.1766 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 12/15\n",
            "1095/1095 [==============================] - 8s 7ms/step - loss: 0.1734 - acc: 0.0000e+00 - val_loss: 0.1407 - val_acc: 0.0000e+00\n",
            "Epoch 13/15\n",
            "1095/1095 [==============================] - 8s 7ms/step - loss: 0.1763 - acc: 0.0000e+00 - val_loss: 0.1495 - val_acc: 0.0000e+00\n",
            "Epoch 14/15\n",
            "1095/1095 [==============================] - 8s 7ms/step - loss: 0.1734 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 15/15\n",
            "1095/1095 [==============================] - 8s 7ms/step - loss: 0.1743 - acc: 0.0000e+00 - val_loss: 0.1739 - val_acc: 0.0000e+00\n",
            "15\n",
            "1\n",
            "Train on 1095 samples, validate on 365 samples\n",
            "Epoch 1/15\n",
            "1095/1095 [==============================] - 1s 752us/step - loss: 0.1752 - acc: 0.0000e+00 - val_loss: 0.1407 - val_acc: 0.0000e+00\n",
            "Epoch 2/15\n",
            "1095/1095 [==============================] - 1s 786us/step - loss: 0.1677 - acc: 0.0000e+00 - val_loss: 0.1389 - val_acc: 0.0000e+00\n",
            "Epoch 3/15\n",
            "1095/1095 [==============================] - 1s 771us/step - loss: 0.1682 - acc: 0.0000e+00 - val_loss: 0.1393 - val_acc: 0.0000e+00\n",
            "Epoch 4/15\n",
            "1095/1095 [==============================] - 1s 795us/step - loss: 0.1675 - acc: 0.0000e+00 - val_loss: 0.1395 - val_acc: 0.0000e+00\n",
            "Epoch 5/15\n",
            "1095/1095 [==============================] - 1s 792us/step - loss: 0.1675 - acc: 0.0000e+00 - val_loss: 0.1398 - val_acc: 0.0000e+00\n",
            "Epoch 6/15\n",
            "1095/1095 [==============================] - 1s 794us/step - loss: 0.1671 - acc: 0.0000e+00 - val_loss: 0.1429 - val_acc: 0.0000e+00\n",
            "Epoch 7/15\n",
            "1095/1095 [==============================] - 1s 774us/step - loss: 0.1672 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "Epoch 8/15\n",
            "1095/1095 [==============================] - 1s 781us/step - loss: 0.1674 - acc: 0.0000e+00 - val_loss: 0.1406 - val_acc: 0.0000e+00\n",
            "Epoch 9/15\n",
            "1095/1095 [==============================] - 1s 788us/step - loss: 0.1686 - acc: 0.0000e+00 - val_loss: 0.1387 - val_acc: 0.0000e+00\n",
            "Epoch 10/15\n",
            "1095/1095 [==============================] - 1s 781us/step - loss: 0.1685 - acc: 0.0000e+00 - val_loss: 0.1408 - val_acc: 0.0000e+00\n",
            "Epoch 11/15\n",
            "1095/1095 [==============================] - 1s 776us/step - loss: 0.1689 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 12/15\n",
            "1095/1095 [==============================] - 1s 774us/step - loss: 0.1684 - acc: 0.0000e+00 - val_loss: 0.1397 - val_acc: 0.0000e+00\n",
            "Epoch 13/15\n",
            "1095/1095 [==============================] - 1s 797us/step - loss: 0.1683 - acc: 0.0000e+00 - val_loss: 0.1387 - val_acc: 0.0000e+00\n",
            "Epoch 14/15\n",
            "1095/1095 [==============================] - 1s 795us/step - loss: 0.1682 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 15/15\n",
            "1095/1095 [==============================] - 1s 779us/step - loss: 0.1673 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "15\n",
            "11\n",
            "Train on 1095 samples, validate on 365 samples\n",
            "Epoch 1/15\n",
            "1095/1095 [==============================] - 0s 425us/step - loss: 0.1673 - acc: 0.0000e+00 - val_loss: 0.1398 - val_acc: 0.0000e+00\n",
            "Epoch 2/15\n",
            "1095/1095 [==============================] - 0s 434us/step - loss: 0.1678 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 3/15\n",
            "1095/1095 [==============================] - 0s 434us/step - loss: 0.1675 - acc: 0.0000e+00 - val_loss: 0.1394 - val_acc: 0.0000e+00\n",
            "Epoch 4/15\n",
            "1095/1095 [==============================] - 0s 420us/step - loss: 0.1670 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 5/15\n",
            "1095/1095 [==============================] - 0s 425us/step - loss: 0.1671 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 6/15\n",
            "1095/1095 [==============================] - 0s 417us/step - loss: 0.1677 - acc: 0.0000e+00 - val_loss: 0.1398 - val_acc: 0.0000e+00\n",
            "Epoch 7/15\n",
            "1095/1095 [==============================] - 0s 422us/step - loss: 0.1684 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 8/15\n",
            "1095/1095 [==============================] - 0s 420us/step - loss: 0.1675 - acc: 0.0000e+00 - val_loss: 0.1386 - val_acc: 0.0000e+00\n",
            "Epoch 9/15\n",
            "1095/1095 [==============================] - 0s 401us/step - loss: 0.1678 - acc: 0.0000e+00 - val_loss: 0.1386 - val_acc: 0.0000e+00\n",
            "Epoch 10/15\n",
            "1095/1095 [==============================] - 0s 414us/step - loss: 0.1668 - acc: 0.0000e+00 - val_loss: 0.1423 - val_acc: 0.0000e+00\n",
            "Epoch 11/15\n",
            "1095/1095 [==============================] - 0s 418us/step - loss: 0.1673 - acc: 0.0000e+00 - val_loss: 0.1401 - val_acc: 0.0000e+00\n",
            "Epoch 12/15\n",
            "1095/1095 [==============================] - 0s 404us/step - loss: 0.1685 - acc: 0.0000e+00 - val_loss: 0.1408 - val_acc: 0.0000e+00\n",
            "Epoch 13/15\n",
            "1095/1095 [==============================] - 0s 409us/step - loss: 0.1664 - acc: 0.0000e+00 - val_loss: 0.1402 - val_acc: 0.0000e+00\n",
            "Epoch 14/15\n",
            "1095/1095 [==============================] - 0s 420us/step - loss: 0.1685 - acc: 0.0000e+00 - val_loss: 0.1392 - val_acc: 0.0000e+00\n",
            "Epoch 15/15\n",
            "1095/1095 [==============================] - 0s 414us/step - loss: 0.1681 - acc: 0.0000e+00 - val_loss: 0.1386 - val_acc: 0.0000e+00\n",
            "15\n",
            "21\n",
            "Train on 1095 samples, validate on 365 samples\n",
            "Epoch 1/15\n",
            "1095/1095 [==============================] - 0s 279us/step - loss: 0.1670 - acc: 0.0000e+00 - val_loss: 0.1388 - val_acc: 0.0000e+00\n",
            "Epoch 2/15\n",
            "1095/1095 [==============================] - 0s 289us/step - loss: 0.1668 - acc: 0.0000e+00 - val_loss: 0.1394 - val_acc: 0.0000e+00\n",
            "Epoch 3/15\n",
            "1095/1095 [==============================] - 0s 281us/step - loss: 0.1675 - acc: 0.0000e+00 - val_loss: 0.1386 - val_acc: 0.0000e+00\n",
            "Epoch 4/15\n",
            "1095/1095 [==============================] - 0s 295us/step - loss: 0.1670 - acc: 0.0000e+00 - val_loss: 0.1393 - val_acc: 0.0000e+00\n",
            "Epoch 5/15\n",
            "1095/1095 [==============================] - 0s 287us/step - loss: 0.1663 - acc: 0.0000e+00 - val_loss: 0.1397 - val_acc: 0.0000e+00\n",
            "Epoch 6/15\n",
            "1095/1095 [==============================] - 0s 276us/step - loss: 0.1678 - acc: 0.0000e+00 - val_loss: 0.1400 - val_acc: 0.0000e+00\n",
            "Epoch 7/15\n",
            "1095/1095 [==============================] - 0s 291us/step - loss: 0.1667 - acc: 0.0000e+00 - val_loss: 0.1386 - val_acc: 0.0000e+00\n",
            "Epoch 8/15\n",
            "1095/1095 [==============================] - 0s 288us/step - loss: 0.1680 - acc: 0.0000e+00 - val_loss: 0.1386 - val_acc: 0.0000e+00\n",
            "Epoch 9/15\n",
            "1095/1095 [==============================] - 0s 281us/step - loss: 0.1667 - acc: 0.0000e+00 - val_loss: 0.1395 - val_acc: 0.0000e+00\n",
            "Epoch 10/15\n",
            "1095/1095 [==============================] - 0s 269us/step - loss: 0.1689 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "Epoch 11/15\n",
            "1095/1095 [==============================] - 0s 276us/step - loss: 0.1677 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "Epoch 12/15\n",
            "1095/1095 [==============================] - 0s 277us/step - loss: 0.1676 - acc: 0.0000e+00 - val_loss: 0.1387 - val_acc: 0.0000e+00\n",
            "Epoch 13/15\n",
            "1095/1095 [==============================] - 0s 279us/step - loss: 0.1667 - acc: 0.0000e+00 - val_loss: 0.1394 - val_acc: 0.0000e+00\n",
            "Epoch 14/15\n",
            "1095/1095 [==============================] - 0s 271us/step - loss: 0.1675 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "Epoch 15/15\n",
            "1095/1095 [==============================] - 0s 276us/step - loss: 0.1671 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "15\n",
            "31\n",
            "Train on 1095 samples, validate on 365 samples\n",
            "Epoch 1/15\n",
            "1095/1095 [==============================] - 0s 213us/step - loss: 0.1668 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "Epoch 2/15\n",
            "1095/1095 [==============================] - 0s 213us/step - loss: 0.1671 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 3/15\n",
            "1095/1095 [==============================] - 0s 208us/step - loss: 0.1670 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "Epoch 4/15\n",
            "1095/1095 [==============================] - 0s 204us/step - loss: 0.1667 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "Epoch 5/15\n",
            "1095/1095 [==============================] - 0s 218us/step - loss: 0.1670 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "Epoch 6/15\n",
            "1095/1095 [==============================] - 0s 211us/step - loss: 0.1683 - acc: 0.0000e+00 - val_loss: 0.1389 - val_acc: 0.0000e+00\n",
            "Epoch 7/15\n",
            "1095/1095 [==============================] - 0s 210us/step - loss: 0.1674 - acc: 0.0000e+00 - val_loss: 0.1390 - val_acc: 0.0000e+00\n",
            "Epoch 8/15\n",
            "1095/1095 [==============================] - 0s 212us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "Epoch 9/15\n",
            "1095/1095 [==============================] - 0s 217us/step - loss: 0.1668 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 10/15\n",
            "1095/1095 [==============================] - 0s 216us/step - loss: 0.1667 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 11/15\n",
            "1095/1095 [==============================] - 0s 213us/step - loss: 0.1676 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 12/15\n",
            "1095/1095 [==============================] - 0s 214us/step - loss: 0.1674 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 13/15\n",
            "1095/1095 [==============================] - 0s 219us/step - loss: 0.1669 - acc: 0.0000e+00 - val_loss: 0.1386 - val_acc: 0.0000e+00\n",
            "Epoch 14/15\n",
            "1095/1095 [==============================] - 0s 211us/step - loss: 0.1669 - acc: 0.0000e+00 - val_loss: 0.1387 - val_acc: 0.0000e+00\n",
            "Epoch 15/15\n",
            "1095/1095 [==============================] - 0s 209us/step - loss: 0.1670 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "15\n",
            "41\n",
            "Train on 1095 samples, validate on 365 samples\n",
            "Epoch 1/15\n",
            "1095/1095 [==============================] - 0s 171us/step - loss: 0.1668 - acc: 0.0000e+00 - val_loss: 0.1388 - val_acc: 0.0000e+00\n",
            "Epoch 2/15\n",
            "1095/1095 [==============================] - 0s 169us/step - loss: 0.1669 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "Epoch 3/15\n",
            "1095/1095 [==============================] - 0s 181us/step - loss: 0.1668 - acc: 0.0000e+00 - val_loss: 0.1386 - val_acc: 0.0000e+00\n",
            "Epoch 4/15\n",
            "1095/1095 [==============================] - 0s 177us/step - loss: 0.1667 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "Epoch 5/15\n",
            "1095/1095 [==============================] - 0s 184us/step - loss: 0.1669 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 6/15\n",
            "1095/1095 [==============================] - 0s 179us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "Epoch 7/15\n",
            "1095/1095 [==============================] - 0s 175us/step - loss: 0.1667 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 8/15\n",
            "1095/1095 [==============================] - 0s 181us/step - loss: 0.1670 - acc: 0.0000e+00 - val_loss: 0.1389 - val_acc: 0.0000e+00\n",
            "Epoch 9/15\n",
            "1095/1095 [==============================] - 0s 180us/step - loss: 0.1674 - acc: 0.0000e+00 - val_loss: 0.1401 - val_acc: 0.0000e+00\n",
            "Epoch 10/15\n",
            "1095/1095 [==============================] - 0s 181us/step - loss: 0.1669 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 11/15\n",
            "1095/1095 [==============================] - 0s 169us/step - loss: 0.1667 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "Epoch 12/15\n",
            "1095/1095 [==============================] - 0s 174us/step - loss: 0.1669 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "Epoch 13/15\n",
            "1095/1095 [==============================] - 0s 167us/step - loss: 0.1667 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 14/15\n",
            "1095/1095 [==============================] - 0s 189us/step - loss: 0.1669 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 15/15\n",
            "1095/1095 [==============================] - 0s 173us/step - loss: 0.1668 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "15\n",
            "51\n",
            "Train on 1095 samples, validate on 365 samples\n",
            "Epoch 1/15\n",
            "1095/1095 [==============================] - 0s 144us/step - loss: 0.1669 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 2/15\n",
            "1095/1095 [==============================] - 0s 143us/step - loss: 0.1668 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "Epoch 3/15\n",
            "1095/1095 [==============================] - 0s 146us/step - loss: 0.1667 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 4/15\n",
            "1095/1095 [==============================] - 0s 145us/step - loss: 0.1668 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 5/15\n",
            "1095/1095 [==============================] - 0s 154us/step - loss: 0.1672 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 6/15\n",
            "1095/1095 [==============================] - 0s 141us/step - loss: 0.1672 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 7/15\n",
            "1095/1095 [==============================] - 0s 144us/step - loss: 0.1673 - acc: 0.0000e+00 - val_loss: 0.1386 - val_acc: 0.0000e+00\n",
            "Epoch 8/15\n",
            "1095/1095 [==============================] - 0s 144us/step - loss: 0.1674 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "Epoch 9/15\n",
            "1095/1095 [==============================] - 0s 148us/step - loss: 0.1674 - acc: 0.0000e+00 - val_loss: 0.1391 - val_acc: 0.0000e+00\n",
            "Epoch 10/15\n",
            "1095/1095 [==============================] - 0s 145us/step - loss: 0.1677 - acc: 0.0000e+00 - val_loss: 0.1388 - val_acc: 0.0000e+00\n",
            "Epoch 11/15\n",
            "1095/1095 [==============================] - 0s 146us/step - loss: 0.1667 - acc: 0.0000e+00 - val_loss: 0.1386 - val_acc: 0.0000e+00\n",
            "Epoch 12/15\n",
            "1095/1095 [==============================] - 0s 140us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "Epoch 13/15\n",
            "1095/1095 [==============================] - 0s 144us/step - loss: 0.1671 - acc: 0.0000e+00 - val_loss: 0.1395 - val_acc: 0.0000e+00\n",
            "Epoch 14/15\n",
            "1095/1095 [==============================] - 0s 144us/step - loss: 0.1672 - acc: 0.0000e+00 - val_loss: 0.1386 - val_acc: 0.0000e+00\n",
            "Epoch 15/15\n",
            "1095/1095 [==============================] - 0s 146us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "15\n",
            "61\n",
            "Train on 1095 samples, validate on 365 samples\n",
            "Epoch 1/15\n",
            "1095/1095 [==============================] - 0s 132us/step - loss: 0.1676 - acc: 0.0000e+00 - val_loss: 0.1386 - val_acc: 0.0000e+00\n",
            "Epoch 2/15\n",
            "1095/1095 [==============================] - 0s 150us/step - loss: 0.1676 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 3/15\n",
            "1095/1095 [==============================] - 0s 141us/step - loss: 0.1686 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "Epoch 4/15\n",
            "1095/1095 [==============================] - 0s 141us/step - loss: 0.1669 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 5/15\n",
            "1095/1095 [==============================] - 0s 129us/step - loss: 0.1667 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 6/15\n",
            "1095/1095 [==============================] - 0s 130us/step - loss: 0.1668 - acc: 0.0000e+00 - val_loss: 0.1386 - val_acc: 0.0000e+00\n",
            "Epoch 7/15\n",
            "1095/1095 [==============================] - 0s 128us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 8/15\n",
            "1095/1095 [==============================] - 0s 126us/step - loss: 0.1669 - acc: 0.0000e+00 - val_loss: 0.1387 - val_acc: 0.0000e+00\n",
            "Epoch 9/15\n",
            "1095/1095 [==============================] - 0s 140us/step - loss: 0.1668 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 10/15\n",
            "1095/1095 [==============================] - 0s 131us/step - loss: 0.1668 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 11/15\n",
            "1095/1095 [==============================] - 0s 134us/step - loss: 0.1667 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "Epoch 12/15\n",
            "1095/1095 [==============================] - 0s 131us/step - loss: 0.1669 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "Epoch 13/15\n",
            "1095/1095 [==============================] - 0s 129us/step - loss: 0.1669 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 14/15\n",
            "1095/1095 [==============================] - 0s 130us/step - loss: 0.1668 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 15/15\n",
            "1095/1095 [==============================] - 0s 137us/step - loss: 0.1675 - acc: 0.0000e+00 - val_loss: 0.1387 - val_acc: 0.0000e+00\n",
            "15\n",
            "71\n",
            "Train on 1095 samples, validate on 365 samples\n",
            "Epoch 1/15\n",
            "1095/1095 [==============================] - 0s 126us/step - loss: 0.1678 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 2/15\n",
            "1095/1095 [==============================] - 0s 113us/step - loss: 0.1669 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 3/15\n",
            "1095/1095 [==============================] - 0s 112us/step - loss: 0.1667 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 4/15\n",
            "1095/1095 [==============================] - 0s 113us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "Epoch 5/15\n",
            "1095/1095 [==============================] - 0s 111us/step - loss: 0.1669 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 6/15\n",
            "1095/1095 [==============================] - 0s 110us/step - loss: 0.1673 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 7/15\n",
            "1095/1095 [==============================] - 0s 113us/step - loss: 0.1674 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 8/15\n",
            "1095/1095 [==============================] - 0s 111us/step - loss: 0.1673 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 9/15\n",
            "1095/1095 [==============================] - 0s 121us/step - loss: 0.1675 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 10/15\n",
            "1095/1095 [==============================] - 0s 115us/step - loss: 0.1665 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 11/15\n",
            "1095/1095 [==============================] - 0s 118us/step - loss: 0.1669 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 12/15\n",
            "1095/1095 [==============================] - 0s 126us/step - loss: 0.1667 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 13/15\n",
            "1095/1095 [==============================] - 0s 116us/step - loss: 0.1670 - acc: 0.0000e+00 - val_loss: 0.1387 - val_acc: 0.0000e+00\n",
            "Epoch 14/15\n",
            "1095/1095 [==============================] - 0s 120us/step - loss: 0.1669 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "Epoch 15/15\n",
            "1095/1095 [==============================] - 0s 117us/step - loss: 0.1667 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "15\n",
            "81\n",
            "Train on 1095 samples, validate on 365 samples\n",
            "Epoch 1/15\n",
            "1095/1095 [==============================] - 0s 109us/step - loss: 0.1669 - acc: 0.0000e+00 - val_loss: 0.1386 - val_acc: 0.0000e+00\n",
            "Epoch 2/15\n",
            "1095/1095 [==============================] - 0s 115us/step - loss: 0.1671 - acc: 0.0000e+00 - val_loss: 0.1386 - val_acc: 0.0000e+00\n",
            "Epoch 3/15\n",
            "1095/1095 [==============================] - 0s 106us/step - loss: 0.1687 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 4/15\n",
            "1095/1095 [==============================] - 0s 105us/step - loss: 0.1669 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "Epoch 5/15\n",
            "1095/1095 [==============================] - 0s 105us/step - loss: 0.1671 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 6/15\n",
            "1095/1095 [==============================] - 0s 107us/step - loss: 0.1670 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 7/15\n",
            "1095/1095 [==============================] - 0s 104us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 8/15\n",
            "1095/1095 [==============================] - 0s 104us/step - loss: 0.1667 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "Epoch 9/15\n",
            "1095/1095 [==============================] - 0s 107us/step - loss: 0.1669 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "Epoch 10/15\n",
            "1095/1095 [==============================] - 0s 112us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1386 - val_acc: 0.0000e+00\n",
            "Epoch 11/15\n",
            "1095/1095 [==============================] - 0s 122us/step - loss: 0.1685 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 12/15\n",
            "1095/1095 [==============================] - 0s 113us/step - loss: 0.1690 - acc: 0.0000e+00 - val_loss: 0.1387 - val_acc: 0.0000e+00\n",
            "Epoch 13/15\n",
            "1095/1095 [==============================] - 0s 109us/step - loss: 0.1675 - acc: 0.0000e+00 - val_loss: 0.1395 - val_acc: 0.0000e+00\n",
            "Epoch 14/15\n",
            "1095/1095 [==============================] - 0s 112us/step - loss: 0.1683 - acc: 0.0000e+00 - val_loss: 0.1386 - val_acc: 0.0000e+00\n",
            "Epoch 15/15\n",
            "1095/1095 [==============================] - 0s 114us/step - loss: 0.1720 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "15\n",
            "91\n",
            "Train on 1095 samples, validate on 365 samples\n",
            "Epoch 1/15\n",
            "1095/1095 [==============================] - 0s 93us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1386 - val_acc: 0.0000e+00\n",
            "Epoch 2/15\n",
            "1095/1095 [==============================] - 0s 99us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 3/15\n",
            "1095/1095 [==============================] - 0s 94us/step - loss: 0.1667 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 4/15\n",
            "1095/1095 [==============================] - 0s 100us/step - loss: 0.1671 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 5/15\n",
            "1095/1095 [==============================] - 0s 95us/step - loss: 0.1678 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 6/15\n",
            "1095/1095 [==============================] - 0s 97us/step - loss: 0.1669 - acc: 0.0000e+00 - val_loss: 0.1388 - val_acc: 0.0000e+00\n",
            "Epoch 7/15\n",
            "1095/1095 [==============================] - 0s 94us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 8/15\n",
            "1095/1095 [==============================] - 0s 92us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 9/15\n",
            "1095/1095 [==============================] - 0s 93us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1386 - val_acc: 0.0000e+00\n",
            "Epoch 10/15\n",
            "1095/1095 [==============================] - 0s 92us/step - loss: 0.1667 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 11/15\n",
            "1095/1095 [==============================] - 0s 92us/step - loss: 0.1667 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 12/15\n",
            "1095/1095 [==============================] - 0s 97us/step - loss: 0.1667 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 13/15\n",
            "1095/1095 [==============================] - 0s 93us/step - loss: 0.1669 - acc: 0.0000e+00 - val_loss: 0.1386 - val_acc: 0.0000e+00\n",
            "Epoch 14/15\n",
            "1095/1095 [==============================] - 0s 95us/step - loss: 0.1673 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "Epoch 15/15\n",
            "1095/1095 [==============================] - 0s 97us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1390 - val_acc: 0.0000e+00\n",
            "15\n",
            "101\n",
            "Train on 1095 samples, validate on 365 samples\n",
            "Epoch 1/15\n",
            "1095/1095 [==============================] - 0s 87us/step - loss: 0.1668 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "Epoch 2/15\n",
            "1095/1095 [==============================] - 0s 88us/step - loss: 0.1668 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 3/15\n",
            "1095/1095 [==============================] - 0s 86us/step - loss: 0.1668 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "Epoch 4/15\n",
            "1095/1095 [==============================] - 0s 83us/step - loss: 0.1665 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 5/15\n",
            "1095/1095 [==============================] - 0s 93us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 6/15\n",
            "1095/1095 [==============================] - 0s 94us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "Epoch 7/15\n",
            "1095/1095 [==============================] - 0s 95us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 8/15\n",
            "1095/1095 [==============================] - 0s 92us/step - loss: 0.1672 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 9/15\n",
            "1095/1095 [==============================] - 0s 92us/step - loss: 0.1677 - acc: 0.0000e+00 - val_loss: 0.1389 - val_acc: 0.0000e+00\n",
            "Epoch 10/15\n",
            "1095/1095 [==============================] - 0s 87us/step - loss: 0.1663 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "Epoch 11/15\n",
            "1095/1095 [==============================] - 0s 93us/step - loss: 0.1669 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 12/15\n",
            "1095/1095 [==============================] - 0s 88us/step - loss: 0.1667 - acc: 0.0000e+00 - val_loss: 0.1386 - val_acc: 0.0000e+00\n",
            "Epoch 13/15\n",
            "1095/1095 [==============================] - 0s 89us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 14/15\n",
            "1095/1095 [==============================] - 0s 91us/step - loss: 0.1667 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "Epoch 15/15\n",
            "1095/1095 [==============================] - 0s 94us/step - loss: 0.1668 - acc: 0.0000e+00 - val_loss: 0.1386 - val_acc: 0.0000e+00\n",
            "15\n",
            "111\n",
            "Train on 1095 samples, validate on 365 samples\n",
            "Epoch 1/15\n",
            "1095/1095 [==============================] - 0s 89us/step - loss: 0.1669 - acc: 0.0000e+00 - val_loss: 0.1387 - val_acc: 0.0000e+00\n",
            "Epoch 2/15\n",
            "1095/1095 [==============================] - 0s 97us/step - loss: 0.1680 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 3/15\n",
            "1095/1095 [==============================] - 0s 86us/step - loss: 0.1683 - acc: 0.0000e+00 - val_loss: 0.1397 - val_acc: 0.0000e+00\n",
            "Epoch 4/15\n",
            "1095/1095 [==============================] - 0s 90us/step - loss: 0.1676 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 5/15\n",
            "1095/1095 [==============================] - 0s 88us/step - loss: 0.1668 - acc: 0.0000e+00 - val_loss: 0.1394 - val_acc: 0.0000e+00\n",
            "Epoch 6/15\n",
            "1095/1095 [==============================] - 0s 93us/step - loss: 0.1667 - acc: 0.0000e+00 - val_loss: 0.1389 - val_acc: 0.0000e+00\n",
            "Epoch 7/15\n",
            "1095/1095 [==============================] - 0s 90us/step - loss: 0.1677 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "Epoch 8/15\n",
            "1095/1095 [==============================] - 0s 89us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 9/15\n",
            "1095/1095 [==============================] - 0s 89us/step - loss: 0.1669 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 10/15\n",
            "1095/1095 [==============================] - 0s 89us/step - loss: 0.1676 - acc: 0.0000e+00 - val_loss: 0.1394 - val_acc: 0.0000e+00\n",
            "Epoch 11/15\n",
            "1095/1095 [==============================] - 0s 93us/step - loss: 0.1662 - acc: 0.0000e+00 - val_loss: 0.1404 - val_acc: 0.0000e+00\n",
            "Epoch 12/15\n",
            "1095/1095 [==============================] - 0s 92us/step - loss: 0.1756 - acc: 0.0000e+00 - val_loss: 0.1386 - val_acc: 0.0000e+00\n",
            "Epoch 13/15\n",
            "1095/1095 [==============================] - 0s 102us/step - loss: 0.1704 - acc: 0.0000e+00 - val_loss: 0.1428 - val_acc: 0.0000e+00\n",
            "Epoch 14/15\n",
            "1095/1095 [==============================] - 0s 94us/step - loss: 0.1680 - acc: 0.0000e+00 - val_loss: 0.1404 - val_acc: 0.0000e+00\n",
            "Epoch 15/15\n",
            "1095/1095 [==============================] - 0s 86us/step - loss: 0.1681 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "15\n",
            "121\n",
            "Train on 1095 samples, validate on 365 samples\n",
            "Epoch 1/15\n",
            "1095/1095 [==============================] - 0s 79us/step - loss: 0.1672 - acc: 0.0000e+00 - val_loss: 0.1391 - val_acc: 0.0000e+00\n",
            "Epoch 2/15\n",
            "1095/1095 [==============================] - 0s 78us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1387 - val_acc: 0.0000e+00\n",
            "Epoch 3/15\n",
            "1095/1095 [==============================] - 0s 82us/step - loss: 0.1665 - acc: 0.0000e+00 - val_loss: 0.1387 - val_acc: 0.0000e+00\n",
            "Epoch 4/15\n",
            "1095/1095 [==============================] - 0s 79us/step - loss: 0.1677 - acc: 0.0000e+00 - val_loss: 0.1387 - val_acc: 0.0000e+00\n",
            "Epoch 5/15\n",
            "1095/1095 [==============================] - 0s 80us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1386 - val_acc: 0.0000e+00\n",
            "Epoch 6/15\n",
            "1095/1095 [==============================] - 0s 78us/step - loss: 0.1667 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "Epoch 7/15\n",
            "1095/1095 [==============================] - 0s 80us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1386 - val_acc: 0.0000e+00\n",
            "Epoch 8/15\n",
            "1095/1095 [==============================] - 0s 79us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 9/15\n",
            "1095/1095 [==============================] - 0s 80us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 10/15\n",
            "1095/1095 [==============================] - 0s 83us/step - loss: 0.1665 - acc: 0.0000e+00 - val_loss: 0.1386 - val_acc: 0.0000e+00\n",
            "Epoch 11/15\n",
            "1095/1095 [==============================] - 0s 81us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 12/15\n",
            "1095/1095 [==============================] - 0s 79us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 13/15\n",
            "1095/1095 [==============================] - 0s 78us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 14/15\n",
            "1095/1095 [==============================] - 0s 78us/step - loss: 0.1667 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 15/15\n",
            "1095/1095 [==============================] - 0s 79us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "15\n",
            "131\n",
            "Train on 1095 samples, validate on 365 samples\n",
            "Epoch 1/15\n",
            "1095/1095 [==============================] - 0s 74us/step - loss: 0.1667 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 2/15\n",
            "1095/1095 [==============================] - 0s 74us/step - loss: 0.1665 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "Epoch 3/15\n",
            "1095/1095 [==============================] - 0s 71us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1386 - val_acc: 0.0000e+00\n",
            "Epoch 4/15\n",
            "1095/1095 [==============================] - 0s 68us/step - loss: 0.1669 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 5/15\n",
            "1095/1095 [==============================] - 0s 68us/step - loss: 0.1665 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 6/15\n",
            "1095/1095 [==============================] - 0s 71us/step - loss: 0.1665 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 7/15\n",
            "1095/1095 [==============================] - 0s 75us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 8/15\n",
            "1095/1095 [==============================] - 0s 74us/step - loss: 0.1667 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "Epoch 9/15\n",
            "1095/1095 [==============================] - 0s 74us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 10/15\n",
            "1095/1095 [==============================] - 0s 70us/step - loss: 0.1667 - acc: 0.0000e+00 - val_loss: 0.1385 - val_acc: 0.0000e+00\n",
            "Epoch 11/15\n",
            "1095/1095 [==============================] - 0s 75us/step - loss: 0.1671 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 12/15\n",
            "1095/1095 [==============================] - 0s 74us/step - loss: 0.1665 - acc: 0.0000e+00 - val_loss: 0.1388 - val_acc: 0.0000e+00\n",
            "Epoch 13/15\n",
            "1095/1095 [==============================] - 0s 71us/step - loss: 0.1667 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 14/15\n",
            "1095/1095 [==============================] - 0s 71us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "Epoch 15/15\n",
            "1095/1095 [==============================] - 0s 74us/step - loss: 0.1666 - acc: 0.0000e+00 - val_loss: 0.1384 - val_acc: 0.0000e+00\n",
            "15\n",
            "141\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HPcg8WmKwq6a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_axis = np.repeat([x_ax], 10)\n",
        "y_axis = np.concatenate([y_ax] * 10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mJ59SqlsWKSs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 766
        },
        "outputId": "405207f5-3eb8-4b8e-aa1d-7e9fae2b22fb"
      },
      "source": [
        "z_ = np.asarray(scores)\n",
        "z_axis = z_.reshape(len(x_ax),len(y_ax))\n",
        "z_axis "
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.38054962, 0.37477666, 0.37198563, 0.37259146, 0.37197827,\n",
              "        0.37217219, 0.37197551, 0.37200708, 0.3719849 , 0.3720116 ,\n",
              "        0.37239275, 0.37197893, 0.37197678, 0.37198857, 0.37207197],\n",
              "       [0.37255579, 0.37202122, 0.37355751, 0.37212112, 0.3720588 ,\n",
              "        0.3719998 , 0.37199915, 0.37212998, 0.37199936, 0.37262889,\n",
              "        0.37203251, 0.37203975, 0.37197662, 0.37205375, 0.37200777],\n",
              "       [0.37881588, 0.37247019, 0.37236393, 0.37254242, 0.37198127,\n",
              "        0.37205729, 0.37197554, 0.37242018, 0.37204186, 0.37220647,\n",
              "        0.37203981, 0.37218714, 0.37198981, 0.37201367, 0.37197872],\n",
              "       [0.429584  , 0.37199428, 0.37198975, 0.37212569, 0.37238487,\n",
              "        0.37198044, 0.37198025, 0.37200803, 0.3720663 , 0.37197821,\n",
              "        0.37215558, 0.37197814, 0.37202646, 0.37199034, 0.37215908],\n",
              "       [0.37265692, 0.37239737, 0.37275216, 0.37198741, 0.3724444 ,\n",
              "        0.37198897, 0.37199967, 0.37221137, 0.37200416, 0.37238306,\n",
              "        0.37202505, 0.37205223, 0.3723328 , 0.37207535, 0.37215589],\n",
              "       [0.40884169, 0.37286027, 0.37199039, 0.37353417, 0.37219247,\n",
              "        0.37219675, 0.37197629, 0.37216296, 0.37206239, 0.37197992,\n",
              "        0.37217954, 0.37200914, 0.37228458, 0.37209613, 0.37198166],\n",
              "       [0.37203829, 0.37211888, 0.37204   , 0.37213599, 0.37198174,\n",
              "        0.37198327, 0.37197641, 0.37197965, 0.37197659, 0.37204808,\n",
              "        0.37219328, 0.37205865, 0.37198884, 0.37205644, 0.37197882],\n",
              "       [0.37542171, 0.37288199, 0.37204728, 0.37240928, 0.37199478,\n",
              "        0.37208618, 0.37207339, 0.37198454, 0.37207268, 0.3720578 ,\n",
              "        0.37225664, 0.37202331, 0.37214262, 0.37198091, 0.37198588],\n",
              "       [0.39713407, 0.37440366, 0.37726095, 0.37198214, 0.3720206 ,\n",
              "        0.37198904, 0.37212695, 0.37203228, 0.37212688, 0.37219263,\n",
              "        0.37216152, 0.37221215, 0.37628305, 0.37221169, 0.37200257],\n",
              "       [0.37581792, 0.37437963, 0.38009382, 0.37220219, 0.37197696,\n",
              "        0.372013  , 0.37239442, 0.37207777, 0.37204775, 0.37418186,\n",
              "        0.37198787, 0.37249346, 0.37301696, 0.37209868, 0.37204499],\n",
              "       [0.38022876, 0.37209516, 0.37267804, 0.37287076, 0.37339146,\n",
              "        0.37291112, 0.37210209, 0.37230122, 0.37198067, 0.37198919,\n",
              "        0.37205266, 0.37218054, 0.37329191, 0.37206283, 0.37201213],\n",
              "       [0.37363096, 0.3767933 , 0.37210708, 0.37444529, 0.37219247,\n",
              "        0.37202226, 0.37207608, 0.37197678, 0.37213276, 0.37259465,\n",
              "        0.37197951, 0.37198023, 0.37280318, 0.3721742 , 0.37200006],\n",
              "       [0.38311483, 0.37473398, 0.37198639, 0.37204725, 0.37258886,\n",
              "        0.3719955 , 0.37215052, 0.37207597, 0.37197711, 0.37209642,\n",
              "        0.3720477 , 0.37214955, 0.37216632, 0.37197692, 0.37204771],\n",
              "       [0.37374183, 0.37444744, 0.37197759, 0.37274882, 0.37243325,\n",
              "        0.37222556, 0.3721403 , 0.37219408, 0.37199134, 0.37230005,\n",
              "        0.37201424, 0.3720926 , 0.37206759, 0.37200306, 0.37217835],\n",
              "       [0.41704301, 0.37218544, 0.37223002, 0.37209073, 0.37203976,\n",
              "        0.37204438, 0.37219728, 0.3723722 , 0.3720017 , 0.37204099,\n",
              "        0.37280035, 0.37235317, 0.37213298, 0.37202828, 0.37205079]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tK0RB2NXew1U",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 247
        },
        "outputId": "1ff64648-48a9-4ef7-9647-8c2ff66e87e7"
      },
      "source": [
        "fig = plt.figure()\n",
        "ax = fig.gca(projection='3d')\n",
        "\n",
        "# Make data.\n",
        "X = x_ax\n",
        "Y = y_ax\n",
        "X, Y = np.meshgrid(X, Y)\n",
        "Z = z_axis\n",
        "\n",
        "# Plot the surface.\n",
        "surf = ax.plot_surface(X, Y, Z, cmap=cm.coolwarm, linewidth=0, antialiased=False)\n",
        "\n",
        "# Customize the z axis.\n",
        "ax.set_zlim(-1.01, 1.01)\n",
        "ax.zaxis.set_major_locator(LinearLocator(10))\n",
        "ax.zaxis.set_major_formatter(FormatStrFormatter('%.02f'))\n",
        "\n",
        "# Add a color bar which maps values to colors.\n",
        "fig.colorbar(surf, shrink=0.5, aspect=5)\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWQAAADnCAYAAAApSCziAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JUdpS3WNzW_X",
        "colab_type": "text"
      },
      "source": [
        "Como se aprecia en la figura anterior, entre a mayor cantidad de *epochs* y a menor cantidad del tamao del *batch* vamos a tener mejor desempeo de la red, pero vamos a tener mayor cantidad de tiempo que se va a demorar el algortimo.\n",
        "\n",
        "Bajo los intervalos de batch size evaluados, no se encuentra que sea sensible para la metrica de desempeo.\n",
        "\n",
        "Por otro lado, a menor cantidad de epochs, el batch size se vuelve mas sensible."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "2d2c8d6c5c27d9f195806f45ea6f66c16541bda4",
        "id": "RxSOLEP8HbgE",
        "colab_type": "text"
      },
      "source": [
        "Reference\n",
        "1. https://www.kaggle.com/vishnus/regression-using-keras\n",
        "1. https://www.kaggle.com/apapiu/regularized-linear-models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "afa7ea0c00715f45be73b59ff7e07067361fbd11",
        "trusted": false,
        "id": "zlgSZ_AAHbgF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}